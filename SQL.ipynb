{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "951e7cba",
   "metadata": {},
   "source": [
    "# ROW_NUMBER(), RANK(), DENSE_RANK() - Complete Deep Dive\n",
    "\n",
    "## The Core Concept: What Are These Functions?\n",
    "\n",
    "These are **window functions** that assign numbers to rows based on some ordering criteria. Think of them as different ways to \"number\" your data rows, each with specific rules about how they handle ties (duplicate values).\n",
    "\n",
    "### The Mental Model\n",
    "Imagine you're ranking students in a class by their test scores:\n",
    "- **ROW_NUMBER()**: Every student gets a unique number, even if they have the same score\n",
    "- **RANK()**: Students with the same score get the same rank, but we skip numbers afterward\n",
    "- **DENSE_RANK()**: Students with the same score get the same rank, but we DON'T skip numbers\n",
    "\n",
    "## Function Syntax & Behavior\n",
    "\n",
    "```sql\n",
    "ROW_NUMBER() OVER (PARTITION BY column ORDER BY column)\n",
    "RANK() OVER (PARTITION BY column ORDER BY column) \n",
    "DENSE_RANK() OVER (PARTITION BY column ORDER BY column)\n",
    "```\n",
    "\n",
    "### Key Components:\n",
    "- **OVER()**: Defines the window (scope of rows to consider)\n",
    "- **PARTITION BY**: Divides data into groups (optional)\n",
    "- **ORDER BY**: Defines the ranking criteria (required)\n",
    "\n",
    "## Detailed Breakdown\n",
    "\n",
    "### ROW_NUMBER()\n",
    "**What it does**: Assigns a unique sequential integer to each row\n",
    "**Ties handling**: No ties - every row gets a different number\n",
    "**Use when**: You need unique identifiers or want to pick exactly N rows\n",
    "\n",
    "### RANK()\n",
    "**What it does**: Assigns the same rank to rows with identical values\n",
    "**Ties handling**: Gaps in sequence (1, 2, 2, 4, 5...)\n",
    "**Use when**: Traditional ranking like sports leaderboards\n",
    "\n",
    "### DENSE_RANK()\n",
    "**What it does**: Same rank for identical values, but no gaps\n",
    "**Ties handling**: No gaps in sequence (1, 2, 2, 3, 4...)\n",
    "**Use when**: You want consecutive ranking numbers\n",
    "\n",
    "## Real-World Examples\n",
    "\n",
    "### Example 1: Employee Salary Ranking\n",
    "```sql\n",
    "-- Sample data: employees table\n",
    "CREATE TABLE employees (\n",
    "    emp_id INT,\n",
    "    name VARCHAR(50),\n",
    "    department VARCHAR(50),\n",
    "    salary DECIMAL(10,2)\n",
    ");\n",
    "\n",
    "-- Ranking queries\n",
    "SELECT \n",
    "    name,\n",
    "    department,\n",
    "    salary,\n",
    "    ROW_NUMBER() OVER (ORDER BY salary DESC) as row_num,\n",
    "    RANK() OVER (ORDER BY salary DESC) as rank_pos,\n",
    "    DENSE_RANK() OVER (ORDER BY salary DESC) as dense_rank_pos\n",
    "FROM employees;\n",
    "```\n",
    "\n",
    "**Result with sample data:**\n",
    "| Name | Department | Salary | ROW_NUMBER | RANK | DENSE_RANK |\n",
    "|------|------------|--------|------------|------|------------|\n",
    "| Alice | Engineering | 95000 | 1 | 1 | 1 |\n",
    "| Bob | Engineering | 90000 | 2 | 2 | 2 |\n",
    "| Carol | Marketing | 90000 | 3 | 2 | 2 |\n",
    "| David | Sales | 85000 | 4 | 4 | 3 |\n",
    "\n",
    "### Example 2: Top N Per Group\n",
    "```sql\n",
    "-- Get top 3 highest paid employees per department\n",
    "WITH ranked_employees AS (\n",
    "    SELECT \n",
    "        name,\n",
    "        department,\n",
    "        salary,\n",
    "        ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as rn\n",
    "    FROM employees\n",
    ")\n",
    "SELECT * FROM ranked_employees WHERE rn <= 3;\n",
    "```\n",
    "\n",
    "## Popular Use Cases\n",
    "\n",
    "### 1. **Pagination & Data Sampling**\n",
    "```sql\n",
    "-- Get rows 11-20 for pagination\n",
    "WITH numbered_rows AS (\n",
    "    SELECT *, ROW_NUMBER() OVER (ORDER BY created_date) as rn\n",
    "    FROM products\n",
    ")\n",
    "SELECT * FROM numbered_rows WHERE rn BETWEEN 11 AND 20;\n",
    "```\n",
    "\n",
    "### 2. **Removing Duplicates**\n",
    "```sql\n",
    "-- Remove duplicate customers, keeping the latest record\n",
    "WITH dedupe AS (\n",
    "    SELECT *,\n",
    "           ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY updated_date DESC) as rn\n",
    "    FROM customers\n",
    ")\n",
    "SELECT * FROM dedupe WHERE rn = 1;\n",
    "```\n",
    "\n",
    "### 3. **Finding Top/Bottom N**\n",
    "```sql\n",
    "-- Top 5 products by revenue in each category\n",
    "WITH product_ranks AS (\n",
    "    SELECT \n",
    "        product_name,\n",
    "        category,\n",
    "        revenue,\n",
    "        DENSE_RANK() OVER (PARTITION BY category ORDER BY revenue DESC) as rank_pos\n",
    "    FROM product_sales\n",
    ")\n",
    "SELECT * FROM product_ranks WHERE rank_pos <= 5;\n",
    "```\n",
    "\n",
    "### 4. **Percentile Analysis**\n",
    "```sql\n",
    "-- Divide customers into quartiles by spending\n",
    "SELECT \n",
    "    customer_id,\n",
    "    total_spent,\n",
    "    NTILE(4) OVER (ORDER BY total_spent) as quartile,\n",
    "    DENSE_RANK() OVER (ORDER BY total_spent DESC) as spending_rank\n",
    "FROM customer_spending;\n",
    "```\n",
    "\n",
    "## Python Pandas Equivalents\n",
    "\n",
    "### ROW_NUMBER()\n",
    "```python\n",
    "# SQL: ROW_NUMBER() OVER (ORDER BY salary DESC)\n",
    "df['row_num'] = df.sort_values('salary', ascending=False).reset_index(drop=True).index + 1\n",
    "\n",
    "# With grouping\n",
    "df['row_num'] = df.sort_values('salary', ascending=False).groupby('department').cumcount() + 1\n",
    "```\n",
    "\n",
    "### RANK()\n",
    "```python\n",
    "# SQL: RANK() OVER (ORDER BY salary DESC)\n",
    "df['rank'] = df['salary'].rank(method='min', ascending=False)\n",
    "\n",
    "# With grouping\n",
    "df['rank'] = df.groupby('department')['salary'].rank(method='min', ascending=False)\n",
    "```\n",
    "\n",
    "### DENSE_RANK()\n",
    "```python\n",
    "# SQL: DENSE_RANK() OVER (ORDER BY salary DESC)\n",
    "df['dense_rank'] = df['salary'].rank(method='dense', ascending=False)\n",
    "\n",
    "# With grouping\n",
    "df['dense_rank'] = df.groupby('department')['salary'].rank(method='dense', ascending=False)\n",
    "```\n",
    "\n",
    "## Excel Equivalents\n",
    "\n",
    "### ROW_NUMBER()\n",
    "```excel\n",
    "=ROW() - 1  # Simple sequential numbering\n",
    "=COUNTIF($A$1:A1, A1)  # For grouped numbering\n",
    "```\n",
    "\n",
    "### RANK()\n",
    "```excel\n",
    "=RANK(B2, $B$2:$B$10, 0)  # 0 for descending order\n",
    "```\n",
    "\n",
    "### DENSE_RANK()\n",
    "Excel doesn't have a direct equivalent, but you can use:\n",
    "```excel\n",
    "=RANK(B2, $B$2:$B$10, 0) - COUNTIF($B$2:B1, \">\"&B2)\n",
    "```\n",
    "\n",
    "## Performance Considerations\n",
    "\n",
    "### 1. **Index Strategy**\n",
    "- Create indexes on columns used in ORDER BY\n",
    "- Composite indexes for PARTITION BY + ORDER BY\n",
    "\n",
    "### 2. **Memory Usage**\n",
    "- Window functions can be memory-intensive\n",
    "- Consider limiting result sets with WHERE clauses\n",
    "\n",
    "### 3. **Query Optimization**\n",
    "```sql\n",
    "-- GOOD: Filter first, then rank\n",
    "WITH filtered_data AS (\n",
    "    SELECT * FROM large_table WHERE date >= '2024-01-01'\n",
    ")\n",
    "SELECT *, ROW_NUMBER() OVER (ORDER BY amount DESC) as rn\n",
    "FROM filtered_data;\n",
    "\n",
    "-- AVOID: Ranking all data then filtering\n",
    "SELECT * FROM (\n",
    "    SELECT *, ROW_NUMBER() OVER (ORDER BY amount DESC) as rn\n",
    "    FROM large_table\n",
    ") WHERE date >= '2024-01-01';\n",
    "```\n",
    "\n",
    "## Common Interview Questions\n",
    "\n",
    "### Q1: \"Get the 2nd highest salary in each department\"\n",
    "```sql\n",
    "WITH salary_ranks AS (\n",
    "    SELECT \n",
    "        emp_id, \n",
    "        department, \n",
    "        salary,\n",
    "        DENSE_RANK() OVER (PARTITION BY department ORDER BY salary DESC) as rank_pos\n",
    "    FROM employees\n",
    ")\n",
    "SELECT * FROM salary_ranks WHERE rank_pos = 2;\n",
    "```\n",
    "\n",
    "### Q2: \"Find duplicate records\"\n",
    "```sql\n",
    "WITH duplicate_check AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        ROW_NUMBER() OVER (PARTITION BY email ORDER BY created_date) as rn\n",
    "    FROM users\n",
    ")\n",
    "SELECT * FROM duplicate_check WHERE rn > 1;\n",
    "```\n",
    "\n",
    "### Q3: \"Calculate running totals with rankings\"\n",
    "```sql\n",
    "SELECT \n",
    "    order_date,\n",
    "    amount,\n",
    "    SUM(amount) OVER (ORDER BY order_date) as running_total,\n",
    "    ROW_NUMBER() OVER (ORDER BY order_date) as day_number,\n",
    "    RANK() OVER (ORDER BY amount DESC) as amount_rank\n",
    "FROM daily_sales;\n",
    "```\n",
    "\n",
    "## Advanced Patterns\n",
    "\n",
    "### 1. **Gap and Island Analysis**\n",
    "```sql\n",
    "-- Find consecutive sequences\n",
    "WITH gaps AS (\n",
    "    SELECT \n",
    "        id,\n",
    "        id - ROW_NUMBER() OVER (ORDER BY id) as grp\n",
    "    FROM sequence_table\n",
    ")\n",
    "SELECT \n",
    "    MIN(id) as start_id,\n",
    "    MAX(id) as end_id,\n",
    "    COUNT(*) as sequence_length\n",
    "FROM gaps\n",
    "GROUP BY grp;\n",
    "```\n",
    "\n",
    "### 2. **Median Calculation**\n",
    "```sql\n",
    "WITH ranked_values AS (\n",
    "    SELECT \n",
    "        value,\n",
    "        ROW_NUMBER() OVER (ORDER BY value) as rn,\n",
    "        COUNT(*) OVER () as total_count\n",
    "    FROM measurements\n",
    ")\n",
    "SELECT AVG(value) as median\n",
    "FROM ranked_values\n",
    "WHERE rn IN ((total_count + 1) / 2, (total_count + 2) / 2);\n",
    "```\n",
    "\n",
    "## When to Use Which Function\n",
    "\n",
    "| Scenario | Use Function | Why |\n",
    "|----------|--------------|-----|\n",
    "| Pagination | ROW_NUMBER() | Need unique sequential numbers |\n",
    "| Top N per group | ROW_NUMBER() | Guarantees exactly N rows |\n",
    "| Sports rankings | RANK() | Traditional ranking with gaps |\n",
    "| Grade distributions | DENSE_RANK() | Consecutive ranking levels |\n",
    "| Duplicate removal | ROW_NUMBER() | Unique identifier for each row |\n",
    "| Percentile analysis | RANK() or DENSE_RANK() | Depending on gap preference |\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **ROW_NUMBER()** is your go-to for unique sequential numbering\n",
    "2. **RANK()** mimics traditional ranking systems (think Olympics)\n",
    "3. **DENSE_RANK()** provides consecutive numbering without gaps\n",
    "4. All three require ORDER BY clause\n",
    "5. PARTITION BY creates separate ranking groups\n",
    "6. Performance depends heavily on indexing strategy\n",
    "7. These functions are essential for data analysis and extremely common in interviews\n",
    "\n",
    "Remember: Master these three functions, and you'll solve 80% of ranking-related problems in SQL!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7b226",
   "metadata": {},
   "source": [
    "# ___________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efb9ab5",
   "metadata": {},
   "source": [
    "# LAG() and LEAD() Functions - Complete Deep Dive\n",
    "\n",
    "## The Core Concept: Looking Backward and Forward\n",
    "\n",
    "LAG() and LEAD() are window functions that let you **access data from other rows** without using self-joins. They're like having a time machine for your data - you can look at previous or future rows while staying in the current row.\n",
    "\n",
    "### The Mental Model\n",
    "Imagine you're looking at a spreadsheet:\n",
    "- **LAG()**: \"What was the value in the row above me?\"\n",
    "- **LEAD()**: \"What's the value in the row below me?\"\n",
    "\n",
    "But unlike spreadsheet formulas, these work with proper ordering and partitioning logic.\n",
    "\n",
    "## Function Syntax & Core Behavior\n",
    "\n",
    "```sql\n",
    "LAG(column, offset, default_value) OVER (PARTITION BY column ORDER BY column)\n",
    "LEAD(column, offset, default_value) OVER (PARTITION BY column ORDER BY column)\n",
    "```\n",
    "\n",
    "### Parameters:\n",
    "- **column**: The column value you want to retrieve\n",
    "- **offset**: How many rows back/forward (default = 1)\n",
    "- **default_value**: What to return if no row exists (default = NULL)\n",
    "- **OVER()**: Defines the window scope\n",
    "\n",
    "### Key Insight: The \"Tiebreaker Principle\" Applies Here Too!\n",
    "Just like with ROW_NUMBER(), if your ORDER BY isn't unique, LAG/LEAD results become unpredictable. Always ensure deterministic ordering.\n",
    "\n",
    "## Detailed Examples\n",
    "\n",
    "### Example 1: Sales Trend Analysis\n",
    "```sql\n",
    "-- Sample data: daily_sales table\n",
    "CREATE TABLE daily_sales (\n",
    "    sale_date DATE,\n",
    "    store_id INT,\n",
    "    revenue DECIMAL(10,2)\n",
    ");\n",
    "\n",
    "-- Compare each day to previous day\n",
    "SELECT \n",
    "    sale_date,\n",
    "    store_id,\n",
    "    revenue,\n",
    "    LAG(revenue, 1) OVER (PARTITION BY store_id ORDER BY sale_date) as prev_day_revenue,\n",
    "    LEAD(revenue, 1) OVER (PARTITION BY store_id ORDER BY sale_date) as next_day_revenue,\n",
    "    revenue - LAG(revenue, 1) OVER (PARTITION BY store_id ORDER BY sale_date) as daily_change,\n",
    "    ROUND(\n",
    "        (revenue - LAG(revenue, 1) OVER (PARTITION BY store_id ORDER BY sale_date)) * 100.0 / \n",
    "        LAG(revenue, 1) OVER (PARTITION BY store_id ORDER BY sale_date), 2\n",
    "    ) as pct_change\n",
    "FROM daily_sales\n",
    "ORDER BY store_id, sale_date;\n",
    "```\n",
    "\n",
    "**Sample Result:**\n",
    "| sale_date | store_id | revenue | prev_day_revenue | next_day_revenue | daily_change | pct_change |\n",
    "|-----------|----------|---------|------------------|------------------|--------------|------------|\n",
    "| 2024-01-01| 1        | 1000    | NULL             | 1200             | NULL         | NULL       |\n",
    "| 2024-01-02| 1        | 1200    | 1000             | 950              | 200          | 20.00      |\n",
    "| 2024-01-03| 1        | 950     | 1200             | NULL             | -250         | -20.83     |\n",
    "\n",
    "## Popular Use Cases\n",
    "\n",
    "### 1. **Customer Journey Analysis**\n",
    "```sql\n",
    "-- Track customer behavior across sessions\n",
    "WITH customer_sessions AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        session_date,\n",
    "        pages_viewed,\n",
    "        session_duration,\n",
    "        LAG(session_date) OVER (PARTITION BY customer_id ORDER BY session_date) as prev_session_date,\n",
    "        LAG(pages_viewed) OVER (PARTITION BY customer_id ORDER BY session_date) as prev_pages_viewed\n",
    "    FROM user_sessions\n",
    ")\n",
    "SELECT \n",
    "    customer_id,\n",
    "    session_date,\n",
    "    pages_viewed,\n",
    "    prev_pages_viewed,\n",
    "    pages_viewed - prev_pages_viewed as engagement_change,\n",
    "    session_date - prev_session_date as days_between_sessions\n",
    "FROM customer_sessions\n",
    "WHERE prev_session_date IS NOT NULL;\n",
    "```\n",
    "\n",
    "### 2. **Stock Price Analysis**\n",
    "```sql\n",
    "-- Calculate moving differences and identify patterns\n",
    "SELECT \n",
    "    ticker,\n",
    "    trade_date,\n",
    "    closing_price,\n",
    "    LAG(closing_price, 1) OVER (PARTITION BY ticker ORDER BY trade_date) as prev_close,\n",
    "    LAG(closing_price, 7) OVER (PARTITION BY ticker ORDER BY trade_date) as week_ago_close,\n",
    "    closing_price - LAG(closing_price, 1) OVER (PARTITION BY ticker ORDER BY trade_date) as daily_change,\n",
    "    closing_price - LAG(closing_price, 7) OVER (PARTITION BY ticker ORDER BY trade_date) as weekly_change,\n",
    "    CASE \n",
    "        When closing_price > LAG(closing_price, 1) OVER (PARTITION BY ticker ORDER BY trade_date) \n",
    "        THEN 'UP'\n",
    "        WHEN closing_price < LAG(closing_price, 1) OVER (PARTITION BY ticker ORDER BY trade_date) \n",
    "        THEN 'DOWN'\n",
    "        ELSE 'FLAT'\n",
    "    END as daily_trend\n",
    "FROM stock_prices;\n",
    "```\n",
    "\n",
    "### 3. **Gap Detection in Sequences**\n",
    "```sql\n",
    "-- Find missing dates in a time series\n",
    "WITH date_gaps AS (\n",
    "    SELECT \n",
    "        event_date,\n",
    "        LAG(event_date) OVER (ORDER BY event_date) as prev_date,\n",
    "        event_date - LAG(event_date) OVER (ORDER BY event_date) as gap_days\n",
    "    FROM events\n",
    ")\n",
    "SELECT \n",
    "    prev_date,\n",
    "    event_date,\n",
    "    gap_days\n",
    "FROM date_gaps\n",
    "WHERE gap_days > 1  -- More than 1 day gap\n",
    "ORDER BY event_date;\n",
    "```\n",
    "\n",
    "### 4. **Funnel Analysis**\n",
    "```sql\n",
    "-- Track user progression through conversion funnel\n",
    "WITH user_funnel AS (\n",
    "    SELECT \n",
    "        user_id,\n",
    "        step_name,\n",
    "        step_timestamp,\n",
    "        LAG(step_name) OVER (PARTITION BY user_id ORDER BY step_timestamp) as prev_step,\n",
    "        LAG(step_timestamp) OVER (PARTITION BY user_id ORDER BY step_timestamp) as prev_timestamp,\n",
    "        LEAD(step_name) OVER (PARTITION BY user_id ORDER BY step_timestamp) as next_step\n",
    "    FROM conversion_events\n",
    ")\n",
    "SELECT \n",
    "    user_id,\n",
    "    prev_step,\n",
    "    step_name,\n",
    "    next_step,\n",
    "    step_timestamp - prev_timestamp as time_between_steps,\n",
    "    CASE \n",
    "        WHEN next_step IS NULL THEN 'DROP_OFF'\n",
    "        ELSE 'CONTINUED'\n",
    "    END as funnel_outcome\n",
    "FROM user_funnel;\n",
    "```\n",
    "\n",
    "## Advanced Patterns\n",
    "\n",
    "### 1. **Running Streaks**\n",
    "```sql\n",
    "-- Find consecutive days with increasing sales\n",
    "WITH sales_trends AS (\n",
    "    SELECT \n",
    "        sale_date,\n",
    "        revenue,\n",
    "        LAG(revenue) OVER (ORDER BY sale_date) as prev_revenue,\n",
    "        CASE \n",
    "            WHEN revenue > LAG(revenue) OVER (ORDER BY sale_date) THEN 1 \n",
    "            ELSE 0 \n",
    "        END as is_increase\n",
    "    FROM daily_sales\n",
    "),\n",
    "streak_groups AS (\n",
    "    SELECT \n",
    "        sale_date,\n",
    "        revenue,\n",
    "        is_increase,\n",
    "        SUM(CASE WHEN is_increase = 0 THEN 1 ELSE 0 END) \n",
    "            OVER (ORDER BY sale_date) as streak_group\n",
    "    FROM sales_trends\n",
    ")\n",
    "SELECT \n",
    "    streak_group,\n",
    "    MIN(sale_date) as streak_start,\n",
    "    MAX(sale_date) as streak_end,\n",
    "    COUNT(*) as streak_length\n",
    "FROM streak_groups\n",
    "WHERE is_increase = 1\n",
    "GROUP BY streak_group\n",
    "HAVING COUNT(*) >= 3  -- Streaks of 3+ days\n",
    "ORDER BY streak_length DESC;\n",
    "```\n",
    "\n",
    "### 2. **First and Last Value in Groups**\n",
    "```sql\n",
    "-- Compare current value to first and last in each group\n",
    "SELECT \n",
    "    department,\n",
    "    employee_name,\n",
    "    hire_date,\n",
    "    salary,\n",
    "    FIRST_VALUE(salary) OVER (PARTITION BY department ORDER BY hire_date) as first_hire_salary,\n",
    "    LAST_VALUE(salary) OVER (\n",
    "        PARTITION BY department \n",
    "        ORDER BY hire_date \n",
    "        ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n",
    "    ) as last_hire_salary,\n",
    "    salary - FIRST_VALUE(salary) OVER (PARTITION BY department ORDER BY hire_date) as vs_first_hire\n",
    "FROM employees;\n",
    "```\n",
    "\n",
    "### 3. **Multi-Step Lookback**\n",
    "```sql\n",
    "-- Compare with multiple previous periods\n",
    "SELECT \n",
    "    month_year,\n",
    "    revenue,\n",
    "    LAG(revenue, 1) OVER (ORDER BY month_year) as prev_month,\n",
    "    LAG(revenue, 3) OVER (ORDER BY month_year) as three_months_ago,\n",
    "    LAG(revenue, 12) OVER (ORDER BY month_year) as year_ago,\n",
    "    -- Month-over-month growth\n",
    "    (revenue - LAG(revenue, 1) OVER (ORDER BY month_year)) * 100.0 / \n",
    "        LAG(revenue, 1) OVER (ORDER BY month_year) as mom_growth,\n",
    "    -- Year-over-year growth\n",
    "    (revenue - LAG(revenue, 12) OVER (ORDER BY month_year)) * 100.0 / \n",
    "        LAG(revenue, 12) OVER (ORDER BY month_year) as yoy_growth\n",
    "FROM monthly_revenue;\n",
    "```\n",
    "\n",
    "## Python Pandas Equivalents\n",
    "\n",
    "### Basic LAG/LEAD\n",
    "```python\n",
    "# SQL: LAG(column) OVER (ORDER BY date)\n",
    "df['prev_value'] = df.sort_values('date')['value'].shift(1)\n",
    "\n",
    "# SQL: LEAD(column) OVER (ORDER BY date)  \n",
    "df['next_value'] = df.sort_values('date')['value'].shift(-1)\n",
    "\n",
    "# With grouping\n",
    "df['prev_value'] = df.sort_values('date').groupby('customer_id')['value'].shift(1)\n",
    "```\n",
    "\n",
    "### Advanced Operations\n",
    "```python\n",
    "# Multiple period shifts\n",
    "df = df.sort_values('date')\n",
    "df['prev_month'] = df['revenue'].shift(1)\n",
    "df['three_months_ago'] = df['revenue'].shift(3)\n",
    "df['year_ago'] = df['revenue'].shift(12)\n",
    "\n",
    "# Percentage changes\n",
    "df['mom_pct'] = df['revenue'].pct_change() * 100\n",
    "df['yoy_pct'] = df['revenue'].pct_change(periods=12) * 100\n",
    "\n",
    "# With default values\n",
    "df['prev_value'] = df['value'].shift(1).fillna(0)  # Default to 0\n",
    "```\n",
    "\n",
    "## Excel Equivalents\n",
    "\n",
    "### Basic References\n",
    "```excel\n",
    "# LAG equivalent (previous row)\n",
    "=OFFSET(B2,-1,0)  # Gets value from row above\n",
    "\n",
    "# LEAD equivalent (next row)  \n",
    "=OFFSET(B2,1,0)   # Gets value from row below\n",
    "\n",
    "# With error handling\n",
    "=IFERROR(OFFSET(B2,-1,0), 0)  # Default to 0 if no previous row\n",
    "```\n",
    "\n",
    "### Percentage Change\n",
    "```excel\n",
    "# Month-over-month growth\n",
    "=(B2-B1)/B1*100\n",
    "\n",
    "# With error handling for first row\n",
    "=IF(ROW()=2, \"\", (B2-B1)/B1*100)\n",
    "```\n",
    "\n",
    "## Performance Considerations\n",
    "\n",
    "### 1. **Index Strategy**\n",
    "```sql\n",
    "-- Create composite indexes for PARTITION BY + ORDER BY\n",
    "CREATE INDEX idx_sales_store_date ON daily_sales(store_id, sale_date);\n",
    "CREATE INDEX idx_events_user_timestamp ON user_events(user_id, event_timestamp);\n",
    "```\n",
    "\n",
    "### 2. **Window Frame Optimization**\n",
    "```sql\n",
    "-- Default frame is usually efficient\n",
    "LAG(revenue) OVER (PARTITION BY store_id ORDER BY sale_date)\n",
    "\n",
    "-- Avoid unnecessary frame specifications\n",
    "-- This is redundant for LAG/LEAD:\n",
    "LAG(revenue) OVER (\n",
    "    PARTITION BY store_id \n",
    "    ORDER BY sale_date \n",
    "    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW  -- Unnecessary!\n",
    ")\n",
    "```\n",
    "\n",
    "### 3. **Reusing Window Definitions**\n",
    "```sql\n",
    "-- Instead of repeating the OVER clause:\n",
    "SELECT \n",
    "    sale_date,\n",
    "    revenue,\n",
    "    LAG(revenue, 1) OVER (PARTITION BY store_id ORDER BY sale_date) as prev_day,\n",
    "    LAG(revenue, 7) OVER (PARTITION BY store_id ORDER BY sale_date) as week_ago,\n",
    "    LEAD(revenue, 1) OVER (PARTITION BY store_id ORDER BY sale_date) as next_day\n",
    "FROM daily_sales;\n",
    "\n",
    "-- Use WINDOW clause:\n",
    "SELECT \n",
    "    sale_date,\n",
    "    revenue,\n",
    "    LAG(revenue, 1) OVER w as prev_day,\n",
    "    LAG(revenue, 7) OVER w as week_ago,\n",
    "    LEAD(revenue, 1) OVER w as next_day\n",
    "FROM daily_sales\n",
    "WINDOW w AS (PARTITION BY store_id ORDER BY sale_date);\n",
    "```\n",
    "\n",
    "## Common Interview Questions\n",
    "\n",
    "### Q1: \"Calculate month-over-month growth rate\"\n",
    "```sql\n",
    "WITH monthly_growth AS (\n",
    "    SELECT \n",
    "        month_year,\n",
    "        revenue,\n",
    "        LAG(revenue) OVER (ORDER BY month_year) as prev_month_revenue\n",
    "    FROM monthly_sales\n",
    ")\n",
    "SELECT \n",
    "    month_year,\n",
    "    revenue,\n",
    "    prev_month_revenue,\n",
    "    ROUND(\n",
    "        (revenue - prev_month_revenue) * 100.0 / prev_month_revenue, 2\n",
    "    ) as growth_rate_pct\n",
    "FROM monthly_growth\n",
    "WHERE prev_month_revenue IS NOT NULL;\n",
    "```\n",
    "\n",
    "### Q2: \"Find the longest streak of increasing values\"\n",
    "```sql\n",
    "WITH value_changes AS (\n",
    "    SELECT \n",
    "        date_col,\n",
    "        value,\n",
    "        CASE \n",
    "            WHEN value > LAG(value) OVER (ORDER BY date_col) THEN 1 \n",
    "            ELSE 0 \n",
    "        END as is_increase\n",
    "    FROM time_series\n",
    "),\n",
    "streak_groups AS (\n",
    "    SELECT \n",
    "        date_col,\n",
    "        value,\n",
    "        is_increase,\n",
    "        SUM(CASE WHEN is_increase = 0 THEN 1 ELSE 0 END) \n",
    "            OVER (ORDER BY date_col) as group_id\n",
    "    FROM value_changes\n",
    ")\n",
    "SELECT \n",
    "    group_id,\n",
    "    COUNT(*) as streak_length,\n",
    "    MIN(date_col) as streak_start,\n",
    "    MAX(date_col) as streak_end\n",
    "FROM streak_groups\n",
    "WHERE is_increase = 1\n",
    "GROUP BY group_id\n",
    "ORDER BY streak_length DESC\n",
    "LIMIT 1;\n",
    "```\n",
    "\n",
    "### Q3: \"Identify customer churn (no activity for 30+ days)\"\n",
    "```sql\n",
    "WITH customer_activity AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        activity_date,\n",
    "        LEAD(activity_date) OVER (\n",
    "            PARTITION BY customer_id \n",
    "            ORDER BY activity_date\n",
    "        ) as next_activity_date\n",
    "    FROM customer_events\n",
    ")\n",
    "SELECT \n",
    "    customer_id,\n",
    "    activity_date as last_activity,\n",
    "    next_activity_date,\n",
    "    next_activity_date - activity_date as days_inactive\n",
    "FROM customer_activity\n",
    "WHERE next_activity_date - activity_date > 30\n",
    "   OR next_activity_date IS NULL;  -- No future activity\n",
    "```\n",
    "\n",
    "## When to Use LAG vs LEAD\n",
    "\n",
    "| Scenario | Use Function | Example |\n",
    "|----------|--------------|---------|\n",
    "| Trend analysis | LAG() | Compare to previous period |\n",
    "| Growth calculations | LAG() | Current vs previous value |\n",
    "| Forecasting validation | LEAD() | Compare prediction to actual |\n",
    "| Pipeline analysis | LEAD() | What happens next? |\n",
    "| Cohort analysis | Both | Entry and exit behaviors |\n",
    "| Time series gaps | LAG() | Find missing periods |\n",
    "\n",
    "## Key Mental Models\n",
    "\n",
    "1. **LAG() = \"What happened before?\"** - Perfect for trend analysis, growth calculations\n",
    "2. **LEAD() = \"What happens next?\"** - Great for predicting outcomes, validating forecasts\n",
    "3. **Both preserve row context** - Unlike self-joins, you stay in the current row while accessing others\n",
    "4. **Offset parameter is powerful** - Look back/forward multiple periods easily\n",
    "5. **Default values handle boundaries** - Control what happens at start/end of partitions\n",
    "\n",
    "## The \"Tiebreaker\" Connection\n",
    "\n",
    "Just like with ROW_NUMBER(), deterministic ordering is crucial:\n",
    "```sql\n",
    "-- ❌ Unpredictable if multiple records per day\n",
    "LAG(revenue) OVER (PARTITION BY store_id ORDER BY sale_date)\n",
    "\n",
    "-- ✅ Predictable with tiebreaker\n",
    "LAG(revenue) OVER (PARTITION BY store_id ORDER BY sale_date, transaction_id)\n",
    "```\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **LAG/LEAD eliminate complex self-joins** for accessing neighboring rows\n",
    "2. **Essential for time series analysis** - trends, growth, patterns\n",
    "3. **Offset parameter** lets you look multiple periods back/forward\n",
    "4. **Default values** handle edge cases gracefully\n",
    "5. **Performance depends on proper indexing** of partition and order columns\n",
    "6. **Always ensure deterministic ordering** to avoid unpredictable results\n",
    "7. **Combine with other window functions** for powerful analytics\n",
    "\n",
    "These functions turn complex temporal analysis into simple, readable queries. Master them, and you'll solve most time-based data problems elegantly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511cab1e",
   "metadata": {},
   "source": [
    "# ___________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bc8246",
   "metadata": {},
   "source": [
    "# Window Functions: first_value(), last_value(), and nth_value()\n",
    "\n",
    "These are **window functions** in SQL that allow you to access values from other rows within a defined \"window\" or partition of your result set, without collapsing the data like traditional aggregate functions would.\n",
    "\n",
    "**Core Purpose**: They retrieve specific positional values from an ordered set of rows while maintaining the granularity of your original dataset. This is crucial for comparative analysis, trend identification, and creating calculated fields that reference other rows in the same group.\n",
    "\n",
    "**Data Analysis Applications**:\n",
    "- **Baseline Comparisons**: Compare current values against first/last values in a time series\n",
    "- **Performance Benchmarking**: Reference top performers or initial states\n",
    "- **Gap Analysis**: Calculate differences from starting points or targets\n",
    "- **Cohort Analysis**: Track changes from initial user behavior\n",
    "- **Financial Analysis**: Compare against period starts, peaks, or specific benchmarks\n",
    "\n",
    "## SQL Examples: Simple to Advanced\n",
    "\n",
    "### Basic Usage\n",
    "```sql\n",
    "-- Simple: Get first and last salary in each department\n",
    "SELECT \n",
    "    employee_id,\n",
    "    department,\n",
    "    salary,\n",
    "    FIRST_VALUE(salary) OVER (PARTITION BY department ORDER BY salary DESC) as highest_salary,\n",
    "    LAST_VALUE(salary) OVER (PARTITION BY department ORDER BY salary DESC \n",
    "                           ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as lowest_salary\n",
    "FROM employees;\n",
    "```\n",
    "\n",
    "### Time Series Analysis\n",
    "```sql\n",
    "-- Compare monthly sales to first month and previous month\n",
    "SELECT \n",
    "    month,\n",
    "    sales,\n",
    "    FIRST_VALUE(sales) OVER (ORDER BY month) as baseline_sales,\n",
    "    sales - FIRST_VALUE(sales) OVER (ORDER BY month) as growth_from_start,\n",
    "    NTH_VALUE(sales, 2) OVER (ORDER BY month ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as second_month_sales\n",
    "FROM monthly_sales\n",
    "ORDER BY month;\n",
    "```\n",
    "\n",
    "### Advanced: Customer Journey Analysis\n",
    "```sql\n",
    "-- Track customer behavior from first purchase\n",
    "WITH customer_orders AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        order_date,\n",
    "        order_value,\n",
    "        ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_date) as order_sequence\n",
    "    FROM orders\n",
    ")\n",
    "SELECT \n",
    "    customer_id,\n",
    "    order_date,\n",
    "    order_value,\n",
    "    FIRST_VALUE(order_value) OVER (PARTITION BY customer_id ORDER BY order_date) as first_order_value,\n",
    "    LAST_VALUE(order_value) OVER (PARTITION BY customer_id ORDER BY order_date \n",
    "                                ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as latest_order_value,\n",
    "    NTH_VALUE(order_value, 3) OVER (PARTITION BY customer_id ORDER BY order_date) as third_order_value,\n",
    "    order_value / FIRST_VALUE(order_value) OVER (PARTITION BY customer_id ORDER BY order_date) as value_ratio_to_first\n",
    "FROM customer_orders;\n",
    "```\n",
    "\n",
    "## Python Parallels\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# SQL FIRST_VALUE/LAST_VALUE equivalent\n",
    "df['first_salary'] = df.groupby('department')['salary'].transform('first')\n",
    "df['last_salary'] = df.groupby('department')['salary'].transform('last')\n",
    "\n",
    "# More precise control with sort\n",
    "df_sorted = df.sort_values(['department', 'salary'], ascending=[True, False])\n",
    "df_sorted['highest_salary'] = df_sorted.groupby('department')['salary'].transform('first')\n",
    "\n",
    "# NTH_VALUE equivalent\n",
    "df['third_value'] = df.groupby('department')['salary'].transform(lambda x: x.iloc[2] if len(x) > 2 else None)\n",
    "\n",
    "# Time series baseline comparison\n",
    "df['baseline_sales'] = df['sales'].iloc[0]  # First value\n",
    "df['growth_from_start'] = df['sales'] - df['baseline_sales']\n",
    "\n",
    "# Using shift for more complex window operations\n",
    "df['previous_month'] = df['sales'].shift(1)\n",
    "df['first_month'] = df.groupby('customer_id')['sales'].transform('first')\n",
    "```\n",
    "\n",
    "## Excel Equivalents\n",
    "\n",
    "```excel\n",
    "# FIRST_VALUE equivalent\n",
    "=INDEX($B$2:$B$10,1)  # First value in range\n",
    "=INDEX(B:B,MATCH(MIN(A:A),A:A,0))  # First value by date\n",
    "\n",
    "# LAST_VALUE equivalent  \n",
    "=INDEX($B$2:$B$10,COUNTA($B$2:$B$10))  # Last non-empty value\n",
    "=LOOKUP(2,1/(B:B<>\"\"),B:B)  # Last value alternative\n",
    "\n",
    "# NTH_VALUE equivalent\n",
    "=INDEX($B$2:$B$10,3)  # Third value\n",
    "\n",
    "# Dynamic with OFFSET\n",
    "=OFFSET($B$1,1,0)  # First value after header\n",
    "=OFFSET($B$1,COUNTA($B:$B)-1,0)  # Last value\n",
    "\n",
    "# Array formulas for grouped operations\n",
    "{=INDEX(B:B,SMALL(IF(A:A=D2,ROW(A:A)),1))}  # First value in group\n",
    "```\n",
    "\n",
    "## Why These Functions Matter\n",
    "\n",
    "**Versus Traditional Alternatives**:\n",
    "\n",
    "1. **Better than Self-Joins**: Self-joins to get first/last values create complex, performance-heavy queries. Window functions are cleaner and faster.\n",
    "\n",
    "2. **Superior to Subqueries**: Instead of correlated subqueries that execute repeatedly, window functions calculate once per partition.\n",
    "\n",
    "3. **More Flexible than GROUP BY**: Unlike aggregates that collapse data, these maintain row-level detail while adding comparative context.\n",
    "\n",
    "4. **Cleaner than Variable Assignments**: No need for complex variable logic or multiple query passes.\n",
    "\n",
    "**Key Advantages**:\n",
    "- **Performance**: Single pass through data vs multiple queries\n",
    "- **Readability**: Intent is clear and declarative\n",
    "- **Maintainability**: Less complex logic, fewer moving parts  \n",
    "- **Analytical Power**: Enable sophisticated comparisons without data restructuring\n",
    "\n",
    "These functions are essential for modern analytics because they bridge the gap between row-level detail and aggregate insights, enabling nuanced analysis that would otherwise require complex workarounds or multiple query steps. They're particularly powerful in time-series analysis, cohort studies, and any scenario where you need to reference \"anchor points\" within your data while preserving granularity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305f46d9",
   "metadata": {},
   "source": [
    "# ___________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cc5af1",
   "metadata": {},
   "source": [
    "# Custom Window Frames: ROWS vs RANGE\n",
    "\n",
    "Window frames define **which rows** within your partition are included in the calculation for each current row. The key difference is **how they count**:\n",
    "\n",
    "- **ROWS**: Counts by **physical row positions** (1st row, 2nd row, etc.)\n",
    "- **RANGE**: Counts by **logical value ranges** (all rows with same value, values within X units)\n",
    "\n",
    "## The Critical Difference\n",
    "\n",
    "**Input Data:**\n",
    "```\n",
    "date       | sales | day_number\n",
    "-----------|-------|------------\n",
    "2024-01-01 | 100   | 1\n",
    "2024-01-02 | 150   | 2  \n",
    "2024-01-03 | 150   | 3  -- Same value as day 2\n",
    "2024-01-04 | 200   | 4\n",
    "2024-01-05 | 180   | 5\n",
    "```\n",
    "\n",
    "## ROWS Frame Examples\n",
    "\n",
    "**ROWS**: Physical position counting\n",
    "```sql\n",
    "SELECT \n",
    "    date,\n",
    "    sales,\n",
    "    -- Last 3 physical rows (including current)\n",
    "    SUM(sales) OVER (ORDER BY date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) as rolling_3day_sum,\n",
    "    -- Next 2 physical rows  \n",
    "    AVG(sales) OVER (ORDER BY date ROWS BETWEEN CURRENT ROW AND 2 FOLLOWING) as next_2day_avg\n",
    "FROM daily_sales;\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "date       | sales | rolling_3day_sum | next_2day_avg\n",
    "-----------|-------|------------------|---------------\n",
    "2024-01-01 | 100   | 100             | 133.33  -- (100+150+150)/3\n",
    "2024-01-02 | 150   | 250             | 166.67  -- (150+150+200)/3  \n",
    "2024-01-03 | 150   | 400             | 176.67  -- (150+200+180)/3\n",
    "2024-01-04 | 200   | 500             | 190     -- (200+180)/2\n",
    "2024-01-05 | 180   | 530             | 180     -- just current row\n",
    "```\n",
    "\n",
    "## RANGE Frame Examples\n",
    "\n",
    "**RANGE**: Logical value counting\n",
    "```sql\n",
    "SELECT \n",
    "    date,\n",
    "    sales,\n",
    "    -- All rows with sales values within 50 of current row's value\n",
    "    COUNT(*) OVER (ORDER BY sales RANGE BETWEEN 50 PRECEDING AND 50 FOLLOWING) as similar_sales_count,\n",
    "    -- All rows with same sales value\n",
    "    COUNT(*) OVER (ORDER BY sales RANGE BETWEEN CURRENT ROW AND CURRENT ROW) as same_value_count\n",
    "FROM daily_sales;\n",
    "```\n",
    "\n",
    "**Output (ordered by sales):**\n",
    "```\n",
    "sales | similar_sales_count | same_value_count\n",
    "------|-------------------|------------------\n",
    "100   | 2                 | 1    -- Within 50: 100,150,150\n",
    "150   | 4                 | 2    -- Within 50: 100,150,150,200  \n",
    "150   | 4                 | 2    -- Same as above (both 150s get same result)\n",
    "180   | 2                 | 1    -- Within 50: 150,150,180,200 but only 180,200 in range\n",
    "200   | 2                 | 1    -- Within 50: 150,150,180,200 but only 180,200 in range\n",
    "```\n",
    "\n",
    "## Advanced Examples\n",
    "\n",
    "### Time-Based Analysis: ROWS vs RANGE\n",
    "\n",
    "**Input Data:**\n",
    "```\n",
    "date       | sales\n",
    "-----------|------\n",
    "2024-01-01 | 100\n",
    "2024-01-03 | 200  -- Skipped day 2\n",
    "2024-01-04 | 150\n",
    "2024-01-07 | 300  -- Skipped days 5,6\n",
    "2024-01-08 | 250\n",
    "```\n",
    "\n",
    "**ROWS Frame (Physical):**\n",
    "```sql\n",
    "SELECT \n",
    "    date,\n",
    "    sales,\n",
    "    -- Last 3 physical rows\n",
    "    AVG(sales) OVER (ORDER BY date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) as avg_last_3_records\n",
    "FROM sales;\n",
    "```\n",
    "\n",
    "**RANGE Frame (Logical):**\n",
    "```sql\n",
    "SELECT \n",
    "    date,\n",
    "    sales,\n",
    "    -- Last 3 days of data (even with gaps)\n",
    "    AVG(sales) OVER (ORDER BY date RANGE BETWEEN INTERVAL '2 days' PRECEDING AND CURRENT ROW) as avg_last_3_days\n",
    "FROM sales;\n",
    "```\n",
    "\n",
    "**Comparison Output:**\n",
    "```\n",
    "date       | sales | avg_last_3_records | avg_last_3_days\n",
    "-----------|-------|-------------------|------------------\n",
    "2024-01-01 | 100   | 100               | 100      -- Only 1 row\n",
    "2024-01-03 | 200   | 150               | 150      -- 2 rows within 3 days\n",
    "2024-01-04 | 150   | 150               | 175      -- 3 rows: (200+150+150)/3  \n",
    "2024-01-07 | 300   | 217               | 300      -- Only current (others outside 3-day window)\n",
    "2024-01-08 | 250   | 233               | 275      -- 2 rows: (300+250)/2\n",
    "```\n",
    "\n",
    "## Python Equivalents\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# ROWS equivalent - physical positions\n",
    "df['rolling_3_rows'] = df['sales'].rolling(window=3, min_periods=1).sum()\n",
    "\n",
    "# RANGE equivalent - time-based windows  \n",
    "df_indexed = df.set_index('date')\n",
    "df_indexed['rolling_3_days'] = df_indexed['sales'].rolling('3D', min_periods=1).mean()\n",
    "\n",
    "# Custom RANGE logic - value-based windows\n",
    "def value_based_window(series, current_idx, range_val):\n",
    "    current_val = series.iloc[current_idx]\n",
    "    mask = (series >= current_val - range_val) & (series <= current_val + range_val)\n",
    "    return series[mask].count()\n",
    "\n",
    "df['similar_values'] = [value_based_window(df['sales'], i, 50) for i in range(len(df))]\n",
    "```\n",
    "\n",
    "## Excel Equivalents\n",
    "\n",
    "```excel\n",
    "# ROWS equivalent (physical positions)\n",
    "=AVERAGE(OFFSET(B2,-2,0,MIN(ROW(B2)-1,3),1))  # Last 3 rows\n",
    "\n",
    "# RANGE equivalent (date-based)  \n",
    "=AVERAGEIFS(B:B,A:A,\">=\"&A2-2,A:A,\"<=\"&A2)    # Within 2 days\n",
    "\n",
    "# Value-based RANGE\n",
    "=COUNTIFS(B:B,\">=\"&B2-50,B:B,\"<=\"&B2+50)      # Values within ±50\n",
    "```\n",
    "\n",
    "## When to Use Each\n",
    "\n",
    "### Use ROWS when:\n",
    "- **Fixed number of records**: \"Last 5 transactions\"\n",
    "- **Sequential analysis**: Moving averages over fixed periods\n",
    "- **Physical positioning matters**: \"Compare with previous 3 entries\"\n",
    "\n",
    "```sql\n",
    "-- Moving average over last 5 sales records\n",
    "SELECT \n",
    "    date,\n",
    "    sales,\n",
    "    AVG(sales) OVER (ORDER BY date ROWS BETWEEN 4 PRECEDING AND CURRENT ROW) as ma_5_records\n",
    "FROM sales;\n",
    "```\n",
    "\n",
    "### Use RANGE when:\n",
    "- **Time-based windows**: \"Last 30 days\" (regardless of record count)\n",
    "- **Value-based analysis**: \"All products within $100 price range\"  \n",
    "- **Logical groupings**: \"Same category or similar values\"\n",
    "\n",
    "```sql\n",
    "-- Average sales within last 30 days\n",
    "SELECT \n",
    "    date,\n",
    "    sales,\n",
    "    AVG(sales) OVER (ORDER BY date RANGE BETWEEN INTERVAL '30 days' PRECEDING AND CURRENT ROW) as avg_30_days\n",
    "FROM sales;\n",
    "```\n",
    "\n",
    "## Key Advantages\n",
    "\n",
    "**ROWS Advantages**:\n",
    "- **Predictable size**: Always includes exactly N rows (when available)\n",
    "- **Performance**: Simpler calculation, often faster\n",
    "- **Consistent**: Same logic regardless of data distribution\n",
    "\n",
    "**RANGE Advantages**:\n",
    "- **Business logic**: Matches real-world time periods or value ranges\n",
    "- **Handles gaps**: Automatically accounts for missing dates/values\n",
    "- **Flexible**: Adapts to data density variations\n",
    "\n",
    "The choice between ROWS and RANGE fundamentally depends on whether you're thinking in terms of **record count** (ROWS) or **business logic** (RANGE). Most financial and time-series analysis benefits from RANGE, while sequential record analysis often needs ROWS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372a9130",
   "metadata": {},
   "source": [
    "# ___________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927e7a29",
   "metadata": {},
   "source": [
    "# Partitioning Strategies and Performance\n",
    "\n",
    "**Partitioning** in window functions determines **how your data is divided into separate groups** before applying the window function. It's the difference between analyzing your entire dataset as one unit versus breaking it into logical segments.\n",
    "\n",
    "**Core Purpose**: Partitioning allows you to perform calculations within meaningful business groups while maintaining row-level detail. It's crucial for comparative analysis where context matters - comparing sales reps within their regions, not globally.\n",
    "\n",
    "**Performance Impact**: Proper partitioning can dramatically improve query performance by reducing the working set size for each calculation and enabling better use of indexes and parallel processing.\n",
    "\n",
    "# Window Function Partitioning Examples\n",
    "\n",
    "## Example Dataset\n",
    "```\n",
    "employee_id | department | region | salary | hire_date\n",
    "------------|------------|--------|--------|----------\n",
    "101         | Sales      | North  | 65000  | 2023-01-15\n",
    "102         | Sales      | North  | 72000  | 2023-03-20\n",
    "103         | Sales      | South  | 58000  | 2023-02-10\n",
    "104         | Marketing  | North  | 68000  | 2023-01-25\n",
    "105         | Marketing  | South  | 71000  | 2023-04-12\n",
    "106         | Engineering| North  | 85000  | 2023-02-28\n",
    "107         | Engineering| South  | 90000  | 2023-03-15\n",
    "108         | Sales      | South  | 62000  | 2023-05-08\n",
    "```\n",
    "\n",
    "## Partitioning Strategy Examples\n",
    "\n",
    "### 1. No Partitioning (Global Analysis)\n",
    "```sql\n",
    "SELECT \n",
    "    employee_id,\n",
    "    department,\n",
    "    salary,\n",
    "    -- Global ranking across all employees\n",
    "    ROW_NUMBER() OVER (ORDER BY salary DESC) as global_rank,\n",
    "    -- Global salary percentile\n",
    "    PERCENT_RANK() OVER (ORDER BY salary) as global_percentile\n",
    "FROM employees;\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "employee_id | department  | salary | global_rank | global_percentile\n",
    "------------|-------------|--------|-------------|------------------\n",
    "107         | Engineering | 90000  | 1           | 1.00\n",
    "106         | Engineering | 85000  | 2           | 0.86\n",
    "102         | Sales       | 72000  | 3           | 0.71\n",
    "105         | Marketing   | 71000  | 4           | 0.57\n",
    "104         | Marketing   | 68000  | 5           | 0.43\n",
    "101         | Sales       | 65000  | 6           | 0.29\n",
    "108         | Sales       | 62000  | 7           | 0.14\n",
    "103         | Sales       | 58000  | 8           | 0.00\n",
    "```\n",
    "\n",
    "### 2. Single Column Partitioning\n",
    "```sql\n",
    "SELECT \n",
    "    employee_id,\n",
    "    department,\n",
    "    salary,\n",
    "    -- Ranking within each department\n",
    "    ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as dept_rank,\n",
    "    -- Department salary percentile\n",
    "    PERCENT_RANK() OVER (PARTITION BY department ORDER BY salary) as dept_percentile,\n",
    "    -- Average salary in department\n",
    "    AVG(salary) OVER (PARTITION BY department) as dept_avg_salary\n",
    "FROM employees;\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "employee_id | department  | salary | dept_rank | dept_percentile | dept_avg_salary\n",
    "------------|-------------|--------|-----------|-----------------|----------------\n",
    "107         | Engineering | 90000  | 1         | 1.00            | 87500\n",
    "106         | Engineering | 85000  | 2         | 0.00            | 87500\n",
    "105         | Marketing   | 71000  | 1         | 1.00            | 69500\n",
    "104         | Marketing   | 68000  | 2         | 0.00            | 69500\n",
    "102         | Sales       | 72000  | 1         | 1.00            | 64250\n",
    "101         | Sales       | 65000  | 2         | 0.67            | 64250\n",
    "108         | Sales       | 62000  | 3         | 0.33            | 64250\n",
    "103         | Sales       | 58000  | 4         | 0.00            | 64250\n",
    "```\n",
    "\n",
    "### 3. Multi-Column Partitioning\n",
    "```sql\n",
    "SELECT \n",
    "    employee_id,\n",
    "    department,\n",
    "    region,\n",
    "    salary,\n",
    "    -- Ranking within department AND region\n",
    "    ROW_NUMBER() OVER (PARTITION BY department, region ORDER BY salary DESC) as dept_region_rank,\n",
    "    -- Count of employees in same dept/region\n",
    "    COUNT(*) OVER (PARTITION BY department, region) as group_size,\n",
    "    -- Salary difference from department/region average\n",
    "    salary - AVG(salary) OVER (PARTITION BY department, region) as diff_from_group_avg\n",
    "FROM employees;\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "employee_id | department  | region | salary | dept_region_rank | group_size | diff_from_group_avg\n",
    "------------|-------------|--------|--------|------------------|------------|--------------------\n",
    "106         | Engineering | North  | 85000  | 1                | 1          | 0\n",
    "107         | Engineering | South  | 90000  | 1                | 1          | 0\n",
    "104         | Marketing   | North  | 68000  | 1                | 1          | 0\n",
    "105         | Marketing   | South  | 71000  | 1                | 1          | 0\n",
    "102         | Sales       | North  | 72000  | 1                | 1          | 0\n",
    "101         | Sales       | South  | 65000  | 1                | 3          | 3333.33\n",
    "108         | Sales       | South  | 62000  | 2                | 3          | 333.33\n",
    "103         | Sales       | South  | 58000  | 3                | 3          | -3666.67\n",
    "```\n",
    "\n",
    "## Performance Optimization Strategies\n",
    "\n",
    "### 1. Index-Aligned Partitioning\n",
    "```sql\n",
    "-- BAD: Partitioning doesn't match available indexes\n",
    "SELECT \n",
    "    product_id,\n",
    "    sale_date,\n",
    "    amount,\n",
    "    SUM(amount) OVER (PARTITION BY EXTRACT(MONTH FROM sale_date) ORDER BY sale_date) as monthly_running_total\n",
    "FROM sales; -- Index on (product_id, sale_date)\n",
    "\n",
    "-- GOOD: Partitioning aligns with index\n",
    "SELECT \n",
    "    product_id,\n",
    "    sale_date,\n",
    "    amount,\n",
    "    SUM(amount) OVER (PARTITION BY product_id ORDER BY sale_date) as product_running_total\n",
    "FROM sales; -- Index on (product_id, sale_date) can be used efficiently\n",
    "```\n",
    "\n",
    "### 2. Cardinality-Aware Partitioning\n",
    "```sql\n",
    "-- BAD: High cardinality partitioning (many small groups)\n",
    "SELECT \n",
    "    customer_id,\n",
    "    order_date,\n",
    "    amount,\n",
    "    ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_date) as customer_order_sequence\n",
    "FROM orders; -- 1 million customers, 5 million orders = tiny partitions\n",
    "\n",
    "-- BETTER: Lower cardinality partitioning (fewer, larger groups)\n",
    "SELECT \n",
    "    customer_id,\n",
    "    order_date,\n",
    "    amount,\n",
    "    ROW_NUMBER() OVER (PARTITION BY customer_segment ORDER BY order_date) as segment_order_sequence\n",
    "FROM orders \n",
    "JOIN customers USING (customer_id); -- 5 segments = larger, more efficient partitions\n",
    "```\n",
    "\n",
    "### 3. Memory-Efficient Partitioning\n",
    "```sql\n",
    "-- Consider partition size for memory usage\n",
    "SELECT \n",
    "    transaction_id,\n",
    "    account_id,\n",
    "    transaction_date,\n",
    "    amount,\n",
    "    -- This could create huge partitions for active accounts\n",
    "    SUM(amount) OVER (PARTITION BY account_id ORDER BY transaction_date) as running_balance,\n",
    "    -- This creates smaller, more manageable partitions\n",
    "    SUM(amount) OVER (PARTITION BY account_id, EXTRACT(YEAR FROM transaction_date) ORDER BY transaction_date) as yearly_running_balance\n",
    "FROM transactions;\n",
    "```\n",
    "\n",
    "## Advanced Partitioning Patterns\n",
    "\n",
    "### 1. Hierarchical Partitioning\n",
    "```sql\n",
    "-- Multi-level business hierarchy analysis\n",
    "WITH sales_analysis AS (\n",
    "    SELECT \n",
    "        rep_id,\n",
    "        region,\n",
    "        country,\n",
    "        quarter,\n",
    "        sales_amount,\n",
    "        -- Global performance\n",
    "        PERCENT_RANK() OVER (ORDER BY sales_amount DESC) as global_percentile,\n",
    "        -- Country performance  \n",
    "        PERCENT_RANK() OVER (PARTITION BY country ORDER BY sales_amount DESC) as country_percentile,\n",
    "        -- Regional performance\n",
    "        PERCENT_RANK() OVER (PARTITION BY region ORDER BY sales_amount DESC) as region_percentile\n",
    "    FROM quarterly_sales\n",
    ")\n",
    "SELECT \n",
    "    *,\n",
    "    CASE \n",
    "        WHEN global_percentile >= 0.9 THEN 'Global Top 10%'\n",
    "        WHEN country_percentile >= 0.8 THEN 'Country Top 20%' \n",
    "        WHEN region_percentile >= 0.7 THEN 'Region Top 30%'\n",
    "        ELSE 'Standard'\n",
    "    END as performance_tier\n",
    "FROM sales_analysis;\n",
    "```\n",
    "\n",
    "### 2. Dynamic Partitioning\n",
    "```sql\n",
    "-- Partition strategy changes based on data characteristics\n",
    "SELECT \n",
    "    product_id,\n",
    "    sale_date,\n",
    "    quantity,\n",
    "    -- For high-volume products: monthly partitions\n",
    "    CASE \n",
    "        WHEN product_category = 'Electronics' \n",
    "        THEN SUM(quantity) OVER (PARTITION BY product_id, EXTRACT(YEAR_MONTH FROM sale_date) ORDER BY sale_date)\n",
    "        -- For low-volume products: yearly partitions  \n",
    "        ELSE SUM(quantity) OVER (PARTITION BY product_id, EXTRACT(YEAR FROM sale_date) ORDER BY sale_date)\n",
    "    END as running_total\n",
    "FROM product_sales ps\n",
    "JOIN products p USING (product_id);\n",
    "```\n",
    "\n",
    "## Performance Monitoring Queries\n",
    "\n",
    "### 1. Partition Size Analysis\n",
    "```sql\n",
    "-- Analyze partition sizes to optimize performance\n",
    "WITH partition_stats AS (\n",
    "    SELECT \n",
    "        department,\n",
    "        region,\n",
    "        COUNT(*) as partition_size,\n",
    "        AVG(salary) as avg_salary,\n",
    "        COUNT(*) OVER () as total_rows\n",
    "    FROM employees\n",
    "    GROUP BY department, region\n",
    ")\n",
    "SELECT \n",
    "    department,\n",
    "    region,\n",
    "    partition_size,\n",
    "    partition_size * 100.0 / total_rows as pct_of_total,\n",
    "    CASE \n",
    "        WHEN partition_size < 10 THEN 'Too Small - Consider Merging'\n",
    "        WHEN partition_size > 10000 THEN 'Too Large - Consider Splitting'\n",
    "        ELSE 'Optimal Size'\n",
    "    END as size_assessment\n",
    "FROM partition_stats\n",
    "ORDER BY partition_size DESC;\n",
    "```\n",
    "\n",
    "## Basic Partitioning Concepts## Python Partitioning Equivalents\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Single column partitioning\n",
    "df['dept_rank'] = df.groupby('department')['salary'].rank(method='dense', ascending=False)\n",
    "df['dept_percentile'] = df.groupby('department')['salary'].rank(pct=True)\n",
    "\n",
    "# Multi-column partitioning  \n",
    "df['dept_region_rank'] = df.groupby(['department', 'region'])['salary'].rank(ascending=False)\n",
    "df['group_avg'] = df.groupby(['department', 'region'])['salary'].transform('mean')\n",
    "\n",
    "# Performance-optimized grouping\n",
    "# Instead of high-cardinality grouping:\n",
    "# df.groupby('customer_id')['amount'].sum()  # Many small groups\n",
    "\n",
    "# Use lower cardinality:\n",
    "df.groupby('customer_segment')['amount'].sum()  # Fewer, larger groups\n",
    "\n",
    "# Hierarchical analysis\n",
    "df['global_rank'] = df['sales'].rank(pct=True, ascending=False)\n",
    "df['country_rank'] = df.groupby('country')['sales'].rank(pct=True, ascending=False)\n",
    "df['region_rank'] = df.groupby('region')['sales'].rank(pct=True, ascending=False)\n",
    "```\n",
    "\n",
    "## Excel Partitioning Patterns\n",
    "\n",
    "```excel\n",
    "# Department-based partitioning using array formulas\n",
    "=RANK(C2,IF($B$2:$B$100=B2,$C$2:$C$100),0)  # Rank within department\n",
    "\n",
    "# Multi-level partitioning\n",
    "=RANK(D2,IF(($B$2:$B$100=B2)*($C$2:$C$100=C2),$D$2:$D$100),0)  # Rank within dept AND region\n",
    "\n",
    "# Performance consideration - use structured references\n",
    "=RANK([@Salary],IF((Table1[Department]=[@Department])*(Table1[Region]=[@Region]),Table1[Salary]),0)\n",
    "```\n",
    "\n",
    "## Performance Best Practices\n",
    "\n",
    "### 1. **Choose Appropriate Cardinality**\n",
    "- **Low cardinality** (10-1000 groups): Better performance, more parallelization\n",
    "- **High cardinality** (100k+ groups): Memory intensive, slower processing\n",
    "- **Sweet spot**: 100-10,000 groups for most analyses\n",
    "\n",
    "### 2. **Align with Indexes**\n",
    "```sql\n",
    "-- If you have index on (customer_id, order_date)\n",
    "-- GOOD: Partition by customer_id\n",
    "PARTITION BY customer_id ORDER BY order_date\n",
    "\n",
    "-- BAD: Partition by product_id (not in index)  \n",
    "PARTITION BY product_id ORDER BY order_date\n",
    "```\n",
    "\n",
    "### 3. **Consider Data Distribution**\n",
    "```sql\n",
    "-- Check partition sizes before implementing\n",
    "SELECT \n",
    "    partition_column,\n",
    "    COUNT(*) as partition_size,\n",
    "    COUNT(*) * 100.0 / SUM(COUNT(*)) OVER () as pct_of_data\n",
    "FROM your_table\n",
    "GROUP BY partition_column\n",
    "HAVING COUNT(*) > 1000  -- Flag large partitions\n",
    "ORDER BY COUNT(*) DESC;\n",
    "```\n",
    "\n",
    "### 4. **Memory Management**\n",
    "- **Large partitions**: May cause memory pressure\n",
    "- **Many small partitions**: Overhead from context switching\n",
    "- **Optimal**: Partitions with 100-10,000 rows each\n",
    "\n",
    "## Why Partitioning Strategy Matters\n",
    "\n",
    "**Versus No Partitioning**:\n",
    "- **Business Relevance**: Comparisons within meaningful groups\n",
    "- **Performance**: Smaller working sets, better parallelization\n",
    "- **Accuracy**: Context-appropriate analysis (regional vs global rankings)\n",
    "\n",
    "**Versus Poor Partitioning**:\n",
    "- **Query Performance**: 10-100x faster with proper partitioning\n",
    "- **Resource Usage**: Better memory and CPU utilization\n",
    "- **Scalability**: Handles larger datasets effectively\n",
    "\n",
    "**Key Advantages**:\n",
    "- **Parallel Processing**: Different partitions can be processed simultaneously\n",
    "- **Index Utilization**: Aligned partitioning leverages existing indexes\n",
    "- **Memory Efficiency**: Smaller partition sizes reduce memory requirements\n",
    "- **Business Logic**: Matches real-world analytical needs\n",
    "\n",
    "The right partitioning strategy can transform a slow, resource-intensive query into a fast, efficient analysis. Always consider your data's cardinality, distribution, and business context when choosing partition columns. Start with business logic needs, then optimize for performance based on your specific data characteristics and infrastructure capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b89b56e",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320cb3d2",
   "metadata": {},
   "source": [
    "# ___________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e96734",
   "metadata": {},
   "source": [
    "# Self Joins for Hierarchical Data\n",
    "\n",
    "**Self joins** occur when a table is joined to itself, typically used to navigate **hierarchical relationships** where parent-child relationships exist within the same table. This is essential for organizational charts, category trees, geographical hierarchies, and any data structure where entities reference other entities in the same dataset.\n",
    "\n",
    "**Core Purpose**: Self joins allow you to traverse hierarchical relationships, compare related records, and flatten tree structures into relational results. They're crucial for reporting chains, product categories, geographical rollups, and any scenario where you need to \"walk up or down\" a hierarchy.\n",
    "\n",
    "**Data Analysis Applications**:\n",
    "- **Organizational Analysis**: Employee-manager relationships, reporting structures\n",
    "- **Product Hierarchies**: Category trees, part-subpart relationships  \n",
    "- **Geographic Rollups**: City→State→Country, Territory→Region→Division\n",
    "- **Time Series**: Period-over-period comparisons, trend analysis\n",
    "- **Network Analysis**: Social connections, referral chains\n",
    "\n",
    "## Basic Hierarchical Structure\n",
    "\n",
    "**Input Data - Employee Hierarchy:**\n",
    "```\n",
    "employee_id | employee_name | manager_id | salary | department\n",
    "------------|---------------|------------|--------|------------\n",
    "1           | John CEO      | NULL       | 200000 | Executive\n",
    "2           | Sarah VP      | 1          | 150000 | Sales\n",
    "3           | Mike VP       | 1          | 145000 | Engineering  \n",
    "4           | Lisa MGR      | 2          | 90000  | Sales\n",
    "5           | Tom MGR       | 2          | 85000  | Sales\n",
    "6           | Anna MGR      | 3          | 95000  | Engineering\n",
    "7           | Bob REP       | 4          | 65000  | Sales\n",
    "8           | Carol REP     | 4          | 70000  | Sales\n",
    "9           | Dave DEV      | 6          | 80000  | Engineering\n",
    "10          | Eve DEV       | 6          | 75000  | Engineering\n",
    "```\n",
    "\n",
    "## Simple to Advanced Examples\n",
    "\n",
    "### 1. Basic Manager-Employee Relationships\n",
    "```sql\n",
    "-- Find each employee with their direct manager\n",
    "SELECT \n",
    "    e.employee_id,\n",
    "    e.employee_name as employee,\n",
    "    e.salary as employee_salary,\n",
    "    m.employee_name as manager,\n",
    "    m.salary as manager_salary\n",
    "FROM employees e\n",
    "LEFT JOIN employees m ON e.manager_id = m.employee_id\n",
    "ORDER BY e.employee_id;\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "employee_id | employee  | employee_salary | manager  | manager_salary\n",
    "------------|-----------|-----------------|----------|---------------\n",
    "1           | John CEO  | 200000         | NULL     | NULL\n",
    "2           | Sarah VP  | 150000         | John CEO | 200000\n",
    "3           | Mike VP   | 145000         | John CEO | 200000\n",
    "4           | Lisa MGR  | 90000          | Sarah VP | 150000\n",
    "5           | Tom MGR   | 85000          | Sarah VP | 150000\n",
    "6           | Anna MGR  | 95000          | Mike VP  | 145000\n",
    "7           | Bob REP   | 65000          | Lisa MGR | 90000\n",
    "8           | Carol REP | 70000          | Lisa MGR | 90000\n",
    "9           | Dave DEV  | 80000          | Anna MGR | 95000\n",
    "10          | Eve DEV   | 75000          | Anna MGR | 95000\n",
    "```\n",
    "\n",
    "### 2. Multi-Level Hierarchy Navigation\n",
    "```sql\n",
    "-- Show employee with their manager and their manager's manager (3 levels)\n",
    "SELECT \n",
    "    e.employee_name as employee,\n",
    "    e.salary,\n",
    "    m1.employee_name as direct_manager,\n",
    "    m2.employee_name as senior_manager,\n",
    "    m3.employee_name as executive\n",
    "FROM employees e\n",
    "LEFT JOIN employees m1 ON e.manager_id = m1.employee_id\n",
    "LEFT JOIN employees m2 ON m1.manager_id = m2.employee_id  \n",
    "LEFT JOIN employees m3 ON m2.manager_id = m3.employee_id\n",
    "WHERE e.manager_id IS NOT NULL  -- Exclude CEO\n",
    "ORDER BY e.employee_id;\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "employee     | salary | direct_manager | senior_manager | executive\n",
    "-------------|--------|----------------|----------------|----------\n",
    "Sarah VP     | 150000 | John CEO       | NULL           | NULL\n",
    "Mike VP      | 145000 | John CEO       | NULL           | NULL\n",
    "Lisa MGR     | 90000  | Sarah VP       | John CEO       | NULL\n",
    "Tom MGR      | 85000  | Sarah VP       | John CEO       | NULL\n",
    "Anna MGR     | 95000  | Mike VP        | John CEO       | NULL\n",
    "Bob REP      | 65000  | Lisa MGR       | Sarah VP       | John CEO\n",
    "Carol REP    | 70000  | Lisa MGR       | Sarah VP       | John CEO\n",
    "Dave DEV     | 80000  | Anna MGR       | Mike VP        | John CEO\n",
    "Eve DEV      | 75000  | Anna MGR       | Mike VP        | John CEO\n",
    "```\n",
    "\n",
    "### 3. Advanced: Hierarchical Aggregations\n",
    "```sql\n",
    "-- Calculate team sizes and total salary costs by manager\n",
    "WITH manager_stats AS (\n",
    "    SELECT \n",
    "        m.employee_id as manager_id,\n",
    "        m.employee_name as manager_name,\n",
    "        COUNT(e.employee_id) as direct_reports,\n",
    "        SUM(e.salary) as team_salary_cost,\n",
    "        AVG(e.salary) as avg_team_salary,\n",
    "        MAX(e.salary) as highest_team_salary,\n",
    "        MIN(e.salary) as lowest_team_salary\n",
    "    FROM employees m\n",
    "    INNER JOIN employees e ON m.employee_id = e.manager_id\n",
    "    GROUP BY m.employee_id, m.employee_name\n",
    ")\n",
    "SELECT \n",
    "    manager_name,\n",
    "    direct_reports,\n",
    "    team_salary_cost,\n",
    "    avg_team_salary,\n",
    "    highest_team_salary - lowest_team_salary as salary_range,\n",
    "    team_salary_cost * 100.0 / SUM(team_salary_cost) OVER () as pct_of_total_cost\n",
    "FROM manager_stats\n",
    "ORDER BY team_salary_cost DESC;\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "manager_name | direct_reports | team_salary_cost | avg_team_salary | salary_range | pct_of_total_cost\n",
    "-------------|----------------|------------------|-----------------|--------------|------------------\n",
    "John CEO     | 2              | 295000          | 147500          | 5000         | 36.8%\n",
    "Sarah VP     | 2              | 175000          | 87500           | 5000         | 21.8%\n",
    "Lisa MGR     | 2              | 135000          | 67500           | 5000         | 16.8%\n",
    "Anna MGR     | 2              | 155000          | 77500           | 5000         | 19.3%\n",
    "Mike VP      | 1              | 95000           | 95000           | 0            | 11.8%\n",
    "```\n",
    "\n",
    "### 4. Complex: Recursive Hierarchy with Path Tracking\n",
    "```sql\n",
    "-- Find all subordinates for each manager (direct and indirect)\n",
    "WITH RECURSIVE hierarchy_path AS (\n",
    "    -- Base case: Direct manager-employee relationships\n",
    "    SELECT \n",
    "        manager_id,\n",
    "        employee_id,\n",
    "        employee_name,\n",
    "        salary,\n",
    "        1 as level,\n",
    "        CAST(employee_name AS VARCHAR(1000)) as path\n",
    "    FROM employees \n",
    "    WHERE manager_id IS NOT NULL\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    -- Recursive case: Find indirect reports\n",
    "    SELECT \n",
    "        hp.manager_id,\n",
    "        e.employee_id,\n",
    "        e.employee_name,\n",
    "        e.salary,\n",
    "        hp.level + 1,\n",
    "        hp.path || ' -> ' || e.employee_name\n",
    "    FROM hierarchy_path hp\n",
    "    JOIN employees e ON hp.employee_id = e.manager_id\n",
    ")\n",
    "SELECT \n",
    "    m.employee_name as manager,\n",
    "    COUNT(*) as total_subordinates,\n",
    "    SUM(hp.salary) as total_team_cost,\n",
    "    STRING_AGG(hp.employee_name || ' (L' || hp.level || ')', ', ') as all_reports\n",
    "FROM employees m\n",
    "JOIN hierarchy_path hp ON m.employee_id = hp.manager_id\n",
    "GROUP BY m.employee_id, m.employee_name\n",
    "ORDER BY total_subordinates DESC;\n",
    "```\n",
    "\n",
    "## Python Equivalents\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# Basic manager-employee join\n",
    "df_with_managers = pd.merge(\n",
    "    employees, \n",
    "    employees[['employee_id', 'employee_name', 'salary']], \n",
    "    left_on='manager_id', \n",
    "    right_on='employee_id', \n",
    "    suffixes=('', '_manager'),\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Multi-level hierarchy\n",
    "def get_hierarchy_levels(df, employee_col='employee_id', manager_col='manager_id', levels=3):\n",
    "    result = df.copy()\n",
    "    current_df = df\n",
    "    \n",
    "    for level in range(1, levels + 1):\n",
    "        manager_info = df[['employee_id', 'employee_name']].rename(\n",
    "            columns={'employee_id': f'manager_id_l{level}', 'employee_name': f'manager_name_l{level}'}\n",
    "        )\n",
    "        \n",
    "        current_df = pd.merge(\n",
    "            current_df, \n",
    "            manager_info, \n",
    "            left_on='manager_id' if level == 1 else f'manager_id_l{level-1}',\n",
    "            right_on=f'manager_id_l{level}',\n",
    "            how='left'\n",
    "        )\n",
    "    \n",
    "    return current_df\n",
    "\n",
    "# Recursive hierarchy using NetworkX\n",
    "def build_org_chart(df):\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add edges (manager -> employee)\n",
    "    for _, row in df.iterrows():\n",
    "        if pd.notna(row['manager_id']):\n",
    "            G.add_edge(row['manager_id'], row['employee_id'])\n",
    "    \n",
    "    # Find all subordinates for each manager\n",
    "    subordinates = {}\n",
    "    for node in G.nodes():\n",
    "        subordinates[node] = list(nx.descendants(G, node))\n",
    "    \n",
    "    return subordinates\n",
    "\n",
    "# Team aggregations\n",
    "team_stats = employees.groupby('manager_id').agg({\n",
    "    'employee_id': 'count',\n",
    "    'salary': ['sum', 'mean', 'min', 'max']\n",
    "}).round(2)\n",
    "```\n",
    "\n",
    "## Excel Hierarchical Patterns\n",
    "\n",
    "```excel\n",
    "# Basic manager lookup\n",
    "=INDEX(employee_names, MATCH([@manager_id], employee_ids, 0))\n",
    "\n",
    "# Multi-level hierarchy (complex but possible)\n",
    "=INDEX(employee_names, MATCH(INDEX(manager_ids, MATCH([@manager_id], employee_ids, 0)), employee_ids, 0))\n",
    "\n",
    "# Team aggregations\n",
    "=SUMIF(manager_id_range, A2, salary_range)  # Total team salary\n",
    "=COUNTIF(manager_id_range, A2)             # Team size\n",
    "\n",
    "# Hierarchical path (requires helper columns)\n",
    "=IF([@manager_id]=\"\", [@employee_name], \n",
    "    INDEX(paths, MATCH([@manager_id], employee_ids, 0)) & \" -> \" & [@employee_name])\n",
    "```\n",
    "\n",
    "## Advanced Use Cases\n",
    "\n",
    "### 1. Product Category Hierarchy\n",
    "```sql\n",
    "-- Product category rollups\n",
    "WITH category_sales AS (\n",
    "    SELECT \n",
    "        c1.category_id,\n",
    "        c1.category_name,\n",
    "        c2.category_name as parent_category,\n",
    "        c3.category_name as grandparent_category,\n",
    "        SUM(s.sales_amount) as category_sales\n",
    "    FROM categories c1\n",
    "    LEFT JOIN categories c2 ON c1.parent_category_id = c2.category_id\n",
    "    LEFT JOIN categories c3 ON c2.parent_category_id = c3.category_id\n",
    "    LEFT JOIN sales s ON c1.category_id = s.category_id\n",
    "    GROUP BY c1.category_id, c1.category_name, c2.category_name, c3.category_name\n",
    ")\n",
    "SELECT \n",
    "    COALESCE(grandparent_category, parent_category, category_name) as top_level_category,\n",
    "    SUM(category_sales) as total_sales,\n",
    "    COUNT(*) as subcategories\n",
    "FROM category_sales\n",
    "GROUP BY COALESCE(grandparent_category, parent_category, category_name);\n",
    "```\n",
    "\n",
    "### 2. Geographic Territory Analysis\n",
    "```sql\n",
    "-- Territory hierarchy with sales rollups\n",
    "SELECT \n",
    "    t1.territory_name as territory,\n",
    "    t2.territory_name as region,\n",
    "    t3.territory_name as division,\n",
    "    COUNT(DISTINCT s.sales_rep_id) as rep_count,\n",
    "    SUM(s.sales_amount) as total_sales,\n",
    "    AVG(s.sales_amount) as avg_deal_size\n",
    "FROM territories t1\n",
    "LEFT JOIN territories t2 ON t1.parent_territory_id = t2.territory_id\n",
    "LEFT JOIN territories t3 ON t2.parent_territory_id = t3.territory_id\n",
    "LEFT JOIN sales s ON t1.territory_id = s.territory_id\n",
    "GROUP BY t1.territory_id, t1.territory_name, t2.territory_name, t3.territory_name\n",
    "HAVING SUM(s.sales_amount) > 100000\n",
    "ORDER BY total_sales DESC;\n",
    "```\n",
    "\n",
    "## Why Self Joins Excel Over Alternatives\n",
    "\n",
    "**Versus Recursive CTEs**:\n",
    "- **Simplicity**: Easier to understand and debug for fixed-depth hierarchies\n",
    "- **Performance**: Often faster for shallow hierarchies (2-4 levels)\n",
    "- **Compatibility**: Works across more database systems\n",
    "\n",
    "**Versus Application Logic**:\n",
    "- **Efficiency**: Single query vs multiple round trips\n",
    "- **Data Integrity**: Consistent view of hierarchy at query time\n",
    "- **Analytical Power**: Easy to combine with aggregations and window functions\n",
    "\n",
    "**Versus Materialized Paths**:\n",
    "- **Flexibility**: No need to maintain separate path columns\n",
    "- **Real-time**: Always reflects current hierarchy structure\n",
    "- **Storage**: No additional storage overhead\n",
    "\n",
    "**Key Advantages**:\n",
    "- **Intuitive**: Mirrors how we think about hierarchical relationships\n",
    "- **Flexible**: Can navigate up or down the hierarchy as needed\n",
    "- **Powerful**: Combines easily with aggregations and analytics\n",
    "- **Standard SQL**: Works across all major database platforms\n",
    "\n",
    "Self joins for hierarchical data are fundamental for organizational reporting, product analytics, and any domain with natural parent-child relationships. They provide the foundation for understanding structure, calculating rollups, and performing comparative analysis within hierarchical contexts. Master this pattern, and you'll handle everything from org charts to product categories with confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8094345b",
   "metadata": {},
   "source": [
    "# ___________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f67887",
   "metadata": {},
   "source": [
    "# Multiple Table Join Optimizations\n",
    "\n",
    "**Join optimization** is the art and science of efficiently combining data from multiple tables while minimizing resource consumption and query execution time. With multiple tables, the complexity grows exponentially - both in terms of possible execution plans and potential performance pitfalls.\n",
    "\n",
    "**Core Purpose**: Optimize how the database engine accesses, filters, and combines data across tables. Poor join strategies can turn millisecond queries into hour-long operations, while proper optimization makes complex multi-table analytics lightning-fast.\n",
    "\n",
    "**Performance Impact Areas**:\n",
    "- **Execution Plan Selection**: Guiding the optimizer toward efficient join orders\n",
    "- **Index Utilization**: Ensuring joins leverage existing indexes effectively  \n",
    "- **Memory Management**: Controlling working set sizes and temporary storage\n",
    "- **I/O Reduction**: Minimizing disk reads through smart filtering and ordering\n",
    "\n",
    "## Join Order Fundamentals\n",
    "# Multiple Table Join Optimization Examples\n",
    "\n",
    "## Sample Schema\n",
    "```sql\n",
    "-- Large tables\n",
    "orders (5M rows)      -- order_id, customer_id, order_date, status, total_amount\n",
    "customers (500K rows) -- customer_id, customer_name, segment, region, signup_date\n",
    "products (50K rows)   -- product_id, product_name, category_id, price, supplier_id\n",
    "\n",
    "-- Medium tables  \n",
    "order_items (15M rows) -- order_id, product_id, quantity, unit_price\n",
    "categories (500 rows)  -- category_id, category_name, parent_category_id\n",
    "suppliers (1K rows)   -- supplier_id, supplier_name, country, rating\n",
    "\n",
    "-- Small tables\n",
    "regions (50 rows)     -- region_id, region_name, country_code\n",
    "promotions (100 rows) -- promo_id, promo_code, discount_pct, start_date, end_date\n",
    "```\n",
    "\n",
    "## 1. Join Order Optimization\n",
    "\n",
    "### BAD: Large Table First, No Filtering\n",
    "```sql\n",
    "-- Inefficient: Starts with largest table, no early filtering\n",
    "SELECT \n",
    "    c.customer_name,\n",
    "    p.product_name,\n",
    "    oi.quantity,\n",
    "    o.order_date\n",
    "FROM order_items oi                    -- 15M rows - LARGEST FIRST (BAD)\n",
    "JOIN orders o ON oi.order_id = o.order_id\n",
    "JOIN customers c ON o.customer_id = c.customer_id\n",
    "JOIN products p ON oi.product_id = p.product_id\n",
    "WHERE o.order_date >= '2024-01-01'     -- Filter applied AFTER joins (BAD)\n",
    "  AND c.segment = 'Premium';           -- Another late filter (BAD)\n",
    "```\n",
    "\n",
    "### GOOD: Filtered Small Tables First\n",
    "```sql\n",
    "-- Efficient: Start with filtered small tables, build up\n",
    "SELECT \n",
    "    c.customer_name,\n",
    "    p.product_name,\n",
    "    oi.quantity,\n",
    "    o.order_date\n",
    "FROM customers c                        -- Start with customers (500K)\n",
    "JOIN orders o ON c.customer_id = o.customer_id\n",
    "JOIN order_items oi ON o.order_id = oi.order_id\n",
    "JOIN products p ON oi.product_id = p.product_id\n",
    "WHERE c.segment = 'Premium'             -- Filter early (reduces customer set)\n",
    "  AND o.order_date >= '2024-01-01'      -- Filter early (reduces order set)\n",
    "  AND o.status = 'completed';           -- Additional early filter\n",
    "```\n",
    "\n",
    "### OPTIMAL: Use CTEs for Complex Filtering\n",
    "```sql\n",
    "-- Best: Pre-filter tables, then join smaller result sets\n",
    "WITH premium_customers AS (\n",
    "    SELECT customer_id, customer_name \n",
    "    FROM customers \n",
    "    WHERE segment = 'Premium'           -- 50K customers instead of 500K\n",
    "),\n",
    "recent_orders AS (\n",
    "    SELECT order_id, customer_id, order_date\n",
    "    FROM orders \n",
    "    WHERE order_date >= '2024-01-01'    -- 1M orders instead of 5M\n",
    "      AND status = 'completed'\n",
    "),\n",
    "relevant_items AS (\n",
    "    SELECT oi.order_id, oi.product_id, oi.quantity\n",
    "    FROM order_items oi\n",
    "    WHERE EXISTS (\n",
    "        SELECT 1 FROM recent_orders ro \n",
    "        WHERE ro.order_id = oi.order_id -- Only items from recent orders\n",
    "    )\n",
    ")\n",
    "SELECT \n",
    "    pc.customer_name,\n",
    "    p.product_name,\n",
    "    ri.quantity,\n",
    "    ro.order_date\n",
    "FROM premium_customers pc\n",
    "JOIN recent_orders ro ON pc.customer_id = ro.customer_id\n",
    "JOIN relevant_items ri ON ro.order_id = ri.order_id\n",
    "JOIN products p ON ri.product_id = p.product_id;\n",
    "```\n",
    "\n",
    "## 2. Index-Driven Join Strategies\n",
    "\n",
    "### Covering Index Optimization\n",
    "```sql\n",
    "-- Create covering indexes for common join patterns\n",
    "CREATE INDEX idx_orders_customer_date_covering \n",
    "ON orders (customer_id, order_date) \n",
    "INCLUDE (order_id, status, total_amount);\n",
    "\n",
    "CREATE INDEX idx_order_items_order_product_covering\n",
    "ON order_items (order_id, product_id)\n",
    "INCLUDE (quantity, unit_price);\n",
    "\n",
    "-- Query that leverages covering indexes\n",
    "SELECT \n",
    "    o.order_id,\n",
    "    o.order_date,\n",
    "    o.total_amount,\n",
    "    SUM(oi.quantity * oi.unit_price) as calculated_total\n",
    "FROM orders o\n",
    "JOIN order_items oi ON o.order_id = oi.order_id\n",
    "WHERE o.customer_id = 12345\n",
    "  AND o.order_date >= '2024-01-01'\n",
    "GROUP BY o.order_id, o.order_date, o.total_amount;\n",
    "```\n",
    "\n",
    "### Composite Key Optimization\n",
    "```sql\n",
    "-- Optimize multi-column join conditions\n",
    "CREATE INDEX idx_composite_join \n",
    "ON order_items (order_id, product_id, quantity);\n",
    "\n",
    "-- Efficient query using composite index\n",
    "SELECT \n",
    "    o.order_date,\n",
    "    p.product_name,\n",
    "    oi.quantity\n",
    "FROM orders o\n",
    "JOIN order_items oi ON o.order_id = oi.order_id\n",
    "JOIN products p ON oi.product_id = p.product_id\n",
    "WHERE o.customer_id IN (SELECT customer_id FROM premium_customers)\n",
    "  AND oi.quantity > 1                  -- Can use composite index\n",
    "ORDER BY o.order_date, p.product_name;\n",
    "```\n",
    "\n",
    "## 3. Join Algorithm Selection\n",
    "\n",
    "### Hash Join Optimization (Large × Large)\n",
    "```sql\n",
    "-- Force hash join for large table combinations\n",
    "SELECT /*+ USE_HASH(o,oi) */\n",
    "    o.order_date,\n",
    "    COUNT(*) as order_count,\n",
    "    SUM(oi.quantity * oi.unit_price) as total_value\n",
    "FROM orders o\n",
    "JOIN order_items oi ON o.order_id = oi.order_id\n",
    "WHERE o.order_date BETWEEN '2024-01-01' AND '2024-03-31'\n",
    "GROUP BY o.order_date\n",
    "ORDER BY o.order_date;\n",
    "```\n",
    "\n",
    "### Nested Loop Optimization (Small × Large)\n",
    "```sql\n",
    "-- Nested loop is efficient when outer table is small\n",
    "SELECT \n",
    "    r.region_name,\n",
    "    c.customer_name,\n",
    "    COUNT(o.order_id) as order_count\n",
    "FROM regions r                          -- Small table (50 rows)\n",
    "JOIN customers c ON r.region_name = c.region\n",
    "JOIN orders o ON c.customer_id = o.customer_id\n",
    "WHERE r.region_name IN ('California', 'Texas', 'New York')  -- Very selective\n",
    "GROUP BY r.region_name, c.customer_name;\n",
    "```\n",
    "\n",
    "### Sort-Merge Join Optimization\n",
    "```sql\n",
    "-- When both tables are already sorted on join keys\n",
    "SELECT \n",
    "    o.order_date,\n",
    "    c.signup_date,\n",
    "    COUNT(*) as orders\n",
    "FROM orders o                           -- Assume clustered on order_date\n",
    "JOIN customers c ON o.customer_id = c.customer_id  -- Assume sorted on customer_id\n",
    "WHERE o.order_date >= '2024-01-01'\n",
    "GROUP BY o.order_date, c.signup_date\n",
    "ORDER BY o.order_date;                  -- Already in order\n",
    "```\n",
    "\n",
    "## 4. Anti-Pattern Detection and Fixes\n",
    "\n",
    "### Problem: Cartesian Product\n",
    "```sql\n",
    "-- BAD: Missing join condition creates cartesian product\n",
    "SELECT \n",
    "    c.customer_name,\n",
    "    p.product_name,\n",
    "    o.order_date\n",
    "FROM customers c, products p, orders o   -- Old-style joins (dangerous)\n",
    "WHERE c.segment = 'Premium'\n",
    "  AND o.order_date >= '2024-01-01';\n",
    "-- Results: 500K × 50K × 5M = 1.25 quadrillion rows!\n",
    "\n",
    "-- GOOD: Proper join conditions\n",
    "SELECT \n",
    "    c.customer_name,\n",
    "    p.product_name,\n",
    "    o.order_date\n",
    "FROM customers c\n",
    "JOIN orders o ON c.customer_id = o.customer_id\n",
    "JOIN order_items oi ON o.order_id = oi.order_id\n",
    "JOIN products p ON oi.product_id = p.product_id\n",
    "WHERE c.segment = 'Premium'\n",
    "  AND o.order_date >= '2024-01-01';\n",
    "```\n",
    "\n",
    "### Problem: Excessive Self-Joins\n",
    "```sql\n",
    "-- BAD: Multiple self-joins for different time periods\n",
    "SELECT \n",
    "    c.customer_name,\n",
    "    o1.total_amount as q1_total,\n",
    "    o2.total_amount as q2_total,\n",
    "    o3.total_amount as q3_total,\n",
    "    o4.total_amount as q4_total\n",
    "FROM customers c\n",
    "LEFT JOIN orders o1 ON c.customer_id = o1.customer_id \n",
    "    AND o1.order_date BETWEEN '2024-01-01' AND '2024-03-31'\n",
    "LEFT JOIN orders o2 ON c.customer_id = o2.customer_id \n",
    "    AND o2.order_date BETWEEN '2024-04-01' AND '2024-06-30'\n",
    "LEFT JOIN orders o3 ON c.customer_id = o3.customer_id \n",
    "    AND o3.order_date BETWEEN '2024-07-01' AND '2024-09-30'\n",
    "LEFT JOIN orders o4 ON c.customer_id = o4.customer_id \n",
    "    AND o4.order_date BETWEEN '2024-10-01' AND '2024-12-31';\n",
    "\n",
    "-- GOOD: Single join with conditional aggregation\n",
    "SELECT \n",
    "    c.customer_name,\n",
    "    SUM(CASE WHEN o.order_date BETWEEN '2024-01-01' AND '2024-03-31' \n",
    "             THEN o.total_amount END) as q1_total,\n",
    "    SUM(CASE WHEN o.order_date BETWEEN '2024-04-01' AND '2024-06-30' \n",
    "             THEN o.total_amount END) as q2_total,\n",
    "    SUM(CASE WHEN o.order_date BETWEEN '2024-07-01' AND '2024-09-30' \n",
    "             THEN o.total_amount END) as q3_total,\n",
    "    SUM(CASE WHEN o.order_date BETWEEN '2024-10-01' AND '2024-12-31' \n",
    "             THEN o.total_amount END) as q4_total\n",
    "FROM customers c\n",
    "LEFT JOIN orders o ON c.customer_id = o.customer_id \n",
    "    AND o.order_date BETWEEN '2024-01-01' AND '2024-12-31'\n",
    "GROUP BY c.customer_id, c.customer_name;\n",
    "```\n",
    "\n",
    "## 5. Advanced Optimization Techniques\n",
    "\n",
    "### Materialized Join Results\n",
    "```sql\n",
    "-- Create materialized view for frequently joined tables\n",
    "CREATE MATERIALIZED VIEW customer_order_summary AS\n",
    "SELECT \n",
    "    c.customer_id,\n",
    "    c.customer_name,\n",
    "    c.segment,\n",
    "    c.region,\n",
    "    COUNT(o.order_id) as total_orders,\n",
    "    SUM(o.total_amount) as total_spent,\n",
    "    MAX(o.order_date) as last_order_date,\n",
    "    AVG(o.total_amount) as avg_order_value\n",
    "FROM customers c\n",
    "LEFT JOIN orders o ON c.customer_id = o.customer_id\n",
    "GROUP BY c.customer_id, c.customer_name, c.segment, c.region;\n",
    "\n",
    "-- Refresh periodically\n",
    "REFRESH MATERIALIZED VIEW customer_order_summary;\n",
    "\n",
    "-- Use in complex queries\n",
    "SELECT \n",
    "    cos.segment,\n",
    "    cos.region,\n",
    "    AVG(cos.avg_order_value) as segment_avg_order_value,\n",
    "    COUNT(*) as customers_in_segment\n",
    "FROM customer_order_summary cos\n",
    "WHERE cos.total_orders > 5\n",
    "GROUP BY cos.segment, cos.region;\n",
    "```\n",
    "\n",
    "### Lateral Joins for Correlated Subqueries\n",
    "```sql\n",
    "-- Replace inefficient correlated subqueries with lateral joins\n",
    "-- BAD: Correlated subquery executes for each customer\n",
    "SELECT \n",
    "    c.customer_name,\n",
    "    (SELECT COUNT(*) FROM orders o WHERE o.customer_id = c.customer_id) as order_count,\n",
    "    (SELECT MAX(o.order_date) FROM orders o WHERE o.customer_id = c.customer_id) as last_order\n",
    "FROM customers c\n",
    "WHERE c.segment = 'Premium';\n",
    "\n",
    "-- GOOD: Lateral join (PostgreSQL/SQL Server)\n",
    "SELECT \n",
    "    c.customer_name,\n",
    "    os.order_count,\n",
    "    os.last_order\n",
    "FROM customers c\n",
    "CROSS APPLY (\n",
    "    SELECT \n",
    "        COUNT(*) as order_count,\n",
    "        MAX(order_date) as last_order\n",
    "    FROM orders o \n",
    "    WHERE o.customer_id = c.customer_id\n",
    ") os\n",
    "WHERE c.segment = 'Premium';\n",
    "```\n",
    "\n",
    "## 6. Performance Monitoring Queries\n",
    "\n",
    "### Join Cost Analysis\n",
    "```sql\n",
    "-- Analyze join selectivity and cardinality\n",
    "WITH join_stats AS (\n",
    "    SELECT \n",
    "        'customers-orders' as join_name,\n",
    "        COUNT(DISTINCT c.customer_id) as left_distinct,\n",
    "        COUNT(DISTINCT o.customer_id) as right_distinct,\n",
    "        COUNT(*) as join_result_rows,\n",
    "        COUNT(*) * 1.0 / COUNT(DISTINCT c.customer_id) as avg_fanout\n",
    "    FROM customers c\n",
    "    JOIN orders o ON c.customer_id = o.customer_id\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'orders-order_items' as join_name,\n",
    "        COUNT(DISTINCT o.order_id) as left_distinct,\n",
    "        COUNT(DISTINCT oi.order_id) as right_distinct,\n",
    "        COUNT(*) as join_result_rows,\n",
    "        COUNT(*) * 1.0 / COUNT(DISTINCT o.order_id) as avg_fanout\n",
    "    FROM orders o\n",
    "    JOIN order_items oi ON o.order_id = oi.order_id\n",
    ")\n",
    "SELECT \n",
    "    join_name,\n",
    "    left_distinct,\n",
    "    right_distinct,\n",
    "    join_result_rows,\n",
    "    ROUND(avg_fanout, 2) as avg_fanout,\n",
    "    CASE \n",
    "        WHEN avg_fanout > 10 THEN 'High Fanout - Consider Aggregation'\n",
    "        WHEN avg_fanout < 1.1 THEN 'Low Fanout - Nearly 1:1'\n",
    "        ELSE 'Moderate Fanout'\n",
    "    END as fanout_assessment\n",
    "FROM join_stats;\n",
    "```\n",
    "\n",
    "### Index Usage Analysis\n",
    "```sql\n",
    "-- Check if joins are using appropriate indexes\n",
    "EXPLAIN (ANALYZE, BUFFERS) \n",
    "SELECT \n",
    "    c.customer_name,\n",
    "    COUNT(o.order_id) as orders,\n",
    "    SUM(o.total_amount) as total_spent\n",
    "FROM customers c\n",
    "JOIN orders o ON c.customer_id = o.customer_id\n",
    "WHERE c.segment = 'Premium'\n",
    "  AND o.order_date >= '2024-01-01'\n",
    "GROUP BY c.customer_id, c.customer_name\n",
    "HAVING COUNT(o.order_id) > 10;\n",
    "```\n",
    "\n",
    "## Python Join Optimization Equivalents\n",
    "\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Join order optimization - filter first, then join\n",
    "# BAD: Join large dataframes first, filter later\n",
    "# result = orders.merge(order_items, on='order_id') \\\n",
    "#               .merge(customers, on='customer_id') \\\n",
    "#               .query(\"segment == 'Premium' and order_date >= '2024-01-01'\")\n",
    "\n",
    "# GOOD: Filter first, then join smaller datasets\n",
    "premium_customers = customers[customers['segment'] == 'Premium']\n",
    "recent_orders = orders[orders['order_date'] >= '2024-01-01']\n",
    "\n",
    "result = premium_customers.merge(recent_orders, on='customer_id') \\\n",
    "                         .merge(order_items, on='order_id') \\\n",
    "                         .merge(products, on='product_id')\n",
    "\n",
    "# Index optimization - set appropriate indexes\n",
    "customers.set_index('customer_id', inplace=True)\n",
    "orders.set_index(['customer_id', 'order_date'], inplace=True)\n",
    "\n",
    "# Memory-efficient joins for large datasets\n",
    "def chunked_join(left_df, right_df, on, chunk_size=10000):\n",
    "    \"\"\"Join large dataframes in chunks to manage memory\"\"\"\n",
    "    results = []\n",
    "    for i in range(0, len(left_df), chunk_size):\n",
    "        chunk = left_df.iloc[i:i+chunk_size]\n",
    "        merged_chunk = chunk.merge(right_df, on=on, how='left')\n",
    "        results.append(merged_chunk)\n",
    "    return pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Use categorical data types for better join performance\n",
    "customers['segment'] = customers['segment'].astype('category')\n",
    "orders['status'] = orders['status'].astype('category')\n",
    "\n",
    "# Optimize join order based on data size\n",
    "def optimize_join_order(*dataframes, join_keys):\n",
    "    \"\"\"Sort dataframes by size for optimal join order\"\"\"\n",
    "    df_sizes = [(df, len(df)) for df in dataframes]\n",
    "    df_sizes.sort(key=lambda x: x[1])  # Sort by size\n",
    "    \n",
    "    result = df_sizes[0][0]  # Start with smallest\n",
    "    for df, _ in df_sizes[1:]:\n",
    "        result = result.merge(df, on=join_keys, how='inner')\n",
    "    return result\n",
    "```\n",
    "\n",
    "## Excel Join Optimization Patterns\n",
    "\n",
    "```excel\n",
    "# Use structured tables for better performance\n",
    "# Create Table1, Table2, etc. instead of ranges\n",
    "\n",
    "# VLOOKUP optimization - exact match only\n",
    "=VLOOKUP(A2,Table2,2,FALSE)\n",
    "\n",
    "# INDEX-MATCH for better performance than VLOOKUP\n",
    "=INDEX(Table2[Return_Column],MATCH(A2,Table2[Lookup_Column],0))\n",
    "\n",
    "# Power Query for complex multi-table joins\n",
    "# Data > Get Data > Combine Queries > Merge\n",
    "\n",
    "# Optimize with helper columns for common lookups\n",
    "# Instead of complex nested lookups, create intermediate columns\n",
    "\n",
    "# Use pivot tables for aggregated joins\n",
    "# Insert > PivotTable > Add multiple tables to data model\n",
    "```\n",
    "\n",
    "## Advanced Optimization Strategies\n",
    "\n",
    "### 1. **Partitioned Joins**\n",
    "```sql\n",
    "-- Partition large tables by date for better join performance\n",
    "CREATE TABLE orders_2024_q1 PARTITION OF orders \n",
    "FOR VALUES FROM ('2024-01-01') TO ('2024-04-01');\n",
    "\n",
    "-- Queries automatically use appropriate partitions\n",
    "SELECT o.*, c.customer_name\n",
    "FROM orders o\n",
    "JOIN customers c ON o.customer_id = c.customer_id\n",
    "WHERE o.order_date BETWEEN '2024-01-01' AND '2024-03-31';\n",
    "```\n",
    "\n",
    "### 2. **Bloom Filters**\n",
    "```sql\n",
    "-- Use EXISTS instead of IN for better performance\n",
    "-- BAD: IN with subquery\n",
    "SELECT * FROM customers c\n",
    "WHERE c.customer_id IN (\n",
    "    SELECT DISTINCT customer_id FROM orders \n",
    "    WHERE order_date >= '2024-01-01'\n",
    ");\n",
    "\n",
    "-- GOOD: EXISTS (often uses bloom filters internally)\n",
    "SELECT * FROM customers c\n",
    "WHERE EXISTS (\n",
    "    SELECT 1 FROM orders o \n",
    "    WHERE o.customer_id = c.customer_id \n",
    "      AND o.order_date >= '2024-01-01'\n",
    ");\n",
    "```\n",
    "\n",
    "### 3. **Star Schema Optimization**\n",
    "```sql\n",
    "-- Optimize fact table joins by joining dimensions first\n",
    "WITH dim_customers AS (\n",
    "    SELECT customer_id FROM customers \n",
    "    WHERE segment = 'Premium' AND region = 'North'\n",
    "),\n",
    "dim_products AS (\n",
    "    SELECT product_id FROM products \n",
    "    WHERE category = 'Electronics' AND price > 100\n",
    ")\n",
    "SELECT \n",
    "    COUNT(*) as sales_count,\n",
    "    SUM(oi.quantity * oi.unit_price) as total_revenue\n",
    "FROM order_items oi\n",
    "JOIN dim_customers dc ON oi.customer_id = dc.customer_id\n",
    "JOIN dim_products dp ON oi.product_id = dp.product_id\n",
    "JOIN orders o ON oi.order_id = o.order_id\n",
    "WHERE o.order_date >= '2024-01-01';\n",
    "```\n",
    "\n",
    "## Performance Monitoring and Debugging\n",
    "\n",
    "### 1. **Execution Plan Analysis**\n",
    "```sql\n",
    "-- Check for table scans, nested loops, hash joins\n",
    "EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON)\n",
    "SELECT /* your complex query */;\n",
    "\n",
    "-- Look for:\n",
    "-- - Table scans instead of index scans\n",
    "-- - Hash joins on large tables\n",
    "-- - High buffer usage\n",
    "-- - Long execution times\n",
    "```\n",
    "\n",
    "### 2. **Join Cardinality Estimation**\n",
    "```sql\n",
    "-- Verify optimizer estimates vs actual\n",
    "SELECT \n",
    "    'Estimate' as type,\n",
    "    estimated_rows,\n",
    "    estimated_cost\n",
    "FROM query_plan\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'Actual' as type,\n",
    "    actual_rows,\n",
    "    actual_cost\n",
    "FROM query_execution;\n",
    "```\n",
    "\n",
    "## Why These Optimizations Matter\n",
    "\n",
    "**Performance Impact**:\n",
    "- **Query Speed**: 10-1000x faster execution with proper join order\n",
    "- **Memory Usage**: Reduced working set sizes prevent out-of-memory errors\n",
    "- **Scalability**: Techniques that work on small data continue working as data grows\n",
    "\n",
    "**Versus Unoptimized Joins**:\n",
    "- **Resource Efficiency**: Lower CPU, memory, and I/O usage\n",
    "- **Concurrency**: Better performance under load with multiple users\n",
    "- **Cost**: Reduced cloud compute costs from efficient resource usage\n",
    "\n",
    "**Key Principles**:\n",
    "- **Filter Early**: Reduce data volume before expensive operations\n",
    "- **Join Small to Large**: Start with most selective tables\n",
    "- **Leverage Indexes**: Ensure join conditions can use existing indexes\n",
    "- **Monitor Performance**: Regular analysis of execution plans and costs\n",
    "\n",
    "Multiple table join optimization is about understanding your data characteristics, query patterns, and database engine capabilities. Start with proper indexing, apply filtering early, and choose join orders that minimize intermediate result sizes. These techniques scale from simple reports to complex analytical workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58924b7",
   "metadata": {},
   "source": [
    "# ___________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec84e3b",
   "metadata": {},
   "source": [
    "# CROSS APPLY v/s OUTER_APPLY\n",
    "\n",
    " **SQL Server** (also supported in **Azure Synapse**, and partially in **Oracle**).\n",
    "\n",
    "\n",
    "## 🧠 Basic Concept\n",
    "\n",
    "Both `CROSS APPLY` and `OUTER APPLY` let you join a **table-valued function (TVF)** or a **subquery** to each row of another table — kind of like a correlated join.\n",
    "\n",
    "But:\n",
    "\n",
    "| Feature               | `CROSS APPLY`                         | `OUTER APPLY`                       |\n",
    "| --------------------- | ------------------------------------- | ----------------------------------- |\n",
    "| Keeps unmatched rows? | ❌ No (acts like INNER JOIN)           | ✅ Yes (acts like LEFT JOIN)         |\n",
    "| Use case              | When subquery **must return a match** | When subquery **might return NULL** |\n",
    "| Performance           | Slightly faster (less work)           | Slightly slower (more rows)         |\n",
    "\n",
    "\n",
    "\n",
    "## 🔧 Syntax\n",
    "\n",
    "```sql\n",
    "-- CROSS APPLY\n",
    "SELECT a.*, b.*\n",
    "FROM tableA a\n",
    "CROSS APPLY (\n",
    "    SELECT TOP 1 * FROM tableB b WHERE b.id = a.id ORDER BY b.date DESC\n",
    ") b\n",
    "\n",
    "-- OUTER APPLY\n",
    "SELECT a.*, b.*\n",
    "FROM tableA a\n",
    "OUTER APPLY (\n",
    "    SELECT TOP 1 * FROM tableB b WHERE b.id = a.id ORDER BY b.date DESC\n",
    ") b\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 📊 Example\n",
    "\n",
    "### Given:\n",
    "\n",
    "**Customers**\n",
    "\n",
    "| customer\\_id | name    |\n",
    "| ------------ | ------- |\n",
    "| 1            | Alice   |\n",
    "| 2            | Bob     |\n",
    "| 3            | Charlie |\n",
    "\n",
    "**Orders**\n",
    "\n",
    "| order\\_id | customer\\_id | amount |\n",
    "| --------- | ------------ | ------ |\n",
    "| 101       | 1            | 200    |\n",
    "| 102       | 1            | 150    |\n",
    "| 103       | 2            | 300    |\n",
    "\n",
    "\n",
    "\n",
    "### 🟢 `CROSS APPLY`: Only customers with orders\n",
    "\n",
    "```sql\n",
    "SELECT c.name, o.amount\n",
    "FROM Customers c\n",
    "CROSS APPLY (\n",
    "    SELECT TOP 1 * FROM Orders o WHERE o.customer_id = c.customer_id ORDER BY o.amount DESC\n",
    ") o;\n",
    "```\n",
    "\n",
    "**Result:**\n",
    "\n",
    "| name  | amount |\n",
    "| ----- | ------ |\n",
    "| Alice | 200    |\n",
    "| Bob   | 300    |\n",
    "\n",
    "**Charlie is excluded** because he has no orders.\n",
    "\n",
    "\n",
    "\n",
    "### 🟡 `OUTER APPLY`: Includes customers with no orders\n",
    "\n",
    "```sql\n",
    "SELECT c.name, o.amount\n",
    "FROM Customers c\n",
    "OUTER APPLY (\n",
    "    SELECT TOP 1 * FROM Orders o WHERE o.customer_id = c.customer_id ORDER BY o.amount DESC\n",
    ") o;\n",
    "```\n",
    "\n",
    "**Result:**\n",
    "\n",
    "| name    | amount |\n",
    "| ------- | ------ |\n",
    "| Alice   | 200    |\n",
    "| Bob     | 300    |\n",
    "| Charlie | NULL   |\n",
    "\n",
    "Charlie is **included** with `NULL` for `amount`.\n",
    "\n",
    "\n",
    "\n",
    "## 💡 Use Cases\n",
    "\n",
    "| Use Case                                       | APPLY Type                                          |\n",
    "| ---------------------------------------------- | --------------------------------------------------- |\n",
    "| Top N items per group                          | CROSS APPLY                                         |\n",
    "| Getting latest row from a detail table         | APPLY both (depending on if you want unmatched too) |\n",
    "| Conditional logic per row (dynamic subqueries) | APPLY both                                          |\n",
    "| Optional related data (e.g., profile image)    | OUTER APPLY                                         |\n",
    "\n",
    "\n",
    "\n",
    "## 🚀 When to Use\n",
    "\n",
    "* Use **`CROSS APPLY`** when you're **sure** there's a match (better performance).\n",
    "* Use **`OUTER APPLY`** when you want to **include rows even if there's no match**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363b772a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a94e93",
   "metadata": {},
   "source": [
    "## INTERSECT and EXCEPT Operations\n",
    "\n",
    "### Brief Explanation\n",
    "\n",
    "**INTERSECT** returns only the rows that exist in both result sets (common rows), while **EXCEPT** returns rows that exist in the first result set but not in the second (difference/subtraction). Both operations automatically remove duplicates from the final result.\n",
    "\n",
    "**Purposes:**\n",
    "- Find common records between datasets (INTERSECT)\n",
    "- Identify missing or unique records (EXCEPT)\n",
    "- Perform set-based comparisons between tables\n",
    "\n",
    "**Data Analysis Context:**\n",
    "- Essential for data quality auditing and validation\n",
    "- Critical for identifying gaps in data migration or synchronization\n",
    "- Fundamental for comparative analysis between time periods, regions, or segments\n",
    "- Used in data reconciliation and exception reporting\n",
    "\n",
    "### Examples (Simple to Advanced)\n",
    "\n",
    "**Simple Example:**\n",
    "```sql\n",
    "-- INTERSECT: Find products sold in both quarters\n",
    "SELECT product_id FROM q1_sales\n",
    "INTERSECT\n",
    "SELECT product_id FROM q2_sales;\n",
    "\n",
    "-- EXCEPT: Find products sold in Q1 but not in Q2\n",
    "SELECT product_id FROM q1_sales\n",
    "EXCEPT\n",
    "SELECT product_id FROM q2_sales;\n",
    "```\n",
    "\n",
    "**Intermediate Example:**\n",
    "```sql\n",
    "-- Find customers who made purchases in both online and store channels\n",
    "SELECT customer_id, customer_email\n",
    "FROM online_customers\n",
    "INTERSECT\n",
    "SELECT customer_id, customer_email\n",
    "FROM store_customers;\n",
    "\n",
    "-- Find customers who shopped online but never visited stores\n",
    "SELECT customer_id, customer_name, email\n",
    "FROM online_customers\n",
    "EXCEPT\n",
    "SELECT customer_id, customer_name, email\n",
    "FROM store_customers;\n",
    "```\n",
    "\n",
    "**Advanced Example:**\n",
    "```sql\n",
    "-- Complex analysis: Find high-value customers common to both regions\n",
    "SELECT customer_id, SUM(order_value) as total_spent\n",
    "FROM north_region_sales\n",
    "WHERE order_date >= '2024-01-01'\n",
    "GROUP BY customer_id\n",
    "HAVING SUM(order_value) > 5000\n",
    "\n",
    "INTERSECT\n",
    "\n",
    "SELECT customer_id, SUM(order_value) as total_spent\n",
    "FROM south_region_sales\n",
    "WHERE order_date >= '2024-01-01'\n",
    "GROUP BY customer_id\n",
    "HAVING SUM(order_value) > 5000;\n",
    "\n",
    "-- Find products that performed well in Region A but poorly in Region B\n",
    "SELECT product_id, product_category\n",
    "FROM region_a_sales\n",
    "WHERE sales_rank <= 10\n",
    "EXCEPT\n",
    "SELECT product_id, product_category\n",
    "FROM region_b_sales\n",
    "WHERE sales_rank <= 50;\n",
    "```\n",
    "\n",
    "### Python and Excel Comparisons\n",
    "\n",
    "**Python (Pandas):**\n",
    "```python\n",
    "# INTERSECT equivalent\n",
    "df_intersect = pd.merge(df1, df2, how='inner').drop_duplicates()\n",
    "# Or using set operations on indices\n",
    "common_rows = df1[df1.index.isin(df2.index)]\n",
    "\n",
    "# EXCEPT equivalent\n",
    "df_except = df1[~df1.isin(df2)].dropna()\n",
    "# Or more explicitly\n",
    "df_except = pd.concat([df1, df2]).drop_duplicates(keep=False)\n",
    "\n",
    "# Using set operations on specific columns\n",
    "set1 = set(df1['column'].tolist())\n",
    "set2 = set(df2['column'].tolist())\n",
    "intersect_result = set1.intersection(set2)\n",
    "except_result = set1.difference(set2)\n",
    "```\n",
    "\n",
    "**Excel:**\n",
    "- **INTERSECT**: Use VLOOKUP or INDEX-MATCH to find common values, then remove duplicates\n",
    "- **EXCEPT**: Use VLOOKUP with ISERROR to find values in first dataset not in second\n",
    "- Power Query: Data → Merge Queries → Inner Join (INTERSECT) or Anti Join (EXCEPT)\n",
    "- Advanced: Use COUNTIF formulas combined with filtering\n",
    "\n",
    "```excel\n",
    "# Excel formula approach for EXCEPT\n",
    "=IF(COUNTIF(Sheet2!A:A,Sheet1!A2)=0,Sheet1!A2,\"\")\n",
    "```\n",
    "\n",
    "### Conclusion and Importance\n",
    "\n",
    "**Why INTERSECT/EXCEPT are crucial:**\n",
    "\n",
    "1. **Set Theory Power**: These operations bring mathematical set theory directly into SQL, enabling precise data comparisons that would require complex JOINs or subqueries otherwise.\n",
    "\n",
    "2. **Data Quality Assurance**: \n",
    "   - **INTERSECT**: Validates data consistency across systems\n",
    "   - **EXCEPT**: Identifies data gaps, missing records, or synchronization issues\n",
    "\n",
    "3. **Performance Advantages Over Alternatives**:\n",
    "   - **vs Complex JOINs**: More readable and often faster than LEFT JOIN with NULL checks\n",
    "   - **vs EXISTS/NOT EXISTS**: Cleaner syntax for set-based operations\n",
    "   - **vs IN/NOT IN**: Handles multiple columns naturally and avoids NULL-related pitfalls\n",
    "\n",
    "4. **Database Engine Optimization**: Most modern databases optimize these operations at the engine level, making them extremely efficient for large datasets.\n",
    "\n",
    "5. **Business Intelligence Applications**:\n",
    "   - Customer retention analysis (who stayed vs. who left)\n",
    "   - Product performance comparisons\n",
    "   - Data migration validation\n",
    "   - Regulatory compliance reporting\n",
    "\n",
    "**Important Note**: Not all SQL databases support INTERSECT/EXCEPT (notably older MySQL versions). However, they can be simulated using JOINs, making understanding these concepts valuable even in limited environments.\n",
    "\n",
    "**Best Practice**: Use INTERSECT/EXCEPT when you need clean, set-based comparisons. They're particularly powerful for data auditing, quality checks, and comparative analysis where you need to understand exactly what's common or different between datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c78b14",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed8b195",
   "metadata": {},
   "source": [
    "## CTEs (Common Table Expressions) and Data Transformation\n",
    "\n",
    "### Brief Explanation\n",
    "\n",
    "**CTEs (Common Table Expressions)** are temporary named result sets that exist only within the execution scope of a single SQL statement. They're defined using the `WITH` clause and act like temporary views or inline subqueries with names.\n",
    "\n",
    "**Purposes:**\n",
    "- Break complex queries into readable, manageable parts\n",
    "- Create reusable query components within a single statement\n",
    "- Enable recursive operations and hierarchical data processing\n",
    "- Improve code maintainability and debugging\n",
    "\n",
    "**Data Analysis Context:**\n",
    "- Essential for multi-step data transformations and calculations\n",
    "- Critical for creating clean, readable analytical queries\n",
    "- Fundamental for handling complex business logic in data pipelines\n",
    "- Used extensively in data warehousing and ETL processes\n",
    "- Enables sophisticated window functions and ranking operations\n",
    "\n",
    "### Examples (Simple to Advanced)\n",
    "\n",
    "**Simple Example:**\n",
    "```sql\n",
    "-- Basic CTE for data filtering\n",
    "WITH high_value_customers AS (\n",
    "    SELECT customer_id, customer_name, total_spent\n",
    "    FROM customers\n",
    "    WHERE total_spent > 10000\n",
    ")\n",
    "SELECT customer_name, total_spent\n",
    "FROM high_value_customers\n",
    "ORDER BY total_spent DESC;\n",
    "```\n",
    "\n",
    "**Intermediate Example:**\n",
    "```sql\n",
    "-- Multiple CTEs for step-by-step transformation\n",
    "WITH monthly_sales AS (\n",
    "    SELECT \n",
    "        EXTRACT(MONTH FROM order_date) as month,\n",
    "        product_id,\n",
    "        SUM(quantity * price) as monthly_revenue\n",
    "    FROM orders\n",
    "    WHERE EXTRACT(YEAR FROM order_date) = 2024\n",
    "    GROUP BY EXTRACT(MONTH FROM order_date), product_id\n",
    "),\n",
    "ranked_products AS (\n",
    "    SELECT \n",
    "        month,\n",
    "        product_id,\n",
    "        monthly_revenue,\n",
    "        ROW_NUMBER() OVER (PARTITION BY month ORDER BY monthly_revenue DESC) as rank\n",
    "    FROM monthly_sales\n",
    ")\n",
    "SELECT \n",
    "    month,\n",
    "    product_id,\n",
    "    monthly_revenue,\n",
    "    rank\n",
    "FROM ranked_products\n",
    "WHERE rank <= 5;\n",
    "```\n",
    "\n",
    "**Advanced Example:**\n",
    "```sql\n",
    "-- Complex data transformation with multiple CTEs\n",
    "WITH customer_metrics AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        COUNT(DISTINCT order_id) as order_count,\n",
    "        SUM(order_value) as total_spent,\n",
    "        AVG(order_value) as avg_order_value,\n",
    "        MIN(order_date) as first_order,\n",
    "        MAX(order_date) as last_order\n",
    "    FROM orders\n",
    "    WHERE order_date >= '2024-01-01'\n",
    "    GROUP BY customer_id\n",
    "),\n",
    "customer_segments AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        order_count,\n",
    "        total_spent,\n",
    "        avg_order_value,\n",
    "        DATEDIFF(last_order, first_order) as customer_lifetime_days,\n",
    "        CASE \n",
    "            WHEN total_spent > 5000 AND order_count > 10 THEN 'VIP'\n",
    "            WHEN total_spent > 2000 AND order_count > 5 THEN 'Premium'\n",
    "            WHEN total_spent > 500 THEN 'Regular'\n",
    "            ELSE 'New'\n",
    "        END as segment\n",
    "    FROM customer_metrics\n",
    "),\n",
    "segment_analysis AS (\n",
    "    SELECT \n",
    "        segment,\n",
    "        COUNT(*) as customer_count,\n",
    "        AVG(total_spent) as avg_total_spent,\n",
    "        AVG(avg_order_value) as avg_order_size,\n",
    "        AVG(customer_lifetime_days) as avg_lifetime_days\n",
    "    FROM customer_segments\n",
    "    GROUP BY segment\n",
    ")\n",
    "SELECT \n",
    "    segment,\n",
    "    customer_count,\n",
    "    ROUND(avg_total_spent, 2) as avg_total_spent,\n",
    "    ROUND(avg_order_size, 2) as avg_order_size,\n",
    "    ROUND(avg_lifetime_days, 0) as avg_lifetime_days,\n",
    "    ROUND(100.0 * customer_count / SUM(customer_count) OVER(), 2) as segment_percentage\n",
    "FROM segment_analysis\n",
    "ORDER BY avg_total_spent DESC;\n",
    "\n",
    "-- Recursive CTE example for hierarchical data\n",
    "WITH RECURSIVE employee_hierarchy AS (\n",
    "    -- Base case: top-level managers\n",
    "    SELECT employee_id, employee_name, manager_id, 1 as level\n",
    "    FROM employees\n",
    "    WHERE manager_id IS NULL\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    -- Recursive case: employees with managers\n",
    "    SELECT e.employee_id, e.employee_name, e.manager_id, eh.level + 1\n",
    "    FROM employees e\n",
    "    JOIN employee_hierarchy eh ON e.manager_id = eh.employee_id\n",
    ")\n",
    "SELECT employee_id, employee_name, level,\n",
    "       REPEAT('  ', level - 1) || employee_name as indented_name\n",
    "FROM employee_hierarchy\n",
    "ORDER BY level, employee_name;\n",
    "```\n",
    "\n",
    "### Python and Excel Comparisons\n",
    "\n",
    "**Python (Pandas):**\n",
    "```python\n",
    "# CTE-like operations using method chaining and intermediate variables\n",
    "# Step 1: Create intermediate dataset (like CTE)\n",
    "high_value_customers = df[df['total_spent'] > 10000]\n",
    "\n",
    "# Step 2: Further transformation\n",
    "result = (high_value_customers\n",
    "    .groupby('customer_segment')\n",
    "    .agg({'total_spent': 'mean', 'customer_id': 'count'})\n",
    "    .reset_index())\n",
    "\n",
    "# Multiple transformations (equivalent to multiple CTEs)\n",
    "monthly_sales = (df\n",
    "    .groupby(['month', 'product_id'])\n",
    "    .agg({'revenue': 'sum'})\n",
    "    .reset_index())\n",
    "\n",
    "ranked_products = (monthly_sales\n",
    "    .assign(rank=lambda x: x.groupby('month')['revenue'].rank(method='dense', ascending=False))\n",
    "    .query('rank <= 5'))\n",
    "\n",
    "# Using pipe for CTE-like readability\n",
    "result = (df\n",
    "    .pipe(lambda x: x[x['total_spent'] > 10000])\n",
    "    .pipe(lambda x: x.groupby('segment').agg({'total_spent': 'mean'}))\n",
    "    .pipe(lambda x: x.reset_index()))\n",
    "```\n",
    "\n",
    "**Excel:**\n",
    "- **Basic CTE equivalent**: Create intermediate calculations in helper columns\n",
    "- **Power Query**: Use multiple transformation steps (similar to CTEs)\n",
    "- **Named ranges**: Define reusable data ranges\n",
    "- **Pivot Tables**: Multi-step aggregations with intermediate groupings\n",
    "\n",
    "```excel\n",
    "# Excel Power Query M language (similar to CTEs)\n",
    "let\n",
    "    Source = Excel.CurrentWorkbook(){[Name=\"SalesData\"]}[Content],\n",
    "    HighValueCustomers = Table.SelectRows(Source, each [Total_Spent] > 10000),\n",
    "    GroupedData = Table.Group(HighValueCustomers, {\"Segment\"}, {{\"AvgSpent\", each List.Average([Total_Spent]), type number}})\n",
    "in\n",
    "    GroupedData\n",
    "```\n",
    "\n",
    "### Conclusion and Importance\n",
    "\n",
    "**Why CTEs are crucial for data transformation:**\n",
    "\n",
    "1. **Readability and Maintainability**: CTEs break complex queries into logical, named steps, making code self-documenting and easier to debug than nested subqueries.\n",
    "\n",
    "2. **Performance Benefits**: \n",
    "   - Database engines can optimize CTE execution plans\n",
    "   - Avoid repeated subquery execution\n",
    "   - Enable better caching strategies\n",
    "\n",
    "3. **Advantages Over Alternatives**:\n",
    "   - **vs Subqueries**: More readable, reusable within the same query\n",
    "   - **vs Views**: Temporary scope, no database object creation needed\n",
    "   - **vs Temporary Tables**: No storage overhead, automatic cleanup\n",
    "\n",
    "4. **Advanced Capabilities**:\n",
    "   - **Recursive CTEs**: Handle hierarchical data (org charts, category trees) impossible with standard SQL\n",
    "   - **Window Functions**: Enable sophisticated analytical calculations with clean syntax\n",
    "\n",
    "5. **Data Pipeline Integration**: CTEs are essential in modern data warehousing:\n",
    "   - **dbt (data build tool)**: Heavily relies on CTEs for transformation logic\n",
    "   - **Analytical databases**: Optimized for CTE-heavy workloads\n",
    "   - **Data modeling**: Enable complex business logic implementation\n",
    "\n",
    "6. **Business Intelligence Power**: \n",
    "   - Multi-step customer segmentation\n",
    "   - Complex financial calculations\n",
    "   - Time-series analysis with multiple aggregation levels\n",
    "   - Data quality checks and transformations\n",
    "\n",
    "**Best Practices**: \n",
    "- Use CTEs for complex multi-step transformations\n",
    "- Name CTEs descriptively to document business logic\n",
    "- Combine with window functions for advanced analytics\n",
    "- Prefer CTEs over nested subqueries for readability\n",
    "- Use recursive CTEs for hierarchical data processing\n",
    "\n",
    "**Modern Relevance**: CTEs are fundamental to modern SQL-based data transformation frameworks and are considered essential for any serious data analysis or data engineering work.\n",
    "\n",
    "I'll continue using this comprehensive format for all future SQL topics!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761381e5",
   "metadata": {},
   "source": [
    "## Recursive CTEs (Common Table Expressions)\n",
    "\n",
    "### Brief Explanation\n",
    "\n",
    "**Recursive CTEs** are specialized Common Table Expressions that reference themselves, enabling queries to process hierarchical or tree-structured data through iterative operations. They consist of two parts: an anchor query (base case) and a recursive query that references the CTE itself.\n",
    "\n",
    "**Purposes:**\n",
    "- Navigate hierarchical data structures (organizational charts, category trees)\n",
    "- Generate sequences and series programmatically\n",
    "- Traverse graphs and networks\n",
    "- Calculate cumulative and running totals across levels\n",
    "\n",
    "**Data Analysis Context:**\n",
    "- Essential for organizational analytics (reporting chains, team structures)\n",
    "- Critical for product catalog management (categories, subcategories)\n",
    "- Fundamental for financial consolidation across business units\n",
    "- Used in network analysis, social media connections, and supply chain tracing\n",
    "- Enables complex time-series analysis with dependency relationships\n",
    "\n",
    "### Examples (Simple to Advanced)\n",
    "\n",
    "**Simple Example:**\n",
    "```sql\n",
    "-- Generate a sequence of numbers\n",
    "WITH RECURSIVE number_sequence AS (\n",
    "    -- Anchor: Starting point\n",
    "    SELECT 1 as num\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    -- Recursive: Add next number\n",
    "    SELECT num + 1\n",
    "    FROM number_sequence\n",
    "    WHERE num < 10\n",
    ")\n",
    "SELECT num FROM number_sequence;\n",
    "-- Result: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
    "```\n",
    "\n",
    "**Intermediate Example:**\n",
    "```sql\n",
    "-- Employee hierarchy navigation\n",
    "WITH RECURSIVE employee_tree AS (\n",
    "    -- Anchor: Find all top-level managers (CEO, VPs)\n",
    "    SELECT \n",
    "        employee_id,\n",
    "        employee_name,\n",
    "        manager_id,\n",
    "        job_title,\n",
    "        1 as level,\n",
    "        CAST(employee_name AS VARCHAR(1000)) as hierarchy_path\n",
    "    FROM employees\n",
    "    WHERE manager_id IS NULL\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    -- Recursive: Find all direct reports\n",
    "    SELECT \n",
    "        e.employee_id,\n",
    "        e.employee_name,\n",
    "        e.manager_id,\n",
    "        e.job_title,\n",
    "        et.level + 1,\n",
    "        CAST(et.hierarchy_path + ' -> ' + e.employee_name AS VARCHAR(1000))\n",
    "    FROM employees e\n",
    "    INNER JOIN employee_tree et ON e.manager_id = et.employee_id\n",
    "    WHERE et.level < 10  -- Prevent infinite recursion\n",
    ")\n",
    "SELECT \n",
    "    level,\n",
    "    REPEAT('  ', level - 1) + employee_name as indented_name,\n",
    "    job_title,\n",
    "    hierarchy_path\n",
    "FROM employee_tree\n",
    "ORDER BY hierarchy_path;\n",
    "```\n",
    "\n",
    "**Advanced Example:**\n",
    "```sql\n",
    "-- Complex business analysis: Sales territory hierarchy with aggregations\n",
    "WITH RECURSIVE territory_hierarchy AS (\n",
    "    -- Anchor: Root territories (countries/regions)\n",
    "    SELECT \n",
    "        territory_id,\n",
    "        territory_name,\n",
    "        parent_territory_id,\n",
    "        territory_level,\n",
    "        1 as depth,\n",
    "        CAST(territory_name AS VARCHAR(500)) as full_path\n",
    "    FROM territories\n",
    "    WHERE parent_territory_id IS NULL\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    -- Recursive: Child territories\n",
    "    SELECT \n",
    "        t.territory_id,\n",
    "        t.territory_name,\n",
    "        t.parent_territory_id,\n",
    "        t.territory_level,\n",
    "        th.depth + 1,\n",
    "        CAST(th.full_path + ' / ' + t.territory_name AS VARCHAR(500))\n",
    "    FROM territories t\n",
    "    INNER JOIN territory_hierarchy th ON t.parent_territory_id = th.territory_id\n",
    "    WHERE th.depth < 5\n",
    "),\n",
    "territory_sales AS (\n",
    "    -- Calculate sales for each territory level\n",
    "    SELECT \n",
    "        th.territory_id,\n",
    "        th.territory_name,\n",
    "        th.depth,\n",
    "        th.full_path,\n",
    "        COALESCE(SUM(s.sale_amount), 0) as direct_sales,\n",
    "        COUNT(s.sale_id) as direct_sale_count\n",
    "    FROM territory_hierarchy th\n",
    "    LEFT JOIN sales s ON th.territory_id = s.territory_id\n",
    "    GROUP BY th.territory_id, th.territory_name, th.depth, th.full_path\n",
    "),\n",
    "territory_totals AS (\n",
    "    -- Roll up sales to parent territories\n",
    "    SELECT \n",
    "        ts.territory_id,\n",
    "        ts.territory_name,\n",
    "        ts.depth,\n",
    "        ts.full_path,\n",
    "        ts.direct_sales,\n",
    "        ts.direct_sale_count,\n",
    "        -- Calculate total sales including all child territories\n",
    "        (SELECT SUM(ts2.direct_sales) \n",
    "         FROM territory_sales ts2 \n",
    "         WHERE ts2.full_path LIKE ts.full_path + '%') as total_sales_with_children\n",
    "    FROM territory_sales ts\n",
    ")\n",
    "SELECT \n",
    "    depth,\n",
    "    REPEAT('  ', depth - 1) + territory_name as hierarchy_display,\n",
    "    direct_sales,\n",
    "    total_sales_with_children,\n",
    "    total_sales_with_children - direct_sales as child_territory_sales,\n",
    "    ROUND(100.0 * direct_sales / NULLIF(total_sales_with_children, 0), 2) as direct_sales_percentage\n",
    "FROM territory_totals\n",
    "ORDER BY full_path;\n",
    "\n",
    "-- Advanced: Bill of Materials explosion (manufacturing)\n",
    "WITH RECURSIVE bom_explosion AS (\n",
    "    -- Anchor: Top-level product\n",
    "    SELECT \n",
    "        product_id,\n",
    "        component_id,\n",
    "        quantity_required,\n",
    "        1 as level,\n",
    "        quantity_required as total_quantity,\n",
    "        CAST(product_id + '->' + component_id AS VARCHAR(1000)) as path\n",
    "    FROM bill_of_materials\n",
    "    WHERE product_id = 'LAPTOP-001'  -- Starting product\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    -- Recursive: Sub-components\n",
    "    SELECT \n",
    "        bom.product_id,\n",
    "        bom.component_id,\n",
    "        bom.quantity_required,\n",
    "        be.level + 1,\n",
    "        be.total_quantity * bom.quantity_required as total_quantity,\n",
    "        CAST(be.path + '->' + bom.component_id AS VARCHAR(1000))\n",
    "    FROM bill_of_materials bom\n",
    "    INNER JOIN bom_explosion be ON bom.product_id = be.component_id\n",
    "    WHERE be.level < 10\n",
    ")\n",
    "SELECT \n",
    "    level,\n",
    "    REPEAT('  ', level - 1) + component_id as indented_component,\n",
    "    quantity_required as unit_quantity,\n",
    "    total_quantity as total_needed,\n",
    "    path as component_path\n",
    "FROM bom_explosion\n",
    "ORDER BY path;\n",
    "```\n",
    "\n",
    "### Python and Excel Comparisons\n",
    "\n",
    "**Python (Pandas + NetworkX):**\n",
    "```python\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# Recursive hierarchy traversal\n",
    "def traverse_hierarchy(df, parent_col, child_col, start_node):\n",
    "    \"\"\"Simulate recursive CTE for hierarchy traversal\"\"\"\n",
    "    result = []\n",
    "    \n",
    "    def recursive_traverse(node, level=1, path=\"\"):\n",
    "        current_path = f\"{path}->{node}\" if path else node\n",
    "        result.append({'node': node, 'level': level, 'path': current_path})\n",
    "        \n",
    "        # Find children\n",
    "        children = df[df[parent_col] == node][child_col].tolist()\n",
    "        for child in children:\n",
    "            recursive_traverse(child, level + 1, current_path)\n",
    "    \n",
    "    recursive_traverse(start_node)\n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "# Using NetworkX for graph traversal\n",
    "G = nx.from_pandas_edgelist(df, source='manager_id', target='employee_id')\n",
    "# Get all descendants\n",
    "descendants = nx.descendants(G, root_node)\n",
    "\n",
    "# Pandas groupby with cumulative operations (limited recursion simulation)\n",
    "def hierarchical_sum(df, hierarchy_cols, value_col):\n",
    "    \"\"\"Simulate recursive aggregation\"\"\"\n",
    "    result = df.copy()\n",
    "    for level in range(len(hierarchy_cols)):\n",
    "        group_cols = hierarchy_cols[:level+1]\n",
    "        grouped = df.groupby(group_cols)[value_col].sum().reset_index()\n",
    "        # Additional logic to roll up to parent levels\n",
    "    return result\n",
    "```\n",
    "\n",
    "**Excel:**\n",
    "- **Hierarchical data**: Use XLOOKUP/INDEX-MATCH with helper columns for level calculation\n",
    "- **Power Query**: Recursive expansion using Table.ExpandTableColumn\n",
    "- **Pivot Tables**: Limited hierarchy with drill-down capabilities\n",
    "- **Manual approach**: Multiple helper columns for each hierarchy level\n",
    "\n",
    "```excel\n",
    "# Excel formula approach (limited depth)\n",
    "=IF(ISBLANK(B2),\"\",XLOOKUP(B2,A:A,C:C)&\"->\"&C2)  # Build hierarchy path\n",
    "=COUNTIF($B$2:B2,B2)  # Calculate level in hierarchy\n",
    "```\n",
    "\n",
    "**Power Query M (Excel/Power BI):**\n",
    "```m\n",
    "// Recursive function in Power Query\n",
    "let\n",
    "    Source = YourTable,\n",
    "    // Define recursive function\n",
    "    ExpandHierarchy = (tbl as table, parent as text) =>\n",
    "        let\n",
    "            Children = Table.SelectRows(tbl, each [ParentID] = parent),\n",
    "            AddChildren = Table.AddColumn(Children, \"Children\", \n",
    "                each @ExpandHierarchy(tbl, [ID]))\n",
    "        in\n",
    "            AddChildren\n",
    "in\n",
    "    ExpandHierarchy(Source, null)\n",
    "```\n",
    "\n",
    "### Conclusion and Importance\n",
    "\n",
    "**Why Recursive CTEs are crucial:**\n",
    "\n",
    "1. **Unique Problem-Solving Capability**: Recursive CTEs solve problems that are extremely difficult or impossible with standard SQL operations:\n",
    "   - **Hierarchical queries**: No equivalent in basic SQL\n",
    "   - **Graph traversal**: Essential for network analysis\n",
    "   - **Tree operations**: Critical for organizational and product structures\n",
    "\n",
    "2. **Performance Advantages**:\n",
    "   - **vs Multiple JOINs**: Eliminates complex self-joins for hierarchical data\n",
    "   - **vs Application Logic**: Database-level processing is faster than application-level recursion\n",
    "   - **vs Cursors**: Set-based approach is more efficient than row-by-row processing\n",
    "\n",
    "3. **Real-World Business Applications**:\n",
    "   - **Organizational Analytics**: Employee reporting chains, span of control analysis\n",
    "   - **Financial Consolidation**: Rolling up P&L across business units and subsidiaries\n",
    "   - **Supply Chain**: Bill of materials explosion, vendor dependency mapping\n",
    "   - **Customer Analytics**: Referral chains, social network effects\n",
    "\n",
    "4. **Advantages Over Alternatives**:\n",
    "   - **vs Stored Procedures with Loops**: Cleaner syntax, better optimization\n",
    "   - **vs Application-Level Recursion**: Reduced data transfer, leverages database optimization\n",
    "   - **vs Materialized Views**: Dynamic calculation without storage overhead\n",
    "\n",
    "5. **Modern Data Architecture Integration**:\n",
    "   - **Data Warehousing**: Essential for dimensional modeling with hierarchies\n",
    "   - **Analytics Platforms**: Snowflake, BigQuery, PostgreSQL all optimize recursive CTEs\n",
    "   - **Graph Databases**: Bridge between relational and graph query paradigms\n",
    "\n",
    "6. **Advanced Analytical Capabilities**:\n",
    "   - **Time-series with dependencies**: Calculate cumulative effects across related entities\n",
    "   - **Network analysis**: Find paths, cycles, and connectivity in business networks\n",
    "   - **Hierarchical aggregations**: Roll-up reporting across organizational levels\n",
    "\n",
    "**Best Practices**:\n",
    "- Always include termination conditions (UNION ALL with WHERE clauses)\n",
    "- Use depth limits to prevent infinite recursion\n",
    "- Index foreign key columns used in recursive joins\n",
    "- Consider materialized views for frequently-accessed hierarchical data\n",
    "- Test performance with realistic data volumes\n",
    "\n",
    "**Critical Importance**: Recursive CTEs are irreplaceable for hierarchical data analysis. While Python and Excel offer alternatives, they cannot match SQL's efficiency and elegance for database-resident hierarchical operations. They're essential for any organization dealing with trees, graphs, or multi-level business structures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0dec65",
   "metadata": {},
   "source": [
    "# Grouping Sets, Cube, and Rollup in SQL\n",
    "\n",
    "## What They Mean\n",
    "**Grouping Sets, Cube, and Rollup** are advanced SQL aggregation techniques that enable **multidimensional analysis** in a single query. They solve the problem of generating multiple levels of summary data without writing complex UNION queries or multiple separate statements.\n",
    "\n",
    "## Core Purpose\n",
    "These techniques serve three primary purposes:\n",
    "- **Data Summarization**: Create hierarchical totals and subtotals across multiple dimensions\n",
    "- **Business Intelligence**: Generate comprehensive reports showing data at different granular levels\n",
    "- **Performance Optimization**: Reduce query complexity and execution time compared to multiple separate aggregations\n",
    "\n",
    "## Data Analysis Significance\n",
    "In data analysis terms, these are **OLAP (Online Analytical Processing) operations** that enable:\n",
    "- **Drill-down/Roll-up Analysis**: Moving between different levels of detail\n",
    "- **Cross-tabulation**: Analyzing data across multiple categorical dimensions\n",
    "- **Pivot-style Analysis**: Creating summary tables with multiple grouping perspectives\n",
    "- **Dimensional Analysis**: Understanding data patterns across business hierarchies\n",
    "\n",
    "\n",
    "## Examples: Simple to Advanced\n",
    "\n",
    "### Simple Example: Basic Sales Analysis\n",
    "```sql\n",
    "-- Simple ROLLUP for time hierarchy\n",
    "SELECT \n",
    "    year, \n",
    "    quarter, \n",
    "    SUM(sales) as total_sales\n",
    "FROM sales_data\n",
    "GROUP BY ROLLUP(year, quarter);\n",
    "```\n",
    "**Output includes:**\n",
    "- Sales by year and quarter\n",
    "- Sales by year only\n",
    "- Grand total across all data\n",
    "\n",
    "### Intermediate Example: Multi-dimensional Analysis\n",
    "```sql\n",
    "-- CUBE for product and region analysis\n",
    "SELECT \n",
    "    region,\n",
    "    product_category,\n",
    "    COUNT(*) as order_count,\n",
    "    SUM(revenue) as total_revenue\n",
    "FROM orders\n",
    "GROUP BY CUBE(region, product_category);\n",
    "```\n",
    "**Output includes all combinations:**\n",
    "- Region + Product Category\n",
    "- Region only\n",
    "- Product Category only  \n",
    "- Grand total\n",
    "\n",
    "### Advanced Example: Complex Business Hierarchy\n",
    "```sql\n",
    "-- GROUPING SETS for custom business analysis\n",
    "SELECT \n",
    "    fiscal_year,\n",
    "    quarter,\n",
    "    region,\n",
    "    sales_rep,\n",
    "    product_line,\n",
    "    SUM(revenue) as revenue,\n",
    "    COUNT(DISTINCT customer_id) as unique_customers,\n",
    "    GROUPING(fiscal_year, quarter, region, sales_rep, product_line) as grouping_id\n",
    "FROM sales_transactions\n",
    "GROUP BY GROUPING SETS (\n",
    "    (fiscal_year, quarter, region, sales_rep, product_line),  -- Detailed level\n",
    "    (fiscal_year, quarter, region),                          -- Regional quarterly\n",
    "    (fiscal_year, product_line),                             -- Annual product performance\n",
    "    (sales_rep),                                             -- Individual rep totals\n",
    "    ()                                                       -- Grand total\n",
    ");\n",
    "```\n",
    "\n",
    "\n",
    "## Python Parallels\n",
    "\n",
    "### Using Pandas GroupBy with Multiple Levels\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Equivalent to SQL ROLLUP\n",
    "df_rollup = df.groupby(['year', 'quarter']).agg({\n",
    "    'sales': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Add subtotals manually (pandas doesn't have direct ROLLUP)\n",
    "year_totals = df.groupby('year')['sales'].sum().reset_index()\n",
    "year_totals['quarter'] = 'Total'\n",
    "grand_total = pd.DataFrame({\n",
    "    'year': ['Grand Total'], \n",
    "    'quarter': [''], \n",
    "    'sales': [df['sales'].sum()]\n",
    "})\n",
    "\n",
    "result = pd.concat([df_rollup, year_totals, grand_total])\n",
    "```\n",
    "\n",
    "### Using Pandas Pivot Tables (CUBE-like)\n",
    "```python\n",
    "# Equivalent to SQL CUBE\n",
    "pivot_result = pd.pivot_table(\n",
    "    df, \n",
    "    values='revenue',\n",
    "    index=['region', 'product_category'],\n",
    "    aggfunc='sum',\n",
    "    margins=True,  # Adds totals\n",
    "    fill_value=0\n",
    ")\n",
    "```\n",
    "\n",
    "### Advanced Python: Custom Grouping Sets\n",
    "```python\n",
    "def custom_grouping_sets(df, grouping_sets, agg_dict):\n",
    "    results = []\n",
    "    for grouping in grouping_sets:\n",
    "        if grouping:  # Not empty tuple\n",
    "            grouped = df.groupby(list(grouping)).agg(agg_dict).reset_index()\n",
    "        else:  # Grand total\n",
    "            grouped = df.agg(agg_dict).to_frame().T\n",
    "        results.append(grouped)\n",
    "    return pd.concat(results, ignore_index=True)\n",
    "```\n",
    "\n",
    "\n",
    "## Excel Parallels\n",
    "\n",
    "### Pivot Tables (CUBE equivalent)\n",
    "```\n",
    "Excel Pivot Table:\n",
    "- Rows: Region, Product Category\n",
    "- Values: Sum of Revenue, Count of Orders\n",
    "- Show Grand Totals: Yes\n",
    "- Show Subtotals: Yes\n",
    "```\n",
    "\n",
    "### Subtotal Function (ROLLUP equivalent)\n",
    "```\n",
    "Excel Formula approach:\n",
    "=SUBTOTAL(9, range)  # For sums at different hierarchy levels\n",
    "```\n",
    "\n",
    "### Power Query (Advanced Grouping Sets)\n",
    "```\n",
    "Power Query M:\n",
    "Table.Group(\n",
    "    Source,\n",
    "    {\"Year\", \"Quarter\"},\n",
    "    {{\"Total Sales\", each List.Sum([Sales]), type number}}\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "## Importance vs Alternative Methods\n",
    "\n",
    "### Why These Techniques Excel Over Alternatives\n",
    "\n",
    "**1. Performance Superiority**\n",
    "- **Single Query Execution**: One pass through data vs multiple queries\n",
    "- **Optimized Execution Plans**: Database engines optimize these operations specifically\n",
    "- **Reduced I/O**: Eliminates need for multiple table scans\n",
    "\n",
    "**2. Code Simplicity vs UNION Approach**\n",
    "```sql\n",
    "-- Traditional approach (inefficient)\n",
    "SELECT dept, NULL as job, SUM(salary) FROM emp GROUP BY dept\n",
    "UNION ALL\n",
    "SELECT NULL, job, SUM(salary) FROM emp GROUP BY job\n",
    "UNION ALL\n",
    "SELECT NULL, NULL, SUM(salary) FROM emp;\n",
    "\n",
    "-- Modern approach (efficient)  \n",
    "SELECT dept, job, SUM(salary) \n",
    "FROM emp \n",
    "GROUP BY GROUPING SETS ((dept), (job), ());\n",
    "```\n",
    "\n",
    "**3. Business Intelligence Integration**\n",
    "- **OLAP Compatibility**: Direct integration with BI tools like Tableau, Power BI\n",
    "- **Dimensional Modeling**: Natural fit for star/snowflake schemas\n",
    "- **Report Generation**: Single query produces complete analytical reports\n",
    "\n",
    "**4. Analytical Flexibility**\n",
    "- **Dynamic Hierarchies**: Easy to modify grouping levels without query restructuring  \n",
    "- **Cross-dimensional Analysis**: Analyze relationships between different business dimensions\n",
    "- **Scalable Complexity**: Handles both simple subtotals and complex multidimensional analysis\n",
    "\n",
    "### When Alternatives Fall Short\n",
    "- **Manual UNION queries**: Become exponentially complex with more dimensions\n",
    "- **Application-level aggregation**: Performance bottlenecks with large datasets\n",
    "- **Multiple separate queries**: Network overhead and consistency issues\n",
    "- **Spreadsheet pivot tables**: Limited scalability and processing power\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427f9298",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e32e22d",
   "metadata": {},
   "source": [
    "# STRING_AGG and Array Aggregations in SQL\n",
    "\n",
    "## What They Mean\n",
    "**STRING_AGG** and **Array Aggregations** are SQL functions that combine multiple values from grouped rows into a single aggregated result. STRING_AGG creates concatenated strings, while array aggregations (like ARRAY_AGG) create arrays/lists of values. These represent **value consolidation techniques** that transform normalized relational data into denormalized, collection-based formats.\n",
    "\n",
    "## Core Purpose\n",
    "These functions serve three primary purposes:\n",
    "- **Data Denormalization**: Convert multiple related rows into single consolidated records\n",
    "- **Reporting Enhancement**: Create readable, comma-separated lists for business reports\n",
    "- **Data Export Preparation**: Format data for downstream systems that expect consolidated formats\n",
    "\n",
    "## Data Analysis Significance\n",
    "In data analysis terms, these are **aggregation transformations** that enable:\n",
    "- **One-to-Many Relationship Flattening**: Collapse child records into parent summaries\n",
    "- **Categorical Data Consolidation**: Combine multiple categories into single descriptive fields\n",
    "- **ETL Processing**: Prepare data for systems expecting consolidated formats\n",
    "- **Analytical Reporting**: Create human-readable summaries from normalized data structures\n",
    "\n",
    "\n",
    "\n",
    "## Examples: Simple to Advanced\n",
    "\n",
    "### Simple Example: Basic Customer Order Summary\n",
    "```sql\n",
    "-- STRING_AGG: Create comma-separated list of products ordered\n",
    "SELECT \n",
    "    customer_id,\n",
    "    customer_name,\n",
    "    STRING_AGG(product_name, ', ') as products_ordered,\n",
    "    COUNT(*) as total_orders\n",
    "FROM orders o\n",
    "JOIN customers c ON o.customer_id = c.customer_id\n",
    "JOIN products p ON o.product_id = p.product_id\n",
    "GROUP BY customer_id, customer_name;\n",
    "\n",
    "-- ARRAY_AGG: Create array of order amounts\n",
    "SELECT \n",
    "    customer_id,\n",
    "    ARRAY_AGG(order_amount) as order_amounts,\n",
    "    ARRAY_AGG(DISTINCT product_category) as categories_purchased\n",
    "FROM orders o\n",
    "JOIN products p ON o.product_id = p.product_id\n",
    "GROUP BY customer_id;\n",
    "```\n",
    "\n",
    "### Intermediate Example: Enhanced Reporting with Ordering\n",
    "```sql\n",
    "-- STRING_AGG with custom separators and ordering\n",
    "SELECT \n",
    "    department,\n",
    "    STRING_AGG(\n",
    "        employee_name || ' ($' || salary || ')', \n",
    "        ' | ' \n",
    "        ORDER BY salary DESC\n",
    "    ) as employee_salary_list,\n",
    "    ARRAY_AGG(\n",
    "        DISTINCT skill \n",
    "        ORDER BY skill\n",
    "    ) as department_skills\n",
    "FROM employees e\n",
    "LEFT JOIN employee_skills es ON e.employee_id = es.employee_id\n",
    "GROUP BY department;\n",
    "\n",
    "-- Advanced formatting with conditional aggregation\n",
    "SELECT \n",
    "    project_id,\n",
    "    project_name,\n",
    "    STRING_AGG(\n",
    "        CASE \n",
    "            WHEN role = 'Lead' THEN '**' || employee_name || '**'\n",
    "            ELSE employee_name \n",
    "        END,\n",
    "        ', '\n",
    "        ORDER BY \n",
    "            CASE WHEN role = 'Lead' THEN 1 ELSE 2 END,\n",
    "            employee_name\n",
    "    ) as team_members,\n",
    "    ARRAY_AGG(DISTINCT technology ORDER BY technology) as tech_stack\n",
    "FROM projects p\n",
    "JOIN project_assignments pa ON p.project_id = pa.project_id\n",
    "JOIN employees e ON pa.employee_id = e.employee_id\n",
    "GROUP BY project_id, project_name;\n",
    "```\n",
    "\n",
    "### Advanced Example: Complex Business Intelligence\n",
    "```sql\n",
    "-- Multi-level aggregation with JSON-like formatting\n",
    "WITH sales_summary AS (\n",
    "    SELECT \n",
    "        s.sales_rep_id,\n",
    "        sr.rep_name,\n",
    "        EXTRACT(YEAR FROM s.sale_date) as year,\n",
    "        EXTRACT(QUARTER FROM s.sale_date) as quarter,\n",
    "        c.industry,\n",
    "        STRING_AGG(\n",
    "            DISTINCT c.company_name \n",
    "            ORDER BY c.company_name\n",
    "        ) as clients,\n",
    "        ARRAY_AGG(\n",
    "            JSON_BUILD_OBJECT(\n",
    "                'deal_id', s.deal_id,\n",
    "                'amount', s.deal_amount,\n",
    "                'product', p.product_name,\n",
    "                'close_date', s.sale_date\n",
    "            ) ORDER BY s.deal_amount DESC\n",
    "        ) as deals_detail,\n",
    "        STRING_AGG(\n",
    "            DISTINCT p.product_category || ' (' || COUNT(s.deal_id) || ')',\n",
    "            ', '\n",
    "        ) as product_performance\n",
    "    FROM sales s\n",
    "    JOIN sales_reps sr ON s.sales_rep_id = sr.rep_id\n",
    "    JOIN customers c ON s.customer_id = c.customer_id\n",
    "    JOIN products p ON s.product_id = p.product_id\n",
    "    WHERE s.sale_date >= CURRENT_DATE - INTERVAL '2 years'\n",
    "    GROUP BY s.sales_rep_id, sr.rep_name, year, quarter, c.industry\n",
    "),\n",
    "performance_metrics AS (\n",
    "    SELECT \n",
    "        sales_rep_id,\n",
    "        rep_name,\n",
    "        STRING_AGG(\n",
    "            DISTINCT industry || ': ' || clients,\n",
    "            '; '\n",
    "            ORDER BY industry\n",
    "        ) as industry_client_map,\n",
    "        ARRAY_AGG(\n",
    "            DISTINCT ARRAY[year::text, quarter::text, deals_detail::text]\n",
    "        ) as quarterly_performance\n",
    "    FROM sales_summary\n",
    "    GROUP BY sales_rep_id, rep_name\n",
    ")\n",
    "SELECT \n",
    "    rep_name,\n",
    "    industry_client_map,\n",
    "    ARRAY_LENGTH(quarterly_performance, 1) as active_quarters\n",
    "FROM performance_metrics\n",
    "ORDER BY rep_name;\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Python Parallels\n",
    "\n",
    "### Basic String and List Aggregation\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# STRING_AGG equivalent using pandas\n",
    "df_string_agg = df.groupby('customer_id').agg({\n",
    "    'product_name': lambda x: ', '.join(x.unique()),  # STRING_AGG equivalent\n",
    "    'customer_name': 'first',  # Get customer name\n",
    "    'order_id': 'count'  # Count orders\n",
    "}).reset_index()\n",
    "\n",
    "# ARRAY_AGG equivalent using pandas\n",
    "df_array_agg = df.groupby('customer_id').agg({\n",
    "    'order_amount': list,  # ARRAY_AGG equivalent\n",
    "    'product_category': lambda x: list(x.unique())  # ARRAY_AGG DISTINCT\n",
    "}).reset_index()\n",
    "```\n",
    "\n",
    "### Advanced Pandas Aggregations\n",
    "```python\n",
    "# Complex aggregation with custom formatting\n",
    "def format_employee_list(group):\n",
    "    \"\"\"Custom function to format employee salary list\"\"\"\n",
    "    sorted_employees = group.sort_values('salary', ascending=False)\n",
    "    return ' | '.join([\n",
    "        f\"{'**' if row.role == 'Lead' else ''}{row.employee_name}{'**' if row.role == 'Lead' else ''} (${row.salary})\"\n",
    "        for _, row in sorted_employees.iterrows()\n",
    "    ])\n",
    "\n",
    "# Apply custom aggregation\n",
    "result = df.groupby('department').agg({\n",
    "    'employee_name': format_employee_list,\n",
    "    'skill': lambda x: sorted(x.unique()),\n",
    "    'salary': ['mean', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "# Using transform for more complex operations\n",
    "df['product_list'] = df.groupby('customer_id')['product_name'].transform(\n",
    "    lambda x: ', '.join(x.unique())\n",
    ")\n",
    "```\n",
    "\n",
    "### JSON-like Aggregation in Python\n",
    "```python\n",
    "import json\n",
    "\n",
    "# Complex nested aggregation\n",
    "def create_deals_summary(group):\n",
    "    deals = []\n",
    "    for _, row in group.iterrows():\n",
    "        deal = {\n",
    "            'deal_id': row['deal_id'],\n",
    "            'amount': row['deal_amount'],\n",
    "            'product': row['product_name'],\n",
    "            'close_date': row['sale_date'].strftime('%Y-%m-%d')\n",
    "        }\n",
    "        deals.append(deal)\n",
    "    return sorted(deals, key=lambda x: x['amount'], reverse=True)\n",
    "\n",
    "# Apply complex aggregation\n",
    "sales_summary = df.groupby(['sales_rep_id', 'year', 'quarter']).agg({\n",
    "    'company_name': lambda x: ', '.join(x.unique()),\n",
    "    'deal_id': create_deals_summary,\n",
    "    'product_category': lambda x: list(x.unique())\n",
    "}).reset_index()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Excel Parallels\n",
    "\n",
    "### Basic Text Concatenation\n",
    "```excel\n",
    "# Excel TEXTJOIN function (STRING_AGG equivalent)\n",
    "=TEXTJOIN(\", \", TRUE, IF(A:A=customer_id, B:B, \"\"))\n",
    "\n",
    "# Excel array formula for unique values\n",
    "=TEXTJOIN(\", \", TRUE, UNIQUE(IF(A:A=customer_id, B:B, \"\")))\n",
    "```\n",
    "\n",
    "### Power Query Aggregations\n",
    "```\n",
    "Power Query M Language:\n",
    "// STRING_AGG equivalent\n",
    "= Table.Group(\n",
    "    Source,\n",
    "    {\"customer_id\"},\n",
    "    {{\"products\", each Text.Combine([product_name], \", \"), type text}}\n",
    ")\n",
    "\n",
    "// ARRAY_AGG equivalent  \n",
    "= Table.Group(\n",
    "    Source,\n",
    "    {\"customer_id\"}, \n",
    "    {{\"order_amounts\", each [order_amount], type list}}\n",
    ")\n",
    "```\n",
    "\n",
    "### Advanced Excel Techniques\n",
    "```excel\n",
    "# Conditional concatenation\n",
    "=TEXTJOIN(\", \", TRUE, \n",
    "    IF((A:A=customer_id)*(C:C=\"Premium\"), \n",
    "       B:B & \" (Premium)\", \n",
    "       IF(A:A=customer_id, B:B, \"\")))\n",
    "\n",
    "# Using FILTER with TEXTJOIN (Excel 365)\n",
    "=TEXTJOIN(\", \", TRUE, \n",
    "    UNIQUE(FILTER(product_name, customer_id_range=target_customer)))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Importance vs Alternative Methods\n",
    "\n",
    "### Why STRING_AGG and Array Aggregations Excel\n",
    "\n",
    "**1. Performance Advantages**\n",
    "```sql\n",
    "-- Efficient single-query approach\n",
    "SELECT \n",
    "    customer_id,\n",
    "    STRING_AGG(product_name, ', ') as products\n",
    "FROM orders o\n",
    "JOIN products p ON o.product_id = p.product_id\n",
    "GROUP BY customer_id;\n",
    "\n",
    "-- vs. Inefficient application-level concatenation requiring multiple queries\n",
    "-- Query 1: Get customers\n",
    "-- Query 2: For each customer, get products\n",
    "-- Application: Concatenate strings\n",
    "```\n",
    "\n",
    "**2. Database-Level Optimization**\n",
    "- **Memory Efficiency**: Database engines optimize string concatenation\n",
    "- **Parallel Processing**: Can leverage database parallelization\n",
    "- **Index Usage**: GROUP BY operations can use indexes effectively\n",
    "\n",
    "**3. Data Consistency**\n",
    "- **Atomic Operations**: Ensures data consistency within single transaction\n",
    "- **Concurrent Access**: Handles multiple users accessing same data\n",
    "- **Snapshot Isolation**: Provides consistent view of data during aggregation\n",
    "\n",
    "### Comparison with Alternative Approaches\n",
    "\n",
    "**1. Application-Level Concatenation**\n",
    "```python\n",
    "# Inefficient approach\n",
    "customers = []\n",
    "for customer_id in get_customer_ids():\n",
    "    products = get_products_for_customer(customer_id)  # Multiple DB calls\n",
    "    product_list = ', '.join([p.name for p in products])\n",
    "    customers.append({'id': customer_id, 'products': product_list})\n",
    "\n",
    "# vs. Efficient single query with STRING_AGG\n",
    "# One database call, processed efficiently by database engine\n",
    "```\n",
    "\n",
    "**2. Multiple Queries with UNION**\n",
    "```sql\n",
    "-- Cumbersome alternative\n",
    "SELECT customer_id, GROUP_CONCAT(...) FROM orders WHERE customer_id = 1\n",
    "UNION ALL\n",
    "SELECT customer_id, GROUP_CONCAT(...) FROM orders WHERE customer_id = 2\n",
    "-- ... continues for each customer\n",
    "\n",
    "-- vs. Clean aggregation approach\n",
    "SELECT customer_id, STRING_AGG(product_name, ', ')\n",
    "FROM orders GROUP BY customer_id;\n",
    "```\n",
    "\n",
    "**3. Cursor-Based Processing**\n",
    "```sql\n",
    "-- Old-school approach (inefficient)\n",
    "DECLARE cursor_name CURSOR FOR SELECT DISTINCT customer_id FROM orders;\n",
    "-- Loop through each customer\n",
    "-- Build string manually\n",
    "-- Much slower and more complex\n",
    "\n",
    "-- vs. Set-based aggregation (efficient)\n",
    "SELECT customer_id, STRING_AGG(product_name, ', ')\n",
    "FROM orders GROUP BY customer_id;\n",
    "```\n",
    "\n",
    "### Advanced Use Cases and Benefits\n",
    "\n",
    "**1. Reporting and Business Intelligence**\n",
    "- **Executive Dashboards**: Create readable summaries for business stakeholders\n",
    "- **Data Export**: Prepare data for Excel/CSV exports with consolidated formats\n",
    "- **Email Reporting**: Generate formatted lists for automated business communications\n",
    "\n",
    "**2. ETL and Data Integration**\n",
    "- **Data Warehouse Loading**: Consolidate operational data for analytical systems\n",
    "- **API Responses**: Format data for REST APIs expecting consolidated responses\n",
    "- **Migration Scripts**: Prepare data for systems with different schema requirements\n",
    "\n",
    "**3. Performance Optimization Strategies**\n",
    "```sql\n",
    "-- Optimize with selective aggregation\n",
    "SELECT \n",
    "    category,\n",
    "    STRING_AGG(\n",
    "        product_name, \n",
    "        ', ' \n",
    "        ORDER BY sales_rank \n",
    "        LIMIT 10  -- Only top 10 products per category\n",
    "    ) as top_products\n",
    "FROM products\n",
    "WHERE is_active = true\n",
    "GROUP BY category;\n",
    "```\n",
    "\n",
    "**4. Advanced Formatting Techniques**\n",
    "```sql\n",
    "-- Custom formatting with business logic\n",
    "SELECT \n",
    "    department,\n",
    "    STRING_AGG(\n",
    "        CASE \n",
    "            WHEN performance_rating >= 4.5 THEN '⭐ ' || employee_name\n",
    "            WHEN performance_rating >= 4.0 THEN '✓ ' || employee_name  \n",
    "            ELSE employee_name\n",
    "        END,\n",
    "        ', '\n",
    "        ORDER BY performance_rating DESC, employee_name\n",
    "    ) as team_roster\n",
    "FROM employees\n",
    "GROUP BY department;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c906d9f2",
   "metadata": {},
   "source": [
    "## Practice Questions\n",
    "\n",
    "1. Write a query to find the 2nd highest salary from an employee table.\n",
    "\n",
    "    ```sql\n",
    "    SELECT\n",
    "    SALARY\n",
    "    FROM \n",
    "    EMPLOYEES\n",
    "    ORDER BY SALARY\n",
    "    LIMIT 1\n",
    "    OFFSET 1\n",
    "    ```\n",
    "\n",
    "2. Write a SQL query to find employees who have the same manager.\n",
    "\n",
    "3. Write a query to find the first and last record for each employee based on the 'hire_date' column.\n",
    "\n",
    "4. Write a query to find the most recent transaction for each customer.\n",
    "\n",
    "5. Write a query to find the total salary of each department and display departments with a total salary greater than a specified value (e.g., 50,000).\n",
    "\n",
    "6. Write a query to find the running total of orders for each customer sorted by order date.\n",
    "\n",
    "7. Write a query to get the total number of employees hired per month and year.\n",
    "\n",
    "8. Write a query to display all employees who earn more than the average salary for their department.\n",
    "\n",
    "9.  Write a query to get the second-lowest salary from the employee table without using LIMIT or OFFSET.\n",
    "\n",
    "10. Write a query to list all products that have never been ordered (assuming an 'orders' table and a 'products' table).\n",
    "\n",
    "11. Write a query to list all the employees who are also managers.\n",
    "\n",
    "12. Write a query to find employees who have joined in the same month and year.\n",
    "\n",
    "13. Write a SQL query to get a list of employees who are older than the average age of all employees in their department.\n",
    "\n",
    "14. Write a query to find the employee(s) with the longest tenure at the company.\n",
    "\n",
    "15. Write a query to delete all records from a table where the column value is NULL.\n",
    "\n",
    "16. Write a query to find all pairs of products that were ordered together at least once.\n",
    "\n",
    "17. Write a query to find the average order value by customer for each month.\n",
    "\n",
    "18. Write a query to identify customers who made a purchase every month for the past year.\n",
    "\n",
    "19. Write a query to find the total revenue for each product in a given quarter.\n",
    "\n",
    "20. Write a query to find the first purchase date of each customer.\n",
    "\n",
    "21. Write a query to calculate the year-on-year growth of revenue.\n",
    "\n",
    "22. Write a query to find the 'Nth' highest salary from the employee table (e.g., 5th highest salary)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
