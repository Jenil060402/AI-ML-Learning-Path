{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda15a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Month</th>\n",
       "      <th>Feb</th>\n",
       "      <th>Jan</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Department</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HR</th>\n",
       "      <td>210.0</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IT</th>\n",
       "      <td>NaN</td>\n",
       "      <td>190.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sales</th>\n",
       "      <td>220.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Month         Feb    Jan\n",
       "Department              \n",
       "HR          210.0  180.0\n",
       "IT            NaN  190.0\n",
       "Sales       220.0  200.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Department': ['Sales', 'Sales', 'HR', 'HR', 'IT'],\n",
    "    'Month': ['Jan', 'Feb', 'Jan', 'Feb', 'Jan'],\n",
    "    'Revenue': [200, 220, 180, 210, 190]\n",
    "})\n",
    "\n",
    "pivot = df.pivot_table(index='Department', columns='Month', values='Revenue', aggfunc='sum')\n",
    "pivot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1f9e8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "059d7a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pivot Table - Sales by Product and Region:\n",
      "Region   North  South\n",
      "Product              \n",
      "A          330      0\n",
      "B            0    330\n",
      "\n",
      "Cross-tabulation with margins:\n",
      "Region   North  South  All\n",
      "Product                   \n",
      "A            3      0    3\n",
      "B            0      2    2\n",
      "All          3      2    5\n"
     ]
    }
   ],
   "source": [
    "# ===== 1. PIVOT TABLES AND CROSS-TABULATION =====\n",
    "\n",
    "# Sample sales data\n",
    "sales_data = {\n",
    "    'Date': ['2024-01-01', '2024-01-01', '2024-01-02', '2024-01-02', '2024-01-03'],\n",
    "    'Product': ['A', 'B', 'A', 'B', 'A'],\n",
    "    'Region': ['North', 'South', 'North', 'South', 'North'],\n",
    "    'Sales': [100, 150, 120, 180, 110],\n",
    "    'Quantity': [10, 15, 12, 18, 11]\n",
    "}\n",
    "df_sales = pd.DataFrame(sales_data)\n",
    "\n",
    "# Pivot table - summarizing sales by product and region\n",
    "pivot_sales = df_sales.pivot_table(\n",
    "    values='Sales', \n",
    "    index='Product', \n",
    "    columns='Region', \n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ")\n",
    "print(\"Pivot Table - Sales by Product and Region:\")\n",
    "print(pivot_sales)\n",
    "print()\n",
    "\n",
    "# Cross-tabulation - frequency counts\n",
    "crosstab_result = pd.crosstab(df_sales['Product'], df_sales['Region'], margins=True)\n",
    "print(\"Cross-tabulation with margins:\")\n",
    "print(crosstab_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e7b3026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiIndex DataFrame:\n",
      "                  col1      col2      col3\n",
      "first second                              \n",
      "A     X       0.461300 -0.295523 -0.670975\n",
      "      Y       1.192192 -0.182910  0.492700\n",
      "B     X      -0.940511  0.976762 -0.747362\n",
      "      Y      -0.970677 -1.177551 -0.641383\n",
      "\n",
      "Access level 'A':\n",
      "            col1      col2      col3\n",
      "second                              \n",
      "X       0.461300 -0.295523 -0.670975\n",
      "Y       1.192192 -0.182910  0.492700\n",
      "\n",
      "Stacked DataFrame:\n",
      "first  second      \n",
      "A      X       col1    0.461300\n",
      "               col2   -0.295523\n",
      "               col3   -0.670975\n",
      "       Y       col1    1.192192\n",
      "               col2   -0.182910\n",
      "               col3    0.492700\n",
      "B      X       col1   -0.940511\n",
      "               col2    0.976762\n",
      "               col3   -0.747362\n",
      "       Y       col1   -0.970677\n",
      "               col2   -1.177551\n",
      "               col3   -0.641383\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ===== 2. MULTI-INDEXING AND HIERARCHICAL INDEXING =====\n",
    "\n",
    "# Creating MultiIndex DataFrame\n",
    "arrays = [\n",
    "    ['A', 'A', 'B', 'B'],\n",
    "    ['X', 'Y', 'X', 'Y']\n",
    "]\n",
    "index = pd.MultiIndex.from_arrays(arrays, names=['first', 'second'])\n",
    "df_multi = pd.DataFrame(np.random.randn(4, 3), index=index, columns=['col1', 'col2', 'col3'])\n",
    "\n",
    "print(\"MultiIndex DataFrame:\")\n",
    "print(df_multi)\n",
    "print()\n",
    "\n",
    "# Accessing multi-index data\n",
    "print(\"Access level 'A':\")\n",
    "print(df_multi.loc['A'])\n",
    "print()\n",
    "\n",
    "# Stack and unstack operations\n",
    "stacked = df_multi.stack()\n",
    "print(\"Stacked DataFrame:\")\n",
    "print(stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f46f107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Students with grades (using apply):\n",
      "      Name  Math  Science  English Grade\n",
      "0    Alice    85       90       78     B\n",
      "1      Bob    78       85       88     B\n",
      "2  Charlie    92       88       85     B\n",
      "3    Diana    88       95       92     A\n",
      "\n",
      "Students with GPA (using map):\n",
      "      Name Grade  GPA\n",
      "0    Alice     B  3.0\n",
      "1      Bob     B  3.0\n",
      "2  Charlie     B  3.0\n",
      "3    Diana     A  4.0\n",
      "\n",
      "Scaled scores (using apply):\n",
      "   Math  Science  English\n",
      "0  0.85     0.90     0.78\n",
      "1  0.78     0.85     0.88\n",
      "2  0.92     0.88     0.85\n",
      "3  0.88     0.95     0.92\n"
     ]
    }
   ],
   "source": [
    "# ===== 3. APPLYING CUSTOM FUNCTIONS =====\n",
    "\n",
    "# Sample dataset\n",
    "df_students = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
    "    'Math': [85, 78, 92, 88],\n",
    "    'Science': [90, 85, 88, 95],\n",
    "    'English': [78, 88, 85, 92]\n",
    "})\n",
    "\n",
    "# Using apply() for row-wise operations\n",
    "def calculate_grade(row):\n",
    "    avg = (row['Math'] + row['Science'] + row['English']) / 3\n",
    "    if avg >= 90:\n",
    "        return 'A'\n",
    "    elif avg >= 80:\n",
    "        return 'B'\n",
    "    else:\n",
    "        return 'C'\n",
    "\n",
    "df_students['Grade'] = df_students.apply(calculate_grade, axis=1)\n",
    "print(\"Students with grades (using apply):\")\n",
    "print(df_students)\n",
    "print()\n",
    "\n",
    "# Using map() for Series transformations\n",
    "grade_points = {'A': 4.0, 'B': 3.0, 'C': 2.0}\n",
    "df_students['GPA'] = df_students['Grade'].map(grade_points)\n",
    "print(\"Students with GPA (using map):\")\n",
    "print(df_students[['Name', 'Grade', 'GPA']])\n",
    "print()\n",
    "\n",
    "# Using apply() for element-wise operations on DataFrame\n",
    "df_numeric = df_students[['Math', 'Science', 'English']]\n",
    "df_scaled = df_numeric.apply(lambda x: x / 100)\n",
    "print(\"Scaled scores (using apply):\")\n",
    "print(df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f812123b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner join:\n",
      "   customer_id     name     city  order_id  amount\n",
      "0            1    Alice      NYC       101     100\n",
      "1            1    Alice      NYC       105      80\n",
      "2            2      Bob       LA       102     150\n",
      "3            2      Bob       LA       103     200\n",
      "4            3  Charlie  Chicago       104     120\n",
      "\n",
      "Left join:\n",
      "   customer_id     name     city  order_id  amount\n",
      "0            1    Alice      NYC     101.0   100.0\n",
      "1            1    Alice      NYC     105.0    80.0\n",
      "2            2      Bob       LA     102.0   150.0\n",
      "3            2      Bob       LA     103.0   200.0\n",
      "4            3  Charlie  Chicago     104.0   120.0\n",
      "5            4    Diana      NYC       NaN     NaN\n",
      "6            5      Eve   Boston       NaN     NaN\n",
      "\n",
      "Merge with suffixes:\n",
      "   customer_id name_customer     city  order_id  amount name_order\n",
      "0            1         Alice      NYC       101     100     Order1\n",
      "1            1         Alice      NYC       105      80     Order5\n",
      "2            2           Bob       LA       102     150     Order2\n",
      "3            2           Bob       LA       103     200     Order3\n",
      "4            3       Charlie  Chicago       104     120     Order4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== 4. EFFICIENT MERGING AND JOINING =====\n",
    "\n",
    "# Sample datasets\n",
    "df_customers = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'city': ['NYC', 'LA', 'Chicago', 'NYC', 'Boston']\n",
    "})\n",
    "\n",
    "df_orders = pd.DataFrame({\n",
    "    'order_id': [101, 102, 103, 104, 105, 106],\n",
    "    'customer_id': [1, 2, 2, 3, 1, 6],  # Note: customer_id 6 doesn't exist in customers\n",
    "    'amount': [100, 150, 200, 120, 80, 300]\n",
    "})\n",
    "\n",
    "# Inner join (default)\n",
    "inner_join = pd.merge(df_customers, df_orders, on='customer_id')\n",
    "print(\"Inner join:\")\n",
    "print(inner_join)\n",
    "print()\n",
    "\n",
    "# Left join to keep all customers\n",
    "left_join = pd.merge(df_customers, df_orders, on='customer_id', how='left')\n",
    "print(\"Left join:\")\n",
    "print(left_join)\n",
    "print()\n",
    "\n",
    "# Handling duplicates with suffixes\n",
    "df_orders_dup = df_orders.copy()\n",
    "df_orders_dup['name'] = ['Order1', 'Order2', 'Order3', 'Order4', 'Order5', 'Order6']\n",
    "merged_with_suffix = pd.merge(df_customers, df_orders_dup, on='customer_id', suffixes=('_customer', '_order'))\n",
    "print(\"Merge with suffixes:\")\n",
    "print(merged_with_suffix.head())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "943f8699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data with missing values:\n",
      "     A     B      C\n",
      "0  1.0  10.0  100.0\n",
      "1  2.0   NaN  200.0\n",
      "2  NaN  30.0  300.0\n",
      "3  4.0  40.0    NaN\n",
      "4  5.0   NaN  500.0\n",
      "5  NaN  60.0  600.0\n",
      "6  7.0  70.0  700.0\n",
      "\n",
      "Forward fill:\n",
      "     A     B      C\n",
      "0  1.0  10.0  100.0\n",
      "1  2.0  10.0  200.0\n",
      "2  2.0  30.0  300.0\n",
      "3  4.0  40.0  300.0\n",
      "4  5.0  40.0  500.0\n",
      "5  5.0  60.0  600.0\n",
      "6  7.0  70.0  700.0\n",
      "\n",
      "Backward fill:\n",
      "     A     B      C\n",
      "0  1.0  10.0  100.0\n",
      "1  2.0  30.0  200.0\n",
      "2  4.0  30.0  300.0\n",
      "3  4.0  40.0  500.0\n",
      "4  5.0  60.0  500.0\n",
      "5  7.0  60.0  600.0\n",
      "6  7.0  70.0  700.0\n",
      "\n",
      "Linear interpolation:\n",
      "     A     B      C\n",
      "0  1.0  10.0  100.0\n",
      "1  2.0  20.0  200.0\n",
      "2  3.0  30.0  300.0\n",
      "3  4.0  40.0  400.0\n",
      "4  5.0  50.0  500.0\n",
      "5  6.0  60.0  600.0\n",
      "6  7.0  70.0  700.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== 5. HANDLING MISSING DATA =====\n",
    "\n",
    "# Create dataset with missing values\n",
    "df_missing = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4, 5, np.nan, 7],\n",
    "    'B': [10, np.nan, 30, 40, np.nan, 60, 70],\n",
    "    'C': [100, 200, 300, np.nan, 500, 600, 700]\n",
    "})\n",
    "\n",
    "print(\"Original data with missing values:\")\n",
    "print(df_missing)\n",
    "print()\n",
    "\n",
    "# Forward fill\n",
    "df_ffill = df_missing.ffill()\n",
    "print(\"Forward fill:\")\n",
    "print(df_ffill)\n",
    "print()\n",
    "\n",
    "# Backward fill\n",
    "df_bfill = df_missing.bfill()\n",
    "print(\"Backward fill:\")\n",
    "print(df_bfill)\n",
    "print()\n",
    "\n",
    "# Interpolation\n",
    "df_interp = df_missing.interpolate()\n",
    "print(\"Linear interpolation:\")\n",
    "print(df_interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50faff85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID  Jan  Feb  Mar\n",
      "0   1  100  110  120\n",
      "1   2  150  160  170\n",
      "2   3  200  210  220\n",
      "\n",
      "Melted data (wide to long):\n",
      "   ID Month  Sales\n",
      "0   1   Jan    100\n",
      "1   2   Jan    150\n",
      "2   3   Jan    200\n",
      "3   1   Feb    110\n",
      "4   2   Feb    160\n",
      "5   3   Feb    210\n",
      "6   1   Mar    120\n",
      "7   2   Mar    170\n",
      "8   3   Mar    220\n",
      "\n",
      "Pivoted back (long to wide):\n",
      "Month  Feb  Jan  Mar\n",
      "ID                  \n",
      "1      110  100  120\n",
      "2      160  150  170\n",
      "3      210  200  220\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== 6. PIVOT AND MELT =====\n",
    "\n",
    "# Wide format data\n",
    "df_wide = pd.DataFrame({\n",
    "    'ID': [1, 2, 3],\n",
    "    'Jan': [100, 150, 200],\n",
    "    'Feb': [110, 160, 210],\n",
    "    'Mar': [120, 170, 220]\n",
    "})\n",
    "\n",
    "print(df_wide)\n",
    "print()\n",
    "\n",
    "# Melt to long format\n",
    "df_long = pd.melt(df_wide, id_vars=['ID'], var_name='Month', value_name='Sales')\n",
    "print(\"Melted data (wide to long):\")\n",
    "print(df_long)\n",
    "print()\n",
    "\n",
    "# Pivot back to wide format\n",
    "df_pivot_back = df_long.pivot(index='ID', columns='Month', values='Sales')\n",
    "print(\"Pivoted back (long to wide):\")\n",
    "print(df_pivot_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b134e480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom aggregation by customer:\n",
      "             total_amount  avg_amount  total_quantity  unique_products\n",
      "customer_id                                                           \n",
      "1                   360.0       120.0             4.0              3.0\n",
      "2                   320.0       160.0             4.0              2.0\n",
      "3                   270.0       135.0             3.0              2.0\n",
      "\n",
      "Multiple aggregations:\n",
      "            amount              quantity product\n",
      "               sum   mean count      sum nunique\n",
      "customer_id                                     \n",
      "1              360  120.0     3        4       3\n",
      "2              320  160.0     2        4       2\n",
      "3              270  135.0     2        3       2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jenil\\AppData\\Local\\Temp\\ipykernel_21196\\4216421807.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  customer_summary = df_transactions.groupby('customer_id').apply(custom_agg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== 7. DATA AGGREGATION =====\n",
    "\n",
    "# Sample transaction data\n",
    "df_transactions = pd.DataFrame({\n",
    "    'customer_id': [1, 1, 2, 2, 3, 3, 1],\n",
    "    'product': ['A', 'B', 'A', 'C', 'B', 'A', 'C'],\n",
    "    'amount': [100, 150, 200, 120, 180, 90, 110],\n",
    "    'quantity': [1, 2, 3, 1, 2, 1, 1]\n",
    "})\n",
    "\n",
    "# Custom aggregation functions\n",
    "def custom_agg(group):\n",
    "    return pd.Series({\n",
    "        'total_amount': group['amount'].sum(),\n",
    "        'avg_amount': group['amount'].mean(),\n",
    "        'total_quantity': group['quantity'].sum(),\n",
    "        'unique_products': group['product'].nunique()\n",
    "    })\n",
    "\n",
    "customer_summary = df_transactions.groupby('customer_id').apply(custom_agg)\n",
    "print(\"Custom aggregation by customer:\")\n",
    "print(customer_summary)\n",
    "print()\n",
    "\n",
    "# Multiple aggregations\n",
    "agg_multiple = df_transactions.groupby('customer_id').agg({\n",
    "    'amount': ['sum', 'mean', 'count'],\n",
    "    'quantity': 'sum',\n",
    "    'product': 'nunique'\n",
    "})\n",
    "print(\"Multiple aggregations:\")\n",
    "print(agg_multiple)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51bf6177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekly resampled data:\n",
      "                 value\n",
      "date                  \n",
      "2024-01-07  100.477451\n",
      "2024-01-14   99.525826\n",
      "2024-01-21   96.796580\n",
      "2024-01-28   94.857637\n",
      "2024-02-04   94.654703\n",
      "\n",
      "Rolling 7-day mean:\n",
      "                 value  rolling_mean\n",
      "date                                \n",
      "2024-01-01  101.070912           NaN\n",
      "2024-01-02   99.856025           NaN\n",
      "2024-01-03  100.085406           NaN\n",
      "2024-01-04  100.947886           NaN\n",
      "2024-01-05  101.211319           NaN\n",
      "2024-01-06   99.973992           NaN\n",
      "2024-01-07  100.196613    100.477451\n",
      "2024-01-08  100.225980    100.356746\n",
      "2024-01-09   99.723389    100.337798\n",
      "2024-01-10   99.278046    100.222461\n",
      "\n",
      "Expanding mean:\n",
      "                 value  expanding_mean\n",
      "date                                  \n",
      "2024-01-01  101.070912      101.070912\n",
      "2024-01-02   99.856025      100.463469\n",
      "2024-01-03  100.085406      100.337448\n",
      "2024-01-04  100.947886      100.490057\n",
      "2024-01-05  101.211319      100.634310\n",
      "2024-01-06   99.973992      100.524257\n",
      "2024-01-07  100.196613      100.477451\n",
      "2024-01-08  100.225980      100.446017\n",
      "2024-01-09   99.723389      100.365725\n",
      "2024-01-10   99.278046      100.256957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== 8. TIME SERIES OPERATIONS =====\n",
    "\n",
    "# Create time series data\n",
    "dates = pd.date_range('2024-01-01', periods=100, freq='D')\n",
    "df_ts = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'value': np.random.randn(100).cumsum() + 100\n",
    "})\n",
    "df_ts.set_index('date', inplace=True)\n",
    "\n",
    "# Resampling - daily to weekly\n",
    "weekly_data = df_ts.resample('W').mean()\n",
    "print(\"Weekly resampled data:\")\n",
    "print(weekly_data.head())\n",
    "print()\n",
    "\n",
    "# Rolling window\n",
    "df_ts['rolling_mean'] = df_ts['value'].rolling(window=7).mean()\n",
    "print(\"Rolling 7-day mean:\")\n",
    "print(df_ts[['value', 'rolling_mean']].head(10))\n",
    "print()\n",
    "\n",
    "# Expanding window\n",
    "df_ts['expanding_mean'] = df_ts['value'].expanding().mean()\n",
    "print(\"Expanding mean:\")\n",
    "print(df_ts[['value', 'expanding_mean']].head(10))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93c47903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenation (vertical):\n",
      "   A  B\n",
      "0  1  3\n",
      "1  2  4\n",
      "2  5  7\n",
      "3  6  8\n",
      "\n",
      "Concatenation (horizontal):\n",
      "   A  B   C   D\n",
      "0  1  3   9  11\n",
      "1  2  4  10  12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== 9. COMBINING DATAFRAMES =====\n",
    "\n",
    "df1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
    "df2 = pd.DataFrame({'A': [5, 6], 'B': [7, 8]})\n",
    "df3 = pd.DataFrame({'C': [9, 10], 'D': [11, 12]})\n",
    "\n",
    "# Concatenation\n",
    "concat_result = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"Concatenation (vertical):\")\n",
    "print(concat_result)\n",
    "print()\n",
    "\n",
    "# Concatenation (horizontal)\n",
    "concat_horizontal = pd.concat([df1, df3], axis=1)\n",
    "print(\"Concatenation (horizontal):\")\n",
    "print(concat_horizontal)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21067b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage comparison:\n",
      "Original types: int_col          int8\n",
      "float_col     float64\n",
      "str_col      category\n",
      "dtype: object\n",
      "Memory usage: Index        132\n",
      "int_col        5\n",
      "float_col     40\n",
      "str_col      427\n",
      "dtype: int64\n",
      "\n",
      "Vectorized operation completed on 1M rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== 10. PERFORMANCE OPTIMIZATION =====\n",
    "\n",
    "# Efficient data types\n",
    "df_optimize = pd.DataFrame({\n",
    "    'int_col': [1, 2, 3, 4, 5],\n",
    "    'float_col': [1.1, 2.2, 3.3, 4.4, 5.5],\n",
    "    'str_col': ['A', 'B', 'C', 'D', 'E']\n",
    "})\n",
    "\n",
    "# Convert to more efficient types\n",
    "df_optimize['int_col'] = df_optimize['int_col'].astype('int8')\n",
    "df_optimize['str_col'] = df_optimize['str_col'].astype('category')\n",
    "\n",
    "print(\"Memory usage comparison:\")\n",
    "print(\"Original types:\", df_optimize.dtypes)\n",
    "print(\"Memory usage:\", df_optimize.memory_usage(deep=True))\n",
    "print()\n",
    "\n",
    "# Vectorized operations (faster than loops)\n",
    "large_df = pd.DataFrame({'A': np.random.randn(1000000)})\n",
    "\n",
    "# Vectorized operation\n",
    "large_df['B'] = large_df['A'] * 2 + 1\n",
    "print(\"Vectorized operation completed on 1M rows\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf63362c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pivot Table - Sales by Product and Region:\n",
      "Region   North  South\n",
      "Product              \n",
      "A          330      0\n",
      "B            0    330\n",
      "\n",
      "Cross-tabulation with margins:\n",
      "Region   North  South  All\n",
      "Product                   \n",
      "A            3      0    3\n",
      "B            0      2    2\n",
      "All          3      2    5\n",
      "\n",
      "MultiIndex DataFrame:\n",
      "                  col1      col2      col3\n",
      "first second                              \n",
      "A     X      -0.921914 -0.112575 -0.524333\n",
      "      Y      -0.529446 -0.838279  0.248345\n",
      "B     X      -0.456554  0.641452 -0.970105\n",
      "      Y      -0.203389  0.401462 -0.620252\n",
      "\n",
      "Access level 'A':\n",
      "            col1      col2      col3\n",
      "second                              \n",
      "X      -0.921914 -0.112575 -0.524333\n",
      "Y      -0.529446 -0.838279  0.248345\n",
      "\n",
      "Stacked DataFrame:\n",
      "first  second      \n",
      "A      X       col1   -0.921914\n",
      "               col2   -0.112575\n",
      "               col3   -0.524333\n",
      "       Y       col1   -0.529446\n",
      "               col2   -0.838279\n",
      "dtype: float64\n",
      "\n",
      "Students with grades (using apply):\n",
      "      Name  Math  Science  English Grade\n",
      "0    Alice    85       90       78     B\n",
      "1      Bob    78       85       88     B\n",
      "2  Charlie    92       88       85     B\n",
      "3    Diana    88       95       92     A\n",
      "\n",
      "Students with GPA (using map):\n",
      "      Name Grade  GPA\n",
      "0    Alice     B  3.0\n",
      "1      Bob     B  3.0\n",
      "2  Charlie     B  3.0\n",
      "3    Diana     A  4.0\n",
      "\n",
      "Scaled scores (using applymap):\n",
      "   Math  Science  English\n",
      "0  0.85     0.90     0.78\n",
      "1  0.78     0.85     0.88\n",
      "2  0.92     0.88     0.85\n",
      "3  0.88     0.95     0.92\n",
      "\n",
      "Inner join:\n",
      "   customer_id     name     city  order_id  amount\n",
      "0            1    Alice      NYC       101     100\n",
      "1            1    Alice      NYC       105      80\n",
      "2            2      Bob       LA       102     150\n",
      "3            2      Bob       LA       103     200\n",
      "4            3  Charlie  Chicago       104     120\n",
      "\n",
      "Left join:\n",
      "   customer_id     name     city  order_id  amount\n",
      "0            1    Alice      NYC     101.0   100.0\n",
      "1            1    Alice      NYC     105.0    80.0\n",
      "2            2      Bob       LA     102.0   150.0\n",
      "3            2      Bob       LA     103.0   200.0\n",
      "4            3  Charlie  Chicago     104.0   120.0\n",
      "5            4    Diana      NYC       NaN     NaN\n",
      "6            5      Eve   Boston       NaN     NaN\n",
      "\n",
      "Merge with suffixes:\n",
      "   customer_id name_customer     city  order_id  amount name_order\n",
      "0            1         Alice      NYC       101     100     Order1\n",
      "1            1         Alice      NYC       105      80     Order5\n",
      "2            2           Bob       LA       102     150     Order2\n",
      "3            2           Bob       LA       103     200     Order3\n",
      "4            3       Charlie  Chicago       104     120     Order4\n",
      "\n",
      "Original data with missing values:\n",
      "     A     B      C\n",
      "0  1.0  10.0  100.0\n",
      "1  2.0   NaN  200.0\n",
      "2  NaN  30.0  300.0\n",
      "3  4.0  40.0    NaN\n",
      "4  5.0   NaN  500.0\n",
      "5  NaN  60.0  600.0\n",
      "6  7.0  70.0  700.0\n",
      "\n",
      "Forward fill:\n",
      "     A     B      C\n",
      "0  1.0  10.0  100.0\n",
      "1  2.0  10.0  200.0\n",
      "2  2.0  30.0  300.0\n",
      "3  4.0  40.0  300.0\n",
      "4  5.0  40.0  500.0\n",
      "5  5.0  60.0  600.0\n",
      "6  7.0  70.0  700.0\n",
      "\n",
      "Backward fill:\n",
      "     A     B      C\n",
      "0  1.0  10.0  100.0\n",
      "1  2.0  30.0  200.0\n",
      "2  4.0  30.0  300.0\n",
      "3  4.0  40.0  500.0\n",
      "4  5.0  60.0  500.0\n",
      "5  7.0  60.0  600.0\n",
      "6  7.0  70.0  700.0\n",
      "\n",
      "Linear interpolation:\n",
      "     A     B      C\n",
      "0  1.0  10.0  100.0\n",
      "1  2.0  20.0  200.0\n",
      "2  3.0  30.0  300.0\n",
      "3  4.0  40.0  400.0\n",
      "4  5.0  50.0  500.0\n",
      "5  6.0  60.0  600.0\n",
      "6  7.0  70.0  700.0\n",
      "\n",
      "Melted data (wide to long):\n",
      "   ID Month  Sales\n",
      "0   1   Jan    100\n",
      "1   2   Jan    150\n",
      "2   3   Jan    200\n",
      "3   1   Feb    110\n",
      "4   2   Feb    160\n",
      "5   3   Feb    210\n",
      "6   1   Mar    120\n",
      "7   2   Mar    170\n",
      "8   3   Mar    220\n",
      "\n",
      "Pivoted back (long to wide):\n",
      "Month  Feb  Jan  Mar\n",
      "ID                  \n",
      "1      110  100  120\n",
      "2      160  150  170\n",
      "3      210  200  220\n",
      "\n",
      "Custom aggregation by customer:\n",
      "             total_amount  avg_amount  total_quantity  unique_products\n",
      "customer_id                                                           \n",
      "1                   360.0       120.0             4.0              3.0\n",
      "2                   320.0       160.0             4.0              2.0\n",
      "3                   270.0       135.0             3.0              2.0\n",
      "\n",
      "Multiple aggregations:\n",
      "            amount              quantity product\n",
      "               sum   mean count      sum nunique\n",
      "customer_id                                     \n",
      "1              360  120.0     3        4       3\n",
      "2              320  160.0     2        4       2\n",
      "3              270  135.0     2        3       2\n",
      "\n",
      "Weekly resampled data:\n",
      "                 value\n",
      "date                  \n",
      "2024-01-07  102.468527\n",
      "2024-01-14  104.432965\n",
      "2024-01-21  105.268048\n",
      "2024-01-28  104.007721\n",
      "2024-02-04  104.950586\n",
      "\n",
      "Rolling 7-day mean:\n",
      "                 value  rolling_mean\n",
      "date                                \n",
      "2024-01-01  100.458717           NaN\n",
      "2024-01-02  102.737205           NaN\n",
      "2024-01-03  103.053159           NaN\n",
      "2024-01-04  101.552560           NaN\n",
      "2024-01-05  102.084539           NaN\n",
      "2024-01-06  103.684427           NaN\n",
      "2024-01-07  103.709082    102.468527\n",
      "2024-01-08  104.162109    102.997583\n",
      "2024-01-09  104.159745    103.200803\n",
      "2024-01-10  103.046613    103.199868\n",
      "\n",
      "Expanding mean:\n",
      "                 value  expanding_mean\n",
      "date                                  \n",
      "2024-01-01  100.458717      100.458717\n",
      "2024-01-02  102.737205      101.597961\n",
      "2024-01-03  103.053159      102.083027\n",
      "2024-01-04  101.552560      101.950410\n",
      "2024-01-05  102.084539      101.977236\n",
      "2024-01-06  103.684427      102.261768\n",
      "2024-01-07  103.709082      102.468527\n",
      "2024-01-08  104.162109      102.680225\n",
      "2024-01-09  104.159745      102.844616\n",
      "2024-01-10  103.046613      102.864816\n",
      "\n",
      "Concatenation (vertical):\n",
      "   A  B\n",
      "0  1  3\n",
      "1  2  4\n",
      "2  5  7\n",
      "3  6  8\n",
      "\n",
      "Concatenation (horizontal):\n",
      "   A  B   C   D\n",
      "0  1  3   9  11\n",
      "1  2  4  10  12\n",
      "\n",
      "Memory usage comparison:\n",
      "Original types: int_col          int8\n",
      "float_col     float64\n",
      "str_col      category\n",
      "dtype: object\n",
      "Memory usage: Index        132\n",
      "int_col        5\n",
      "float_col     40\n",
      "str_col      427\n",
      "dtype: int64\n",
      "\n",
      "Vectorized operation completed on 1M rows\n",
      "\n",
      "One-hot encoded data:\n",
      "   price  color_blue  color_green  color_red  size_large  size_medium  \\\n",
      "0     10       False        False       True       False        False   \n",
      "1     20        True        False      False       False         True   \n",
      "2     30       False         True      False        True        False   \n",
      "3     15       False        False       True       False        False   \n",
      "4     25        True        False      False        True        False   \n",
      "\n",
      "   size_small  \n",
      "0        True  \n",
      "1       False  \n",
      "2       False  \n",
      "3        True  \n",
      "4       False  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jenil\\AppData\\Local\\Temp\\ipykernel_21196\\2985153204.py:95: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_scaled = df_numeric.applymap(lambda x: x / 100)\n",
      "C:\\Users\\Jenil\\AppData\\Local\\Temp\\ipykernel_21196\\2985153204.py:149: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_ffill = df_missing.fillna(method='ffill')\n",
      "C:\\Users\\Jenil\\AppData\\Local\\Temp\\ipykernel_21196\\2985153204.py:155: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_bfill = df_missing.fillna(method='bfill')\n",
      "C:\\Users\\Jenil\\AppData\\Local\\Temp\\ipykernel_21196\\2985153204.py:207: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  customer_summary = df_transactions.groupby('customer_id').apply(custom_agg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoded data:\n",
      "   color  color_encoded\n",
      "0    red              2\n",
      "1   blue              0\n",
      "2  green              1\n",
      "3    red              2\n",
      "4   blue              0\n",
      "\n",
      "IT employees over 30:\n",
      "      name  age  salary department\n",
      "2  Charlie   35   70000         IT\n",
      "4      Eve   32   65000         IT\n",
      "\n",
      "High earners under 35:\n",
      "  name  age  salary department\n",
      "1  Bob   30   60000         HR\n",
      "4  Eve   32   65000         IT\n",
      "\n",
      "Data from SQL query:\n",
      "      name     city  total_spent\n",
      "0    Alice      NYC        180.0\n",
      "1      Bob       LA        350.0\n",
      "2  Charlie  Chicago        120.0\n",
      "3    Diana      NYC          NaN\n",
      "4      Eve   Boston          NaN\n",
      "\n",
      "Stock data with window functions:\n",
      "        date  price  cumulative_max  cumulative_min         ewm\n",
      "0 2024-01-01    100           100.0           100.0  100.000000\n",
      "1 2024-01-02    102           102.0           100.0  101.200000\n",
      "2 2024-01-03     98           102.0            98.0   99.684211\n",
      "3 2024-01-04    105           105.0            98.0  101.892308\n",
      "4 2024-01-05    108           108.0            98.0  104.236967\n",
      "5 2024-01-06    106           108.0            98.0  104.881203\n",
      "6 2024-01-07    110           110.0            98.0  106.693541\n",
      "7 2024-01-08    107           110.0            98.0  106.799841\n",
      "8 2024-01-09    112           112.0            98.0  108.579521\n",
      "9 2024-01-10    115           115.0            98.0  110.757449\n",
      "\n",
      "DataFrame saved to pickle\n",
      "DataFrame loaded from pickle:\n",
      "      Name  Math  Science  English Grade  GPA\n",
      "0    Alice    85       90       78     B  3.0\n",
      "1      Bob    78       85       88     B  3.0\n",
      "2  Charlie    92       88       85     B  3.0\n",
      "3    Diana    88       95       92     A  4.0\n",
      "\n",
      "Properly modified DataFrame using .loc[]:\n",
      "   A   B     C\n",
      "0  1  10   NaN\n",
      "1  2  20   NaN\n",
      "2  3  30   6.0\n",
      "3  4  40   8.0\n",
      "4  5  50  10.0\n",
      "\n",
      "Comprehensive statistics:\n",
      "                 A            B            C\n",
      "count  1000.000000  1000.000000  1000.000000\n",
      "mean     99.869513    49.619007     1.929624\n",
      "std      15.236534    10.156689     1.827873\n",
      "min      49.162267    17.799231     0.003680\n",
      "25%      89.557563    42.761154     0.544466\n",
      "50%      99.599838    49.968928     1.386298\n",
      "75%     110.292432    56.580530     2.692622\n",
      "max     161.814457    80.026931    12.361479\n",
      "\n",
      "Custom percentiles:\n",
      "                 A            B            C\n",
      "count  1000.000000  1000.000000  1000.000000\n",
      "mean     99.869513    49.619007     1.929624\n",
      "std      15.236534    10.156689     1.827873\n",
      "min      49.162267    17.799231     0.003680\n",
      "10%      80.132995    36.294986     0.194369\n",
      "25%      89.557563    42.761154     0.544466\n",
      "50%      99.599838    49.968928     1.386298\n",
      "75%     110.292432    56.580530     2.692622\n",
      "90%     119.701882    62.511215     4.733897\n",
      "max     161.814457    80.026931    12.361479\n",
      "\n",
      "Correlation matrix:\n",
      "          A         B         C\n",
      "A  1.000000  0.030919 -0.018996\n",
      "B  0.030919  1.000000 -0.044785\n",
      "C -0.018996 -0.044785  1.000000\n",
      "\n",
      "Skewness:\n",
      "A    0.060806\n",
      "B   -0.101098\n",
      "C    1.497397\n",
      "dtype: float64\n",
      "\n",
      "Kurtosis:\n",
      "A    0.032198\n",
      "B   -0.013422\n",
      "C    2.477711\n",
      "dtype: float64\n",
      "\n",
      "=== ADVANCED PANDAS CONCEPTS DEMONSTRATED ===\n",
      "This comprehensive example covers all major advanced pandas concepts\n",
      "including pivot tables, multi-indexing, custom functions, merging,\n",
      "missing data handling, time series operations, and performance optimization.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ===== 11. ENCODING FOR ML =====\n",
    "\n",
    "# Sample categorical data\n",
    "df_categorical = pd.DataFrame({\n",
    "    'color': ['red', 'blue', 'green', 'red', 'blue'],\n",
    "    'size': ['small', 'medium', 'large', 'small', 'large'],\n",
    "    'price': [10, 20, 30, 15, 25]\n",
    "})\n",
    "\n",
    "# One-hot encoding\n",
    "df_onehot = pd.get_dummies(df_categorical, columns=['color', 'size'], prefix=['color', 'size'])\n",
    "print(\"One-hot encoded data:\")\n",
    "print(df_onehot)\n",
    "print()\n",
    "\n",
    "# Label encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_categorical['color_encoded'] = le.fit_transform(df_categorical['color'])\n",
    "print(\"Label encoded data:\")\n",
    "print(df_categorical[['color', 'color_encoded']])\n",
    "print()\n",
    "\n",
    "# ===== 12. ADVANCED INDEXING WITH QUERY =====\n",
    "\n",
    "# Sample data for querying\n",
    "df_query = pd.DataFrame({\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'age': [25, 30, 35, 28, 32],\n",
    "    'salary': [50000, 60000, 70000, 55000, 65000],\n",
    "    'department': ['IT', 'HR', 'IT', 'Finance', 'IT']\n",
    "})\n",
    "\n",
    "# Using query for filtering\n",
    "it_employees = df_query.query('department == \"IT\" and age > 30')\n",
    "print(\"IT employees over 30:\")\n",
    "print(it_employees)\n",
    "print()\n",
    "\n",
    "# Complex query with variables\n",
    "min_salary = 55000\n",
    "high_earners = df_query.query('salary > @min_salary and age < 35')\n",
    "print(\"High earners under 35:\")\n",
    "print(high_earners)\n",
    "print()\n",
    "\n",
    "# ===== 13. SQL INTEGRATION =====\n",
    "\n",
    "# Create in-memory SQLite database\n",
    "conn = sqlite3.connect(':memory:')\n",
    "\n",
    "# Write DataFrame to SQL\n",
    "df_customers.to_sql('customers', conn, index=False, if_exists='replace')\n",
    "df_orders.to_sql('orders', conn, index=False, if_exists='replace')\n",
    "\n",
    "# Read from SQL with custom query\n",
    "sql_query = \"\"\"\n",
    "SELECT c.name, c.city, SUM(o.amount) as total_spent\n",
    "FROM customers c\n",
    "LEFT JOIN orders o ON c.customer_id = o.customer_id\n",
    "GROUP BY c.customer_id, c.name, c.city\n",
    "\"\"\"\n",
    "\n",
    "df_from_sql = pd.read_sql_query(sql_query, conn)\n",
    "print(\"Data from SQL query:\")\n",
    "print(df_from_sql)\n",
    "print()\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# ===== 14. WINDOW FUNCTIONS =====\n",
    "\n",
    "# Sample stock data\n",
    "df_stock = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-01', periods=10),\n",
    "    'price': [100, 102, 98, 105, 108, 106, 110, 107, 112, 115]\n",
    "})\n",
    "\n",
    "# Expanding window for cumulative statistics\n",
    "df_stock['cumulative_max'] = df_stock['price'].expanding().max()\n",
    "df_stock['cumulative_min'] = df_stock['price'].expanding().min()\n",
    "\n",
    "# Exponentially weighted moving average\n",
    "df_stock['ewm'] = df_stock['price'].ewm(span=5).mean()\n",
    "\n",
    "print(\"Stock data with window functions:\")\n",
    "print(df_stock)\n",
    "print()\n",
    "\n",
    "# ===== 15. SERIALIZATION =====\n",
    "\n",
    "# Save DataFrame to pickle\n",
    "df_students.to_pickle('students.pkl')\n",
    "print(\"DataFrame saved to pickle\")\n",
    "\n",
    "# Load DataFrame from pickle\n",
    "df_loaded = pd.read_pickle('students.pkl')\n",
    "print(\"DataFrame loaded from pickle:\")\n",
    "print(df_loaded)\n",
    "print()\n",
    "\n",
    "# ===== 16. SETTINGWITHCOPYWARNING AND .loc[] =====\n",
    "\n",
    "# Create sample data\n",
    "df_copy = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4, 5],\n",
    "    'B': [10, 20, 30, 40, 50]\n",
    "})\n",
    "\n",
    "# WRONG way (may cause SettingWithCopyWarning)\n",
    "# df_subset = df_copy[df_copy['A'] > 2]\n",
    "# df_subset['C'] = df_subset['A'] * 2  # This might cause warning\n",
    "\n",
    "# CORRECT way using .loc[]\n",
    "df_copy.loc[df_copy['A'] > 2, 'C'] = df_copy.loc[df_copy['A'] > 2, 'A'] * 2\n",
    "\n",
    "print(\"Properly modified DataFrame using .loc[]:\")\n",
    "print(df_copy)\n",
    "print()\n",
    "\n",
    "# ===== 17. STATISTICAL FUNCTIONS =====\n",
    "\n",
    "# Sample dataset for statistics\n",
    "df_stats = pd.DataFrame({\n",
    "    'A': np.random.normal(100, 15, 1000),\n",
    "    'B': np.random.normal(50, 10, 1000),\n",
    "    'C': np.random.exponential(2, 1000)\n",
    "})\n",
    "\n",
    "# Comprehensive statistics\n",
    "stats_summary = df_stats.describe()\n",
    "print(\"Comprehensive statistics:\")\n",
    "print(stats_summary)\n",
    "print()\n",
    "\n",
    "# Custom percentiles\n",
    "custom_percentiles = df_stats.describe(percentiles=[.1, .25, .5, .75, .9])\n",
    "print(\"Custom percentiles:\")\n",
    "print(custom_percentiles)\n",
    "print()\n",
    "\n",
    "# Correlation matrix\n",
    "correlation_matrix = df_stats.corr()\n",
    "print(\"Correlation matrix:\")\n",
    "print(correlation_matrix)\n",
    "print()\n",
    "\n",
    "# Skewness and kurtosis\n",
    "print(\"Skewness:\")\n",
    "print(df_stats.skew())\n",
    "print()\n",
    "\n",
    "print(\"Kurtosis:\")\n",
    "print(df_stats.kurtosis())\n",
    "print()\n",
    "\n",
    "print(\"=== ADVANCED PANDAS CONCEPTS DEMONSTRATED ===\")\n",
    "print(\"This comprehensive example covers all major advanced pandas concepts\")\n",
    "print(\"including pivot tables, multi-indexing, custom functions, merging,\")\n",
    "print(\"missing data handling, time series operations, and performance optimization.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
